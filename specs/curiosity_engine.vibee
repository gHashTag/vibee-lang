# CuriosityEngine - Intrinsic Motivation and Exploration
# Source: arXiv:2510.05013 - Curiosity-Driven Development, arXiv:2502.10077 - ECL
# PAS Analysis: MLS (prediction error), PRE (novelty cache), PRB (exploration)

name: curiosity_engine
version: "1.0.0"
language: 999
module: ⲔⲨⲢⲒⲞⲤⲒⲦⲨ_ⲈⲚⲄⲒⲚⲈ

pas_analysis:
  source_paper: "arXiv:2510.05013, arXiv:2502.10077, arXiv:2501.02997"
  current_complexity: "O(n) prediction per state"
  theoretical_lower_bound: "O(log n) cached novelty"
  gap: "Linear to logarithmic via caching"
  patterns_applicable:
    - symbol: MLS
      name: "ML-Guided Search"
      success_rate: 0.06
      rationale: "Learn prediction model"
    - symbol: PRE
      name: "Precomputation"
      success_rate: 0.16
      rationale: "Cache visited states"
    - symbol: PRB
      name: "Probabilistic"
      success_rate: 0.12
      rationale: "Stochastic exploration"
    - symbol: HSH
      name: "Hashing"
      success_rate: 0.12
      rationale: "State hashing for novelty"
  confidence: 0.77
  predicted_improvement: "Efficient exploration without extrinsic rewards"

creation_pattern:
  source: EnvironmentState
  transformer: CuriosityModule
  result: IntrinsicReward

behaviors:
  - name: prediction_error_curiosity
    given: "Current state and action"
    when: "Predict next state"
    then: "Curiosity from prediction error"
    test_cases:
      - name: icm_curiosity
        input:
          state: "s_t"
          action: "a_t"
          next_state: "s_{t+1}"
        expected:
          prediction_error: 0.3
          intrinsic_reward: 0.3

  - name: novelty_detection
    given: "New state"
    when: "Compare to visited states"
    then: "Novelty score"
    test_cases:
      - name: rnd_novelty
        input:
          state: "new_state"
          random_network: "fixed"
        expected:
          novelty: 0.8
          exploration_bonus: true

  - name: empowerment_maximization
    given: "State and action space"
    when: "Maximize mutual information"
    then: "Empowerment value"
    test_cases:
      - name: empowerment
        input:
          horizon: 5
          action_dim: 4
        expected:
          empowerment: 2.5
          controllability: "high"

  - name: compositional_generalization
    given: "Limited experience"
    when: "Apply curiosity-driven learning"
    then: "Generalize to new combinations"
    test_cases:
      - name: language_action
        input:
          training_pairs: 100
          novel_combinations: 50
        expected:
          generalization: 0.85
          u_shaped_learning: true

algorithms:
  icm:
    forward_model: "Predict next state"
    inverse_model: "Predict action from states"
    curiosity: "Forward model error"
    
  rnd:
    target_network: "Random, fixed"
    predictor_network: "Trained"
    novelty: "Prediction error"
    
  empowerment:
    formula: "I(A; S' | S)"
    maximization: "Variational bound"

curiosity_types:
  prediction_error: "ICM, forward dynamics"
  count_based: "State visitation counts"
  information_gain: "Bayesian surprise"
  empowerment: "Mutual information"

metrics:
  exploration_coverage: 0.95
  sample_efficiency: 10
  generalization: 0.85
  sparse_reward_success: 0.9
