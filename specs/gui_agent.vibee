# GUIAgent - Vision-Language GUI Understanding and Interaction
# Source: arXiv:2511.09127 - HAR-GUI, arXiv:2509.15566 - BTL-UI, arXiv:2408.00203 - OmniParser
# PAS Analysis: MLS (screen understanding), D&C (hierarchy parsing), PRE (element detection)

name: gui_agent
version: "1.0.0"
language: 999
module: ⲄⲨⲒ_ⲀⲄⲈⲚⲦ

pas_analysis:
  source_paper: "arXiv:2511.09127, arXiv:2509.15566, arXiv:2408.00203"
  current_complexity: "O(n²) full screen attention"
  theoretical_lower_bound: "O(n) hierarchical parsing"
  gap: "Quadratic to linear via hierarchy"
  patterns_applicable:
    - symbol: MLS
      name: "ML-Guided Search"
      success_rate: 0.06
      rationale: "Vision-language screen understanding"
    - symbol: D&C
      name: "Divide-and-Conquer"
      success_rate: 0.31
      rationale: "Hierarchical screen parsing"
    - symbol: PRE
      name: "Precomputation"
      success_rate: 0.16
      rationale: "Pre-detect interactable elements"
    - symbol: HSH
      name: "Hashing"
      success_rate: 0.12
      rationale: "Fast element lookup"
  confidence: 0.79
  predicted_improvement: "History-aware GUI reasoning"

creation_pattern:
  source: ScreenImage
  transformer: GUIUnderstanding
  result: ActionSequence

behaviors:
  - name: screen_parsing
    given: "Screenshot of application"
    when: "Apply OmniParser"
    then: "Extract structured elements"
    test_cases:
      - name: web_parsing
        input:
          screenshot: "webpage.png"
          platform: "web"
        expected:
          elements_detected: 50
          accuracy: 0.95

  - name: history_aware_reasoning
    given: "Current screen and interaction history"
    when: "Apply HAR-GUI"
    then: "Generate context-aware action"
    test_cases:
      - name: multi_step_task
        input:
          history_length: 5
          task: "fill_form"
        expected:
          success_rate: 0.85
          history_utilized: true

  - name: blink_think_link
    given: "Screen and instruction"
    when: "Apply BTL-UI"
    then: "Execute human-like interaction"
    test_cases:
      - name: btl_interaction
        input:
          instruction: "Click submit button"
          screen: "form_page"
        expected:
          blink_accuracy: 0.9
          think_reasoning: true
          link_execution: true

  - name: element_grounding
    given: "Natural language reference"
    when: "Ground to screen element"
    then: "Return element coordinates"
    test_cases:
      - name: referring_expression
        input:
          reference: "the blue button on the right"
          screen: "dashboard"
        expected:
          iou: 0.85
          confidence: 0.9

algorithms:
  omniparser:
    detection: "Fine-tuned icon detector"
    caption: "Element semantic extraction"
    grounding: "Region-action association"
    
  har_gui:
    memory: "Episodic interaction history"
    reasoning: "Chain-of-thought with history"
    adaptation: "RL fine-tuning"
    
  btl_ui:
    blink: "Rapid attention to relevant areas"
    think: "Higher-level reasoning"
    link: "Executable command generation"

platforms:
  web: "HTML/CSS/JavaScript"
  desktop: "Windows/macOS/Linux"
  mobile: "iOS/Android"

metrics:
  screenspot_accuracy: 0.85
  mind2web_success: 0.75
  aitw_accuracy: 0.80
  androidworld_success: 0.70
