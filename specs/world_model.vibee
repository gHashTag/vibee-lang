# WorldModel - Imagination-Based Learning via Latent Dynamics
# Source: arXiv:2301.04104 - DreamerV3, arXiv:2509.24527 - Dreamer 4
# PAS Analysis: MLS (150+ tasks), PRE (latent imagination)

name: world_model
version: "1.0.0"
language: 999
module: ⲰⲞⲢⲖⲆ_ⲘⲞⲆⲈⲖ

pas_analysis:
  source_paper: "arXiv:2301.04104, arXiv:2509.24527"
  current_complexity: "O(n) real environment steps"
  theoretical_lower_bound: "O(1) imagined steps"
  gap: "Factor of n (sample efficiency)"
  patterns_applicable:
    - symbol: MLS
      name: "ML-Guided Search"
      success_rate: 0.06
      rationale: "Learn world dynamics from data"
    - symbol: PRE
      name: "Precomputation"
      success_rate: 0.16
      rationale: "Pre-imagine trajectories in latent space"
    - symbol: D&C
      name: "Divide-and-Conquer"
      success_rate: 0.31
      rationale: "Separate encoder, dynamics, decoder"
  confidence: 0.78
  predicted_improvement: "100x sample efficiency"

creation_pattern:
  source: RealExperience
  transformer: LatentDynamicsModel
  result: ImaginedTrajectory

behaviors:
  - name: encode_observation
    given: "Raw observation (image, state)"
    when: "Pass through encoder"
    then: "Produce latent representation"
    test_cases:
      - name: image_encoding
        input:
          observation: "64x64 RGB image"
          encoder: "CNN"
        expected:
          latent_dim: 32
          deterministic: true

  - name: predict_dynamics
    given: "Latent state and action"
    when: "Apply dynamics model"
    then: "Predict next latent state"
    test_cases:
      - name: one_step_prediction
        input:
          latent: [0.1, 0.2, 0.3]
          action: "move_forward"
        expected:
          next_latent: [0.15, 0.25, 0.35]
          reward: 0.5

  - name: imagine_trajectory
    given: "Initial latent state"
    when: "Rollout policy in imagination"
    then: "Generate imagined trajectory"
    test_cases:
      - name: horizon_15
        input:
          initial_latent: [0.0, 0.0, 0.0]
          horizon: 15
        expected:
          trajectory_length: 15
          total_reward: 7.5

  - name: actor_critic_learning
    given: "Imagined trajectories"
    when: "Compute returns and gradients"
    then: "Update policy and value"
    test_cases:
      - name: policy_improvement
        input:
          trajectories: 16
          lambda_return: 0.95
        expected:
          policy_loss_decreased: true
          value_loss_decreased: true

algorithms:
  rssm:
    name: "Recurrent State-Space Model"
    components:
      - "Deterministic path: h_t = f(h_{t-1}, z_{t-1}, a_{t-1})"
      - "Stochastic path: z_t ~ p(z_t | h_t)"
      - "Observation: o_t ~ p(o_t | h_t, z_t)"
    
  symlog:
    formula: "symlog(x) = sign(x) * log(|x| + 1)"
    purpose: "Handle varying reward scales"
    
  lambda_return:
    formula: "G_t^λ = r_t + γ * ((1-λ) * V(s_{t+1}) + λ * G_{t+1}^λ)"
    lambda: 0.95
    gamma: 0.997

achievements:
  dreamerv3:
    tasks: 150
    domains: ["Atari", "DMC", "Minecraft", "Crafter"]
    single_config: true
    
  dreamer4:
    achievement: "First agent to get diamonds in Minecraft from offline data"
    sample_efficiency: "100x vs model-free"

metrics:
  sample_efficiency: 100
  imagination_horizon: 15
  latent_dimension: 32
