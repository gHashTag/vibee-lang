# v3005 - Attention Mechanism Specification
# ==========================================
# Self-attention and Flash Attention for CPU
# φ² + 1/φ² = 3 | PHOENIX = 999

name: attention
version: "3.0.5"
language: zig
module: attention

constants:
  PHI: 1.618033988749895
  SOFTMAX_SCALE: 1.0
  BLOCK_SIZE: 64

types:
  AttentionConfig:
    fields:
      num_heads: Int
      head_dim: Int
      dropout: Float
      is_causal: Bool
      
  AttentionOutput:
    fields:
      output: List
      attention_weights: List
      
  FlashAttentionConfig:
    fields:
      block_size: Int
      num_heads: Int
      head_dim: Int
      is_causal: Bool
      
  MultiHeadConfig:
    fields:
      hidden_size: Int
      num_heads: Int
      head_dim: Int
      dropout: Float

behaviors:
  - name: scaled_dot_product
    given: Query, Key, Value tensors
    when: Computing attention scores
    then: Return softmax(QK^T / sqrt(d)) * V
    
  - name: apply_causal_mask
    given: Attention scores and sequence length
    when: Masking future positions
    then: Return masked scores with -inf for future
    
  - name: softmax_attention
    given: Attention scores
    when: Normalizing scores
    then: Return probability distribution
    
  - name: flash_attention_forward
    given: Q, K, V and config
    when: Computing memory-efficient attention
    then: Return output with O(N) memory
    
  - name: multi_head_attention
    given: Input, weights, and config
    when: Computing multi-head attention
    then: Return concatenated head outputs
    
  - name: compute_attention_scale
    given: Head dimension
    when: Computing scale factor
    then: Return 1/sqrt(head_dim) scaled by φ
    
  - name: split_heads
    given: Tensor and num_heads
    when: Reshaping for multi-head
    then: Return tensor with head dimension
    
  - name: merge_heads
    given: Multi-head tensor
    when: Concatenating heads
    then: Return merged tensor
