name: webarena_evaluator_v22
version: "22.4.0"
language: zig
module: webarena_evaluator_v22

types:
  EvalType:
    fields:
      name: String

  EvalConfig:
    fields:
      strict_mode: Bool
      partial_credit: Bool
      timeout_ms: Int

  EvalInput:
    fields:
      task_id: String
      agent_answer: String
      reference_answer: String
      eval_type: String
      page_state: Option<String>

  EvalResult:
    fields:
      task_id: String
      success: Bool
      score: Float
      eval_type: String
      details: String

  EvalMetrics:
    fields:
      total_tasks: Int
      passed: Int
      failed: Int
      success_rate: Float
      avg_score: Float

  Evaluator:
    fields:
      config: Object
      metrics: Object

behaviors:
  - name: create_evaluator
    given: EvalConfig
    when: Initialize evaluator
    then: Evaluator instance

  - name: evaluate
    given: Evaluator and EvalInput
    when: Evaluate result
    then: EvalResult

  - name: eval_string_match
    given: Agent answer and reference
    when: Exact string match
    then: Match result

  - name: eval_url_match
    given: Current URL and expected URL
    when: URL comparison
    then: Match result

  - name: eval_element_exists
    given: Page state and selector
    when: Check element
    then: Exists result

  - name: eval_text_contains
    given: Page text and expected text
    when: Text search
    then: Contains result

  - name: eval_program
    given: Agent answer and program
    when: Programmatic eval
    then: Program result

  - name: eval_fuzzy_match
    given: Agent answer and reference
    when: Fuzzy comparison
    then: Similarity score

  - name: get_metrics
    given: Evaluator
    when: Get eval metrics
    then: EvalMetrics

  - name: reset_metrics
    given: Evaluator
    when: Reset metrics
    then: Reset complete

  - name: batch_evaluate
    given: Evaluator and results list
    when: Evaluate batch
    then: Batch results
