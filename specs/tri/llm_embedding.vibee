# LLM Embedding - RoPE Position Encoding
# φ² + 1/φ² = 3 | PHOENIX = 999

name: llm_embedding
version: "1.0.0"
language: zig
module: llm_embedding

sacred_formula:
  expression: "V = n × 3^k × π^m × φ^p"
  golden_identity: "φ² + 1/φ² = 3"

creation_pattern:
  source: TokenIDs
  transformer: EmbeddingLayer
  result: HiddenStates

types:
  EmbeddingConfig:
    fields:
      vocab_size: Int  # 32000
      hidden_size: Int  # 768
      max_position: Int  # 4096
      rope_theta: Float  # 10000.0
      
  RoPEConfig:
    fields:
      dim: Int
      base: Float
      scaling_factor: Float
      
behaviors:
  - name: embed_tokens
    given: "Token IDs [batch, seq_len]"
    when: "Lookup embeddings"
    then: "Return embeddings [batch, seq_len, hidden]"
    
  - name: compute_rope_freqs
    given: "Sequence length, head_dim"
    when: "Precompute RoPE frequencies"
    then: "Return cos/sin cache"
    formula: "θ_i = base^(-2i/d)"
    
  - name: apply_rope
    given: "Q, K tensors and position"
    when: "Apply rotary embedding"
    then: "Return position-encoded Q, K"
    
  - name: rope_scaling
    given: "Position beyond max_position"
    when: "Extend context"
    then: "Apply NTK-aware scaling"
