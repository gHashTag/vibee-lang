# iGLA Training Evaluation
# Model evaluation during training
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_evaluation
version: "1.0.0"
language: zig
module: igla_training_evaluation

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  EvaluationConfig:
    fields:
      benchmarks: List<String>
      eval_batch_size: Int
      num_fewshot: Int
      limit_samples: Int

  BenchmarkResult:
    fields:
      benchmark: String
      score: Float
      std_err: Float
      num_samples: Int

  EvaluationSuite:
    fields:
      results: List<BenchmarkResult>
      aggregate_score: Float
      timestamp: String

  EvaluationMetrics:
    fields:
      humaneval: Float
      mbpp: Float
      mmlu: Float
      gsm8k: Float
      hellaswag: Float
      arc: Float
      winogrande: Float

behaviors:
  - name: run_lm_eval
    given: "Model checkpoint"
    when: "Evaluation"
    then: "lm-evaluation-harness run"

  - name: eval_humaneval
    given: "Model"
    when: "HumanEval"
    then: "Code generation evaluated"

  - name: eval_mmlu
    given: "Model"
    when: "MMLU"
    then: "Knowledge evaluated"

  - name: eval_gsm8k
    given: "Model"
    when: "GSM8K"
    then: "Math reasoning evaluated"

  - name: aggregate_scores
    given: "All results"
    when: "Aggregation"
    then: "Weighted average computed"

  - name: compare_baseline
    given: "Results"
    when: "Comparison"
    then: "vs Llama/Mistral baselines"

  - name: phi_evaluation_harmony
    given: "Evaluation"
    when: "Harmony"
    then: "φ-weighted benchmark score"
