name: agent_scraper_v492
version: "492.0.0"
language: zig
module: agent_scraper_v492

types:
  ScraperAgent:
    fields:
      agent_id: String
      target_domain: String
      rate_limit_ms: Int
      respect_robots: Bool
      user_agent: String

  ScrapeTarget:
    fields:
      url: String
      selectors: Map<String,String>
      pagination: Option<String>
      max_pages: Int

  ScrapedData:
    fields:
      source_url: String
      extracted_at: Timestamp
      fields: Map<String,String>
      raw_html: Option<String>

  ScrapeRule:
    fields:
      field_name: String
      selector: String
      attribute: Option<String>
      transform: Option<String>
      required: Bool

  ScrapeResult:
    fields:
      success: Bool
      items_count: Int
      errors: List<String>
      duration_ms: Int

behaviors:
  - name: configure_scraper
    given: Scraper settings
    when: Initialize
    then: Return configured scraper

  - name: add_scrape_rule
    given: Field and selector
    when: Rule added
    then: Register extraction rule

  - name: scrape_page
    given: Target URL
    when: Scrape requested
    then: Extract data from page

  - name: scrape_with_pagination
    given: Paginated target
    when: Multi-page scrape
    then: Extract from all pages

  - name: transform_data
    given: Raw extracted data
    when: Transform needed
    then: Apply transformations

  - name: validate_data
    given: Extracted data
    when: Validation requested
    then: Check required fields

  - name: export_data
    given: Scraped data and format
    when: Export requested
    then: Return formatted output

  - name: handle_rate_limit
    given: Rate limit hit
    when: 429 response
    then: Apply backoff strategy

  - name: respect_robots_txt
    given: Target domain
    when: Check robots
    then: Return allowed paths
