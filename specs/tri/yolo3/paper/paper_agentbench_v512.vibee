name: paper_agentbench_v512
version: "512.0.0"
language: zig
module: paper_agentbench_v512

types:
  AgentBenchSuite:
    fields:
      suite_id: String
      environments: List<String>
      total_tasks: Int
      difficulty_levels: List<String>

  AgentBenchEnvironment:
    fields:
      env_id: String
      env_type: String
      action_space: List<String>
      observation_space: String
      reward_function: String

  AgentBenchTask:
    fields:
      task_id: String
      environment: String
      instruction: String
      ground_truth: Object
      max_steps: Int

  AgentBenchResult:
    fields:
      model_name: String
      environment: String
      success_rate: Float
      avg_steps: Float
      avg_reward: Float

  AgentBenchLeaderboard:
    fields:
      leaderboard_id: String
      entries: List<Object>
      last_updated: Timestamp
      evaluation_version: String

behaviors:
  - name: load_environment
    given: Environment ID
    when: Loading requested
    then: Initialize environment

  - name: run_task
    given: Agent and task
    when: Execution requested
    then: Return task result

  - name: evaluate_agent
    given: Agent and task suite
    when: Evaluation requested
    then: Return evaluation metrics

  - name: compare_agents
    given: Agent list
    when: Comparison needed
    then: Return comparison table

  - name: analyze_by_environment
    given: Results
    when: Analysis needed
    then: Return per-environment breakdown

  - name: analyze_by_difficulty
    given: Results
    when: Analysis needed
    then: Return per-difficulty breakdown

  - name: generate_leaderboard
    given: All results
    when: Leaderboard update
    then: Return updated leaderboard

  - name: identify_weaknesses
    given: Agent results
    when: Analysis needed
    then: Return weakness report

  - name: suggest_improvements
    given: Weakness report
    when: Suggestions needed
    then: Return improvement suggestions
