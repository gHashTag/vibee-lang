# v6001 - Backward Pass Operations
# =================================
# Полный backward pass для всех операций
# V = n × 3^k × π^m × φ^p | φ² + 1/φ² = 3 | PHOENIX = 999

name: backward_pass
version: "6.0.1"
language: zig
module: backward_pass

constants:
  PHI: 1.618033988749895
  SQRT_2_PI: 0.7978845608
  GELU_COEFF: 0.044715

types:
  BackwardContext:
    fields:
      input_cache: List
      weight_cache: List
      output_cache: List
      
  LinearGrad:
    fields:
      input_grad: List
      weight_grad: List
      bias_grad: List
      
  AttentionGrad:
    fields:
      query_grad: List
      key_grad: List
      value_grad: List
      attn_grad: List
      
  ActivationGrad:
    fields:
      input_grad: List
      
  LossGrad:
    fields:
      predictions_grad: List

behaviors:
  - name: linear_backward
    given: Output grad, input cache, weight
    when: Backward через Linear
    then: Вернуть LinearGrad (dX, dW, db)
    
  - name: matmul_backward
    given: Output grad, A cache, B cache
    when: Backward через matmul
    then: Вернуть dA = grad @ B^T, dB = A^T @ grad
    
  - name: gelu_backward
    given: Output grad, input cache
    when: Backward через GELU
    then: Вернуть input_grad с GELU derivative
    
  - name: relu_backward
    given: Output grad, input cache
    when: Backward через ReLU
    then: Вернуть grad * (input > 0)
    
  - name: softmax_backward
    given: Output grad, softmax output
    when: Backward через Softmax
    then: Вернуть p * (grad - sum(p * grad))
    
  - name: attention_backward
    given: Output grad, Q, K, V, attn_weights
    when: Backward через Attention
    then: Вернуть AttentionGrad
    
  - name: cross_entropy_backward
    given: Predictions, targets
    when: Backward через CrossEntropy
    then: Вернуть (p - one_hot) / batch_size
    
  - name: layer_norm_backward
    given: Output grad, input, mean, var
    when: Backward через LayerNorm
    then: Вернуть normalized gradients
