# LLM Provider Core Interface
# Unified interface for all LLM providers
# Integrates with Context Engineering system

name: llm_provider_core
version: "1.0.0"
language: zig
module: llm_provider_core

types:
  ProviderType:
    fields:
      provider_id: String
      name: String
      api_version: String
      supports_streaming: Bool
      supports_tools: Bool
      supports_vision: Bool
      max_context: Int

  ProviderCredentials:
    fields:
      api_key: String
      org_id: Option<String>
      base_url: String
      timeout_ms: Int

  ModelInfo:
    fields:
      model_id: String
      provider: String
      context_window: Int
      cost_input_per_1m: Float
      cost_output_per_1m: Float
      supports_json_mode: Bool

  ChatMessage:
    fields:
      role: String
      content: String
      name: Option<String>
      tool_calls: Option<String>

  ChatRequest:
    fields:
      model: String
      messages: List<String>
      temperature: Float
      max_tokens: Int
      stream: Bool
      tools: Option<String>

  ChatResponse:
    fields:
      response_id: String
      content: String
      model: String
      finish_reason: String
      usage_input: Int
      usage_output: Int
      latency_ms: Int

  StreamChunk:
    fields:
      chunk_id: String
      delta: String
      is_final: Bool

  ProviderError:
    fields:
      error_code: String
      message: String
      retryable: Bool
      retry_after_ms: Option<Int>

behaviors:
  - name: initialize_provider
    given: Provider type and credentials
    when: Provider client is created
    then: Returns configured provider ready for requests

  - name: validate_credentials
    given: Provider credentials
    when: Validation is requested
    then: Returns validation result with error details

  - name: send_chat_request
    given: Chat request with messages
    when: Request is sent to provider API
    then: Returns chat response with usage metrics

  - name: stream_chat_response
    given: Chat request with stream enabled
    when: Streaming response begins
    then: Yields stream chunks until completion

  - name: handle_rate_limit
    given: Rate limit error from provider
    when: Retry logic is triggered
    then: Returns retry strategy with backoff

  - name: estimate_cost
    given: Request tokens and model info
    when: Cost estimation requested
    then: Returns estimated cost in USD

  - name: select_optimal_model
    given: Task requirements and budget
    when: Model selection requested
    then: Returns best model for constraints

  - name: health_check
    given: Provider configuration
    when: Health check is triggered
    then: Returns provider availability status
