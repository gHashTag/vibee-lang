# iGLA Training Loss
# Cross-entropy and auxiliary losses
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_loss
version: "1.0.0"
language: zig
module: igla_training_loss

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  LossConfig:
    fields:
      loss_type: String
      label_smoothing: Float
      z_loss_weight: Float
      aux_loss_weight: Float

  LossOutput:
    fields:
      total_loss: Float
      lm_loss: Float
      aux_loss: Float
      z_loss: Float

  LossStats:
    fields:
      perplexity: Float
      bits_per_byte: Float
      accuracy: Float

  LossMetrics:
    fields:
      train_loss: Float
      val_loss: Float
      loss_variance: Float
      convergence_rate: Float

behaviors:
  - name: cross_entropy
    given: "Logits, labels"
    when: "CE loss"
    then: "Cross-entropy computed"

  - name: label_smoothing
    given: "Labels"
    when: "Smoothing"
    then: "Soft labels created"

  - name: z_loss
    given: "Logits"
    when: "Z-loss"
    then: "Logit regularization"

  - name: load_balance_loss
    given: "Router outputs"
    when: "MoE aux loss"
    then: "Expert balance loss"

  - name: compute_perplexity
    given: "Loss"
    when: "Perplexity"
    then: "exp(loss)"

  - name: aggregate_losses
    given: "All losses"
    when: "Aggregation"
    then: "Weighted sum"

  - name: phi_loss_harmony
    given: "Losses"
    when: "Harmony"
    then: "φ-weighted loss combination"
