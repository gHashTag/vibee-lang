# iGLA Self-RAG Training
# Fine-tuning critic models for Self-RAG
# Scientific basis: Asai et al. 2023
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_selfrag_training
version: "1.0.0"
language: zig
module: igla_selfrag_training

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  TrainingConfig:
    fields:
      learning_rate: Float
      batch_size: Int
      epochs: Int
      warmup_steps: Int

  TrainingExample:
    fields:
      query: String
      passage: String
      label: String
      score: Float

  TrainingBatch:
    fields:
      examples: String
      batch_id: Int

  ModelCheckpoint:
    fields:
      epoch: Int
      loss: Float
      path: String

  TrainingStats:
    fields:
      train_loss: Float
      val_loss: Float
      accuracy: Float

  Optimizer:
    fields:
      name: String
      params: String

behaviors:
  - name: prepare_dataset
    given: "Raw data"
    when: "Preparation"
    then: "Training dataset"

  - name: train_epoch
    given: "Model, data, config"
    when: "Training"
    then: "Updated model"

  - name: validate
    given: "Model, val_data"
    when: "Validation"
    then: "Validation metrics"

  - name: save_checkpoint
    given: "Model, path"
    when: "Checkpointing"
    then: "Checkpoint saved"

  - name: load_checkpoint
    given: "Path"
    when: "Loading"
    then: "Model restored"

  - name: compute_loss
    given: "Predictions, labels"
    when: "Loss computation"
    then: "Loss value"

  - name: update_weights
    given: "Gradients, optimizer"
    when: "Weight update"
    then: "Weights updated"

  - name: phi_learning_schedule
    given: "Total steps"
    when: "Sacred scheduling"
    then: "φ-ratio LR schedule"
