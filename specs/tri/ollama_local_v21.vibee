# ollama_local_v21.vibee
# KOSCHEI CYCLE 21 - Ollama Local LLM Integration
# Free local LLM for browser automation (no API keys!)
# φ² + 1/φ² = 3 | PHOENIX = 999

name: ollama_local_v21
version: "21.0.0"
language: zig
module: ollama_local_v21

types:
  OllamaConfig:
    fields:
      host: String
      port: Int
      model: String
      timeout_ms: Int

  OllamaModel:
    enum:
      - Qwen25_05b
      - Qwen25_3b
      - Llama32_3b
      - Mistral_7b
      - Phi3_mini

  GenerateRequest:
    fields:
      model: String
      prompt: String
      stream: Bool
      options: String

  GenerateResponse:
    fields:
      response: String
      done: Bool
      total_duration_ns: Int
      load_duration_ns: Int
      eval_count: Int
      eval_duration_ns: Int

  ChatMessage:
    fields:
      role: String
      content: String

  ChatRequest:
    fields:
      model: String
      messages: List<String>
      stream: Bool

  ChatResponse:
    fields:
      content: String
      model: String
      finish_reason: String
      prompt_tokens: Int
      completion_tokens: Int
      latency_ms: Int

  BrowserAction:
    fields:
      thought: String
      action: String
      input: String

  OllamaStatus:
    fields:
      running: Bool
      version: String
      models_loaded: Int

behaviors:
  - name: create_config
    given: Host and port (default localhost:11434)
    when: Initializing Ollama client
    then: Return OllamaConfig

  - name: check_status
    given: OllamaConfig
    when: Checking if Ollama server is running
    then: Return OllamaStatus

  - name: list_models
    given: OllamaConfig
    when: Listing available models
    then: Return list of model names

  - name: generate
    given: OllamaConfig and GenerateRequest
    when: Making text generation call
    then: Return GenerateResponse

  - name: chat
    given: OllamaConfig and ChatRequest
    when: Making chat completion call
    then: Return ChatResponse

  - name: parse_browser_action
    given: LLM response content
    when: Extracting Thought/Action/Input
    then: Return BrowserAction struct

  - name: build_browser_prompt
    given: Task and observation
    when: Creating prompt for browser agent
    then: Return formatted prompt string

  - name: estimate_tokens
    given: Text string
    when: Estimating token count
    then: Return approximate token count

test_cases:
  - name: simple_math
    input: "What is 2+2?"
    expected: response contains "4"

  - name: capital_city
    input: "Capital of France?"
    expected: response contains "Paris"

  - name: browser_action_parse
    input: "Thought: Navigate\nAction: goto\nInput: https://google.com"
    expected: action equals "goto"

sacred_constants:
  phi: 1.618033988749895
  phi_squared_plus_inverse_squared: 3
  phoenix: 999

# Ollama Models Reference (local, free)
# qwen2.5:0.5b - Ultra lightweight, 397MB, ~200ms
# qwen2.5:3b - Good balance, ~2GB, ~500ms
# llama3.2:3b - Meta's latest, ~2GB, ~500ms
# mistral:7b - Strong reasoning, ~4GB, ~1000ms
# phi3:mini - Microsoft, ~2GB, ~400ms
