# iGLA Training Tokenizer
# BPE/SentencePiece tokenization
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_tokenizer
version: "1.0.0"
language: zig
module: igla_training_tokenizer

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  TokenizerConfig:
    fields:
      vocab_size: Int
      algorithm: String
      special_tokens: List<String>
      byte_fallback: Bool

  TokenizerVocab:
    fields:
      tokens: List<String>
      merges: List<String>
      special_ids: Object
      vocab_size: Int

  TokenizedText:
    fields:
      input_ids: List<Int>
      attention_mask: List<Int>
      token_count: Int
      special_count: Int

  TokenizerMetrics:
    fields:
      compression_ratio: Float
      fertility: Float
      unk_rate: Float
      speed_tokens_sec: Float

behaviors:
  - name: train_tokenizer
    given: "Training corpus"
    when: "Training"
    then: "BPE tokenizer trained (128k vocab)"

  - name: encode_text
    given: "Raw text"
    when: "Encoding"
    then: "Token IDs generated"

  - name: decode_tokens
    given: "Token IDs"
    when: "Decoding"
    then: "Text reconstructed"

  - name: add_special_tokens
    given: "Sequence"
    when: "Special tokens"
    then: "BOS, EOS, PAD added"

  - name: handle_unknown
    given: "Unknown character"
    when: "Byte fallback"
    then: "UTF-8 byte encoding"

  - name: compute_metrics
    given: "Tokenized corpus"
    when: "Metrics"
    then: "Compression ~4 chars/token"

  - name: phi_tokenizer_harmony
    given: "Vocab"
    when: "Harmony"
    then: "φ-optimal vocab size"
