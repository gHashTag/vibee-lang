name: v6767_differential_attention
version: "1.0.0"
language: zig
module: v6767_differential_attention

types:
  DiffAttnConfig:
    fields:
      num_heads: Int
      lambda_init: Float

behaviors:
  - name: differential_softmax
    given: Two attention maps
    when: Compute difference
    then: Return A1 minus lambda times A2

  - name: compute_lambda
    given: Layer index
    when: Compute learnable lambda
    then: Return scaling factor

  - name: noise_cancellation
    given: Attention scores
    when: Cancel noise via differential
    then: Return cleaner attention
