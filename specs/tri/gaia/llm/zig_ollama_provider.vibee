name: zig_ollama_provider
version: "1.0.0"
language: zig
module: ollama_provider

description: |
  Ollama Provider - Local, FREE, no API key needed.
  Run models locally: Llama, Phi, Qwen, Mistral.
  OpenAI-compatible API at localhost:11434.
  φ² + 1/φ² = 3

constants:
  OLLAMA_API_URL: "http://localhost:11434/api/chat"
  OLLAMA_OPENAI_URL: "http://localhost:11434/v1/chat/completions"
  LLAMA32_3B: "llama3.2:3b"
  LLAMA31_8B: "llama3.1:8b"
  PHI3_MINI: "phi3:mini"
  QWEN25_7B: "qwen2.5:7b"
  MISTRAL_7B: "mistral:7b"
  DEFAULT_MODEL: "llama3.2:3b"

types:
  OllamaConfig:
    fields:
      host: String
      port: Int
      model: String
      use_openai_compat: Bool

  OllamaResponse:
    fields:
      content: String
      done: Bool
      total_duration_ns: Int
      eval_count: Int

behaviors:
  - name: init
    given: OllamaConfig
    when: Creating Ollama provider
    then: Returns initialized provider

  - name: complete
    given: Messages array
    when: Calling Ollama chat API
    then: Returns response with content

  - name: list_models
    given: Ollama instance
    when: Getting available models
    then: Returns list of installed models

  - name: pull_model
    given: Model name
    when: Downloading model
    then: Downloads and returns success
