name: zig_groq_provider
version: "1.0.0"
language: zig
module: groq_provider

description: |
  Groq Provider - FREE and FAST LLM API.
  OpenAI-compatible API at api.groq.com.
  Models: Llama 3.3 70B, Llama 3.1 8B, Qwen3-32B.
  φ² + 1/φ² = 3

constants:
  GROQ_API_URL: "https://api.groq.com/openai/v1/chat/completions"
  LLAMA_33_70B: "llama-3.3-70b-versatile"
  LLAMA_31_8B: "llama-3.1-8b-instant"
  QWEN3_32B: "qwen/qwen3-32b"
  DEFAULT_MODEL: "llama-3.3-70b-versatile"

types:
  GroqConfig:
    fields:
      api_key: String
      model: String
      max_tokens: Int
      temperature: Float

  GroqResponse:
    fields:
      content: String
      finish_reason: String
      prompt_tokens: Int
      completion_tokens: Int
      total_tokens: Int
      latency_ms: Int
      model: String

behaviors:
  - name: init
    given: GroqConfig
    when: Creating Groq provider
    then: Returns initialized provider

  - name: complete
    given: Messages array
    when: Calling Groq chat completions API
    then: Returns response with content

  - name: complete_with_system
    given: System prompt and user message
    when: Calling with system context
    then: Returns response

  - name: set_model
    given: Model name string
    when: Changing model
    then: Updates provider model
