name: zig_llm_client
version: "1.0.0"
language: zig
module: llm_client

description: |
  Universal LLM Client - Provider-agnostic interface.
  Supports OpenAI, Claude, and local models.
  For GAIA/WebArena agent integration.
  φ² + 1/φ² = 3

constants:
  DEFAULT_MAX_TOKENS: 4096
  DEFAULT_TEMPERATURE: 0.7
  DEFAULT_TIMEOUT_MS: 60000

types:
  LLMProvider:
    enum:
      - openai
      - claude
      - local
      - ollama

  LLMMessage:
    fields:
      role: String
      content: String

  LLMConfig:
    fields:
      provider: LLMProvider
      api_key: String
      model: String
      max_tokens: Int
      temperature: Float
      timeout_ms: Int

  LLMResponse:
    fields:
      content: String
      finish_reason: String
      prompt_tokens: Int
      completion_tokens: Int
      total_tokens: Int
      latency_ms: Int
      model: String

  LLMError:
    enum:
      - api_error
      - rate_limited
      - invalid_key
      - network_error
      - parse_error
      - timeout

behaviors:
  - name: create_client
    given: LLMConfig
    when: Initializing LLM client
    then: Returns configured client instance

  - name: chat
    given: List of LLMMessage
    when: Sending chat completion request
    then: Returns LLMResponse

  - name: chat_with_system
    given: System prompt and user message
    when: Sending chat with system context
    then: Returns LLMResponse

  - name: stream_chat
    given: List of LLMMessage and callback
    when: Streaming response tokens
    then: Calls callback for each token

  - name: count_tokens
    given: Text string
    when: Estimating token count
    then: Returns approximate token count
