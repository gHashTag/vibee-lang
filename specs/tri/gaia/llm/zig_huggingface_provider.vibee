name: zig_huggingface_provider
version: "1.0.0"
language: zig
module: huggingface_provider

description: |
  HuggingFace Inference Provider - FREE models via HF Router.
  OpenAI-compatible API at router.huggingface.co.
  Models: GPT-OSS 120B, DeepSeek R1, Llama 3.3 70B.
  φ² + 1/φ² = 3

constants:
  HF_ROUTER_URL: "https://router.huggingface.co/v1/chat/completions"
  GPT_OSS_120B: "openai/gpt-oss-120b"
  DEEPSEEK_R1: "deepseek-ai/DeepSeek-R1"
  LLAMA_33_70B: "meta-llama/Llama-3.3-70B-Instruct"
  QWEN_25_72B: "Qwen/Qwen2.5-72B-Instruct"
  DEFAULT_MODEL: "openai/gpt-oss-120b"

types:
  HFConfig:
    fields:
      api_key: String
      model: String
      max_tokens: Int
      temperature: Float
      provider_policy: String

  HFResponse:
    fields:
      content: String
      finish_reason: String
      prompt_tokens: Int
      completion_tokens: Int
      total_tokens: Int
      latency_ms: Int
      model: String
      provider: String

behaviors:
  - name: init
    given: HFConfig
    when: Creating HuggingFace provider
    then: Returns initialized provider

  - name: complete
    given: Messages array
    when: Calling HF Router chat completions
    then: Returns response with content

  - name: complete_fastest
    given: Messages array
    when: Using fastest provider policy
    then: Returns response from fastest provider

  - name: complete_cheapest
    given: Messages array
    when: Using cheapest provider policy
    then: Returns response from cheapest provider
