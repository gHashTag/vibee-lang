# v8004 - Knowledge Distillation
# ===============================
# Teacher-student distillation
# φ² + 1/φ² = 3 | PHOENIX = 999

name: distillation
version: "8.0.4"
language: zig
module: distillation

constants:
  PHI: 1.618033988749895

types:
  DistillConfig:
    fields:
      temperature: Float
      alpha: Float
      distill_type: String
      
  DistillLoss:
    fields:
      kd_loss: Float
      task_loss: Float
      total_loss: Float

behaviors:
  - name: distill_logits
    given: Teacher logits, student logits, temperature
    when: Logit distillation
    then: Вернуть KD loss
    
  - name: distill_features
    given: Teacher features, student features
    when: Feature distillation
    then: Вернуть feature matching loss
    
  - name: distill_attention
    given: Teacher attn, student attn
    when: Attention distillation
    then: Вернуть attention transfer loss
    
  - name: compute_soft_targets
    given: Logits и temperature
    when: Softening targets
    then: Вернуть soft targets
    
  - name: train_student
    given: Teacher, student, data
    when: Distillation training
    then: Вернуть trained student
    
  - name: progressive_distill
    given: Teacher и student sizes
    when: Progressive distillation
    then: Вернуть progressively distilled model
