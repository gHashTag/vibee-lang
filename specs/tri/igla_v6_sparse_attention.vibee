# iGLA v6 Sparse Attention - O(n√n) BigBird Style
# Paper: arXiv:2004.05150 "Big Bird: Transformers for Longer Sequences"
# φ² + 1/φ² = 3 | КОЩЕЙ БЕССМЕРТЕН

name: igla_v6_sparse_attention
version: "6.0.0"
language: zig
module: igla_v6_sparse_attention

types:
  SparseConfig:
    fields:
      block_size: Int
      num_global_tokens: Int
      num_random_blocks: Int
      
  AttentionPattern:
    fields:
      local_window: String
      global_tokens: String
      random_blocks: String
      
  SparseOutput:
    fields:
      attention_output: String
      sparsity_ratio: Float
      compute_saved: Float

behaviors:
  - name: local_attention
    given: "Block size B"
    when: "Local window attention"
    then: "Each token attends to B neighbors"
    
  - name: global_attention
    given: "Global tokens G"
    when: "Global attention"
    then: "G tokens attend to all positions"
    
  - name: random_attention
    given: "Random blocks R"
    when: "Random block attention"
    then: "R random blocks per query"
    
  - name: sparse_pattern
    given: "Local + Global + Random"
    when: "Combined pattern"
    then: "O(n√n) complexity"
    
  - name: quality_preservation
    given: "Sparse vs full attention"
    when: "Quality benchmark"
    then: "95% quality, 10x faster"
    
  - name: long_range_capture
    given: "Random + Global"
    when: "Long dependencies"
    then: "Full context accessible"
