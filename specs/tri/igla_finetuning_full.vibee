# iGLA Finetuning Full
# Full parameter fine-tuning
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_finetuning_full
version: "1.0.0"
language: zig
module: igla_finetuning_full

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  FullFinetuneConfig:
    fields:
      learning_rate: Float
      epochs: Int
      warmup_ratio: Float
      weight_decay: Float
      gradient_checkpointing: Bool

  FinetuneDataset:
    fields:
      train_samples: Int
      val_samples: Int
      format: String
      max_length: Int

  FinetuneState:
    fields:
      current_epoch: Int
      global_step: Int
      best_loss: Float
      early_stop: Bool

  FinetuneMetrics:
    fields:
      train_loss: Float
      val_loss: Float
      perplexity: Float
      task_accuracy: Float

behaviors:
  - name: prepare_data
    given: "Raw dataset"
    when: "Preparation"
    then: "Instruction format applied"

  - name: full_finetune
    given: "Base model"
    when: "Full fine-tuning"
    then: "All parameters updated"

  - name: gradient_checkpoint
    given: "Forward pass"
    when: "Checkpointing"
    then: "Memory-efficient training"

  - name: evaluate
    given: "Val set"
    when: "Evaluation"
    then: "Validation metrics computed"

  - name: early_stopping
    given: "Val loss"
    when: "Early stop check"
    then: "Stop if no improvement"

  - name: save_best
    given: "Best model"
    when: "Saving"
    then: "Best checkpoint saved"

  - name: phi_finetune_harmony
    given: "Fine-tuning"
    when: "Harmony"
    then: "φ-optimal learning rate"
