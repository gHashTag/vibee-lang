# iGLA RAG LLM Integration
# Integration of RAG with LLM inference server
# Scientific basis: Lewis et al. 2020, Guu et al. 2020 (REALM)
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_rag_llm_integration
version: "1.0.0"
language: zig
module: igla_rag_llm_integration

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

scientific_references:
  - "Lewis et al. 2020: RAG"
  - "Guu et al. 2020: REALM"
  - "Izacard & Grave 2021: FiD"
  - "Shi et al. 2023: REPLUG"

types:
  LLMConfig:
    fields:
      endpoint: String
      model: String
      max_tokens: Int
      temperature: Float
      api_key: String

  RAGRequest:
    fields:
      query: String
      top_k: Int
      include_sources: Bool
      stream: Bool

  RAGResponse:
    fields:
      answer: String
      sources: String
      latency_ms: Float
      tokens_used: Int

  PromptTemplate:
    fields:
      system: String
      context_prefix: String
      query_prefix: String
      answer_prefix: String

  ContextWindow:
    fields:
      max_tokens: Int
      reserved_for_answer: Int
      context_budget: Int

  GenerationConfig:
    fields:
      temperature: Float
      top_p: Float
      frequency_penalty: Float
      presence_penalty: Float

behaviors:
  - name: build_rag_prompt
    given: "Query, retrieved chunks"
    when: "Prompt construction"
    then: "Complete RAG prompt"

  - name: call_llm
    given: "Prompt, config"
    when: "LLM inference"
    then: "Generated response"

  - name: call_llm_streaming
    given: "Prompt, config"
    when: "Streaming inference"
    then: "Token stream"

  - name: fit_context
    given: "Chunks, max_tokens"
    when: "Context fitting"
    then: "Chunks within budget"

  - name: format_sources
    given: "Retrieved chunks"
    when: "Source formatting"
    then: "Formatted source citations"

  - name: validate_response
    given: "Response, context"
    when: "Validation"
    then: "Hallucination check result"

  - name: retry_with_backoff
    given: "Failed request"
    when: "Retry logic"
    then: "Retried response"

  - name: phi_prompt_optimization
    given: "Prompt"
    when: "Sacred optimization"
    then: "φ-ratio optimized prompt"
