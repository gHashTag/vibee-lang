# iGLA Finetuning DoRA
# Weight-Decomposed Low-Rank Adaptation
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_finetuning_dora
version: "1.0.0"
language: zig
module: igla_finetuning_dora

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  DoRAConfig:
    fields:
      r: Int
      alpha: Float
      dropout: Float
      target_modules: List<String>
      magnitude_vector: Bool

  DoRALayer:
    fields:
      lora_A: List<Float>
      lora_B: List<Float>
      magnitude: List<Float>
      direction: List<Float>

  DoRAState:
    fields:
      trainable_params: Int
      magnitude_params: Int
      direction_params: Int

  DoRAMetrics:
    fields:
      magnitude_change: Float
      direction_change: Float
      quality_improvement: Float
      vs_lora_gain: Float

behaviors:
  - name: decompose_weight
    given: "Weight matrix"
    when: "Decomposition"
    then: "Magnitude and direction separated"

  - name: inject_dora
    given: "Base model"
    when: "DoRA injection"
    then: "DoRA adapters added"

  - name: forward_dora
    given: "Input"
    when: "Forward pass"
    then: "m * (W + BA) / ||W + BA||"

  - name: train_magnitude
    given: "Magnitude vector"
    when: "Training"
    then: "Magnitude learned separately"

  - name: merge_dora
    given: "Trained DoRA"
    when: "Merging"
    then: "DoRA merged into base"

  - name: compare_lora
    given: "DoRA vs LoRA"
    when: "Comparison"
    then: "DoRA typically +1-2% better"

  - name: phi_dora_harmony
    given: "DoRA"
    when: "Harmony"
    then: "φ-decomposed adaptation"
