# VIBEE Specification - IGLA CLIP Embeddings
# CLIP image and text embeddings for multi-modal RAG
# RAG v3 - Multi-Modal
# φ² + 1/φ² = 3 | PHOENIX = 999

name: igla_clip_embeddings
version: "3.0.0"
language: zig
module: igla_clip_embeddings

types:
  CLIPModel:
    fields:
      id: String
      vision_model_path: String
      text_model_path: String
      embedding_dim: Int
      loaded: Bool

  ImageInput:
    fields:
      data: String
      width: Int
      height: Int
      channels: Int
      format: String

  ProcessedImage:
    fields:
      tensor: String
      original_size: String
      normalized: Bool

  CLIPEmbedding:
    fields:
      vector: String
      dimension: Int
      modality: String
      normalized: Bool

  ImageTextPair:
    fields:
      image_embedding: String
      text_embedding: String
      similarity: Float

  CLIPConfig:
    fields:
      image_size: Int
      patch_size: Int
      use_gpu: Bool
      batch_size: Int

  CLIPMetrics:
    fields:
      images_processed: Int
      texts_processed: Int
      avg_image_latency_ms: Float
      avg_text_latency_ms: Float

behaviors:
  - name: load_model
    given: Model paths
    when: Model loading
    then: CLIP model ready

  - name: preprocess_image
    given: Raw image
    when: Preprocessing
    then: Normalized tensor returned

  - name: embed_image
    given: Image input
    when: Image embedding
    then: 512-dim vector returned

  - name: embed_text
    given: Text input
    when: Text embedding
    then: 512-dim vector returned

  - name: compute_similarity
    given: Image and text embeddings
    when: Similarity computed
    then: Cosine similarity score

  - name: rank_images
    given: Text query and images
    when: Ranking requested
    then: Images ranked by relevance

  - name: rank_texts
    given: Image query and texts
    when: Ranking requested
    then: Texts ranked by relevance

  - name: embed_batch_images
    given: Image array
    when: Batch embedding
    then: Embedding batch returned

  - name: embed_batch_texts
    given: Text array
    when: Batch embedding
    then: Embedding batch returned

  - name: get_metrics
    given: Model
    when: Metrics requested
    then: CLIP metrics returned

test_cases:
  - name: test_load_model
    input: { path: "models/clip" }
    expected: { loaded: true, dim: 512 }

  - name: test_preprocess
    input: { width: 224, height: 224 }
    expected: { normalized: true }

  - name: test_embed_image
    input: { image: "test.png" }
    expected: { dimension: 512 }

  - name: test_embed_text
    input: { text: "a photo of a cat" }
    expected: { dimension: 512 }

  - name: test_similarity
    input: { image: "cat.png", text: "cat" }
    expected: { score_range: [0.0, 1.0] }

  - name: test_rank
    input: { query: "dog", images: ["a.png", "b.png"] }
    expected: { ranked: true }

  - name: test_metrics
    input: {}
    expected: { images_processed: 0 }
