name: paper_seeact_v350
version: "1.0.0"
language: zig
module: paper_seeact_v350

types:
  SeeActObservation:
    fields:
      screenshot: String
      html: String
      accessibility_tree: String
      url: String

  SeeActAction:
    fields:
      action_type: String
      element_description: String
      value: Option<String>
      reasoning: String

  SeeActGrounding:
    fields:
      description: String
      candidates: List<Object>
      selected: Object
      confidence: Float

  SeeActTask:
    fields:
      instruction: String
      website: String
      max_steps: Int
      success_criteria: String

  SeeActConfig:
    fields:
      model: String
      grounding_model: String
      screenshot_only: Bool

behaviors:
  - name: observe_with_vision
    given: Page loaded
    When: Observation runs
    then: Screenshot analyzed by GPT-4V

  - name: predict_action
    given: Observation and task
    When: Prediction runs
    then: Action with reasoning

  - name: ground_element
    given: Element description
    When: Grounding runs
    then: Element located on page

  - name: execute_action
    given: Grounded action
    When: Execution runs
    then: Action performed

  - name: evaluate_seeact
    given: Task suite
    When: Evaluation runs
    then: SeeAct performance measured

  - name: compare_modalities
    given: HTML vs screenshot
    When: Comparison runs
    then: Modality effectiveness compared

  - name: apply_to_vibee
    given: VIBEE browser
    When: SeeAct applied
    then: Vision-based automation

  - name: benchmark_websites
    given: Website suite
    When: Benchmark runs
    then: Cross-website performance
