name: paper_agentbench_v353
version: "1.0.0"
language: zig
module: paper_agentbench_v353

types:
  AgentBenchTask:
    fields:
      task_id: String
      environment: String
      instruction: String
      difficulty: String

  AgentBenchEnv:
    fields:
      name: String
      type: String
      action_space: List<String>
      observation_space: String

  AgentBenchResult:
    fields:
      environment: String
      success_rate: Float
      avg_reward: Float
      avg_steps: Float

  AgentBenchSuite:
    fields:
      environments: List<AgentBenchEnv>
      total_tasks: Int
      categories: List<String>

  AgentBenchConfig:
    fields:
      environments: List<String>
      max_steps: Int
      parallel: Bool

behaviors:
  - name: setup_environment
    given: Env config
    When: Setup runs
    then: Environment initialized

  - name: run_episode
    given: Task and agent
    When: Episode runs
    then: Agent interacts with env

  - name: evaluate_episode
    given: Episode complete
    When: Evaluation runs
    then: Reward and success computed

  - name: aggregate_results
    given: All episodes
    When: Aggregation runs
    then: Overall metrics computed

  - name: compare_models
    given: Multiple models
    When: Comparison runs
    then: Model rankings produced

  - name: analyze_by_category
    given: Results
    When: Analysis runs
    then: Category breakdown

  - name: benchmark_vibee
    given: VIBEE agent
    When: Benchmark runs
    then: VIBEE on AgentBench

  - name: submit_results
    given: Benchmark complete
    When: Submission runs
    then: Results submitted
