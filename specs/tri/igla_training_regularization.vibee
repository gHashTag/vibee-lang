# iGLA Training Regularization
# Dropout and weight decay
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_regularization
version: "1.0.0"
language: zig
module: igla_training_regularization

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  RegularizationConfig:
    fields:
      dropout_rate: Float
      attention_dropout: Float
      hidden_dropout: Float
      weight_decay: Float

  DropoutState:
    fields:
      training: Bool
      dropout_mask: List<Bool>
      keep_prob: Float

  RegularizationOutput:
    fields:
      regularized: List<Float>
      l2_penalty: Float
      dropout_applied: Bool

  RegularizationMetrics:
    fields:
      effective_dropout: Float
      weight_norm: Float
      sparsity: Float

behaviors:
  - name: apply_dropout
    given: "Hidden states"
    when: "Dropout"
    then: "Random dropout applied"

  - name: attention_dropout
    given: "Attention weights"
    when: "Attention dropout"
    then: "Attention dropout applied"

  - name: compute_weight_decay
    given: "Parameters"
    when: "Weight decay"
    then: "L2 penalty computed"

  - name: apply_l2
    given: "Gradients"
    when: "L2 regularization"
    then: "Weight decay added"

  - name: disable_dropout
    given: "Model"
    when: "Eval mode"
    then: "Dropout disabled"

  - name: stochastic_depth
    given: "Layer"
    when: "Stochastic depth"
    then: "Random layer skip"

  - name: phi_regularization_harmony
    given: "Regularization"
    when: "Harmony"
    then: "φ-optimal dropout rate"
