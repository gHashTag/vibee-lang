# GAIA Benchmark Specification
# General AI Assistants evaluation
# Multi-level difficulty with complex reasoning

name: benchmark_gaia
version: "1.0.0"
language: zig
module: benchmark_gaia

types:
  GAIATask:
    fields:
      task_id: String
      level: Int
      question: String
      required_tools: List<String>
      expected_answer: String

  DifficultyLevel:
    fields:
      level_number: Int
      description: String
      avg_steps_required: Int
      tool_complexity: String

  ToolUsage:
    fields:
      tool_name: String
      invocation_count: Int
      success_rate: Float
      avg_latency_ms: Int

  GAIAResult:
    fields:
      task_id: String
      agent_answer: String
      is_correct: Bool
      reasoning_trace: String
      tools_used: List<String>

  LevelScore:
    fields:
      level: Int
      tasks_attempted: Int
      tasks_correct: Int
      accuracy: Float

behaviors:
  - name: load_level_tasks
    given: GAIA level specification (1, 2, or 3)
    when: Task loader filters by difficulty
    then: Returns tasks for specified level

  - name: evaluate_answer
    given: Agent answer and ground truth
    when: Answer comparison runs
    then: Returns correctness with explanation

  - name: track_tool_usage
    given: Agent execution trace
    when: Tool invocations are analyzed
    then: Returns tool usage statistics

  - name: compute_level_score
    given: All results for a specific level
    when: Level aggregation runs
    then: Returns accuracy for that difficulty tier

  - name: assess_reasoning_quality
    given: Agent reasoning trace
    when: Reasoning analysis runs
    then: Returns quality metrics for thought process

  - name: benchmark_against_sota
    given: Agent scores across all levels
    when: Comparison with top performers
    then: Returns ranking and gap analysis
