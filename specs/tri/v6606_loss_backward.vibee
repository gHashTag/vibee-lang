# v6606 - Loss Functions with Backward
# ======================================
# Cross-entropy и другие loss с градиентами
# CE = -log(p[target])
# dCE/dp[i] = p[i] - 1{i=target}
# φ² + 1/φ² = 3 | PHOENIX = 999

name: loss_backward
version: "6.6.6"
language: zig
module: loss_backward

constants:
  PHI: 1.618033988749895
  LOG_EPSILON: 1e-10

types:
  LossResult:
    fields:
      loss: Float
      grad: List
      
  SoftmaxOutput:
    fields:
      probs: List
      
  CEConfig:
    fields:
      reduction: String
      label_smoothing: Float

behaviors:
  - name: softmax_forward
    given: Logits, size
    when: Softmax computation
    then: Вернуть normalized probabilities
    
  - name: cross_entropy_loss
    given: Probs, target, num_classes
    when: CE loss computation
    then: Вернуть -log(probs[target])
    
  - name: cross_entropy_backward
    given: Probs, target
    when: CE gradient
    then: Вернуть probs - one_hot(target)
    
  - name: batch_cross_entropy
    given: Batch_probs, targets, batch_size
    when: Batched CE
    then: Вернуть mean loss over batch
    
  - name: batch_ce_backward
    given: Batch_probs, targets, batch_size
    when: Batched CE gradient
    then: Вернуть scaled gradients
    
  - name: mse_loss
    given: Predictions, targets
    when: MSE computation
    then: Вернуть mean squared error
    
  - name: mse_backward
    given: Predictions, targets
    when: MSE gradient
    then: Вернуть 2 * (pred - target) / n
