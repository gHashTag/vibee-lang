name: value_learning_v11500
version: "11500"
language: zig
module: value_learning

description: |
  TIER 234: Value Learning
  Learns human values from demonstrations and preferences
  Based on: Ng & Russell IRL, Hadfield-Menell CIRL

types:
  ValueConfig:
    fields:
      learning_method: ValueMethod
      preference_model: PreferenceModel
      value_uncertainty: Bool
      human_in_loop: Bool
      value_extrapolation: Bool

  ValueMethod:
    variants:
      - inverse_rl
      - preference_learning
      - cooperative_irl
      - reward_modeling
      - value_extrapolation

  PreferenceModel:
    variants:
      - bradley_terry
      - thurstone
      - plackett_luce
      - gaussian_process

  LearnedValues:
    fields:
      reward_weights: List<Float>
      value_function: String
      uncertainty: Float
      human_approval: Float
      extrapolation_confidence: Float

  ValueAlignment:
    fields:
      alignment_score: Float
      value_conflicts: List<String>
      resolution_strategy: String
      human_override_needed: Bool

behaviors:
  - name: learn_from_demonstrations
    given: Expert demonstrations
    when: Running inverse RL
    then: Returns inferred reward function

  - name: learn_from_preferences
    given: Pairwise comparisons
    when: Learning preference model
    then: Returns learned reward model

  - name: cooperative_value_learning
    given: Human-AI interaction data
    when: Running CIRL
    then: Returns jointly learned values

  - name: extrapolate_values
    given: Learned values and new context
    when: Generalizing values
    then: Returns extrapolated value judgments

  - name: detect_value_conflicts
    given: Multiple value sources
    when: Checking consistency
    then: Returns detected conflicts

  - name: resolve_value_uncertainty
    given: Uncertain value estimate
    when: Seeking clarification
    then: Returns clarified values

  - name: compute_value_alignment
    given: Agent behavior and learned values
    when: Measuring alignment
    then: Returns alignment score

  - name: update_values_online
    given: New human feedback
    when: Updating value model
    then: Returns updated values

creation_pattern:
  source: HumanFeedback
  transformer: ValueLearner
  result: LearnedValues

test_cases:
  - name: test_inverse_rl
    input: { method: inverse_rl }
    expected: { learns_reward: true }

  - name: test_preference_learning
    input: { model: bradley_terry }
    expected: { learns_preferences: true }

  - name: test_value_extrapolation
    input: { extrapolation_enabled: true }
    expected: { extrapolates: true }
