# IGLA SWE-bench Reporter
# Generates reports and visualizations for benchmark results
# φ² + 1/φ² = 3 | PHOENIX = 999

name: igla_swe_bench_reporter
version: "1.0.0"
language: zig
module: igla_swe_bench_reporter

sacred_constants:
  phi: 1.618033988749895
  phoenix: 999
  trinity: 3

creation_pattern:
  source: EvaluationReport
  transformer: Reporter
  result: FormattedReport

types:
  ReportFormat:
    enum:
      - JSON
      - Markdown
      - HTML
      - CSV
      - Console

  ReportConfig:
    fields:
      format: String
      include_details: Bool
      include_charts: Bool
      output_path: String

  SummaryStats:
    fields:
      total_instances: Int
      resolved: Int
      resolved_pct: Float
      avg_duration_ms: Int
      median_duration_ms: Int
      p95_duration_ms: Int

  RepoBreakdown:
    fields:
      repo_name: String
      total: Int
      resolved: Int
      resolved_pct: Float

  VersionBreakdown:
    fields:
      version: String
      total: Int
      resolved: Int
      resolved_pct: Float

  FormattedReport:
    fields:
      content: String
      format: String
      generated_at: String
      file_path: String

  ProgressUpdate:
    fields:
      completed: Int
      total: Int
      current_instance: String
      elapsed_seconds: Int
      eta_seconds: Int

  CompetitorComparison:
    fields:
      model_name: String
      our_score: Float
      competitor_score: Float
      difference: Float

  TrendData:
    fields:
      run_id: String
      timestamp: String
      resolved_pct: Float

behaviors:
  - name: generate_summary
    given: "EvaluationReport"
    when: "Summary requested"
    then: "Returns SummaryStats"

  - name: generate_repo_breakdown
    given: "EvaluationReport"
    when: "Repo breakdown requested"
    then: "Returns list of RepoBreakdown"

  - name: generate_version_breakdown
    given: "EvaluationReport"
    when: "Version breakdown requested"
    then: "Returns list of VersionBreakdown"

  - name: format_as_markdown
    given: "EvaluationReport"
    when: "Markdown format requested"
    then: "Returns FormattedReport with markdown content"

  - name: format_as_json
    given: "EvaluationReport"
    when: "JSON format requested"
    then: "Returns FormattedReport with JSON content"

  - name: format_as_csv
    given: "EvaluationReport"
    when: "CSV format requested"
    then: "Returns FormattedReport with CSV content"

  - name: format_as_console
    given: "EvaluationReport"
    when: "Console format requested"
    then: "Returns FormattedReport with ANSI-colored output"

  - name: print_progress
    given: "ProgressUpdate"
    when: "Progress display requested"
    then: "Prints progress bar to console"

  - name: compare_to_competitors
    given: "EvaluationReport and competitor scores"
    when: "Comparison requested"
    then: "Returns list of CompetitorComparison"

  - name: generate_trend_chart
    given: "List of TrendData"
    when: "Trend visualization requested"
    then: "Returns ASCII chart or chart data"

  - name: save_report
    given: "FormattedReport and output path"
    when: "Save requested"
    then: "Writes report to file"

  - name: generate_github_annotation
    given: "EvaluationReport"
    when: "GitHub annotation format requested"
    then: "Returns GitHub Actions compatible annotations"

  - name: generate_slack_message
    given: "EvaluationReport"
    when: "Slack notification requested"
    then: "Returns Slack-formatted message"

  - name: calculate_percentiles
    given: "List of durations"
    when: "Percentile calculation requested"
    then: "Returns p50, p90, p95, p99"

test_cases:
  - name: test_summary_generation
    input:
      total: 300
      resolved: 45
    expected:
      resolved_pct: 15.0

  - name: test_markdown_format
    input:
      format: "Markdown"
    expected:
      contains_header: true

  - name: test_repo_breakdown
    input:
      repos: ["django/django", "sqlfluff/sqlfluff"]
    expected:
      breakdown_count: 2

  - name: test_progress_display
    input:
      completed: 50
      total: 100
    expected:
      percentage: 50

  - name: test_competitor_comparison
    input:
      our_score: 15.0
      competitor: 12.0
    expected:
      difference: 3.0

  - name: test_percentile_calculation
    input:
      durations: [100, 200, 300, 400, 500]
    expected:
      p50: 300
