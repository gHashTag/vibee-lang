# iGLA Training Gradient
# Gradient accumulation and clipping
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_gradient
version: "1.0.0"
language: zig
module: igla_training_gradient

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  GradientConfig:
    fields:
      accumulation_steps: Int
      max_grad_norm: Float
      clip_type: String
      sync_gradients: Bool

  GradientState:
    fields:
      accumulated_steps: Int
      accumulated_grads: List<Float>
      grad_norm: Float

  GradientStats:
    fields:
      grad_norm: Float
      max_grad: Float
      min_grad: Float
      sparsity: Float

  GradientMetrics:
    fields:
      effective_batch_size: Int
      clip_ratio: Float
      overflow_rate: Float
      gradient_variance: Float

behaviors:
  - name: accumulate_gradients
    given: "Micro-batch gradients"
    when: "Accumulation"
    then: "Gradients accumulated"

  - name: clip_by_norm
    given: "Gradients"
    when: "Norm clipping"
    then: "Global norm clipped to max"

  - name: clip_by_value
    given: "Gradients"
    when: "Value clipping"
    then: "Element-wise clipping"

  - name: compute_grad_norm
    given: "Gradients"
    when: "Norm computation"
    then: "L2 norm computed"

  - name: sync_gradients
    given: "Distributed gradients"
    when: "Synchronization"
    then: "All-reduce across ranks"

  - name: zero_gradients
    given: "Model"
    when: "Reset"
    then: "Gradients zeroed"

  - name: phi_gradient_harmony
    given: "Gradients"
    when: "Harmony"
    then: "φ-based accumulation steps"
