# iGLA Model 13B
# 13 billion parameter architecture
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_model_13b
version: "1.0.0"
language: zig
module: igla_model_13b

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  Model13BConfig:
    fields:
      hidden_size: Int
      num_layers: Int
      num_heads: Int
      num_kv_heads: Int
      intermediate_size: Int
      vocab_size: Int
      max_position: Int

  Model13BArchitecture:
    fields:
      params_billions: Float
      flops_per_token: Float
      memory_fp16_gb: Float
      memory_int4_gb: Float

  Model13BTraining:
    fields:
      tokens_chinchilla: String
      compute_flops: String
      gpu_hours_a100: Int
      estimated_cost: Float

  Model13BMetrics:
    fields:
      humaneval: Float
      mmlu: Float
      gsm8k: Float
      hellaswag: Float

behaviors:
  - name: define_13b
    given: "Config"
    when: "Architecture definition"
    then: "hidden=5120, layers=40, heads=40"

  - name: compute_params
    given: "Architecture"
    when: "Param count"
    then: "~13B parameters"

  - name: estimate_training
    given: "Chinchilla optimal"
    when: "Training estimate"
    then: "260B tokens, ~$100k compute"

  - name: estimate_memory
    given: "Precision"
    when: "Memory estimate"
    then: "FP16=26GB, INT4=7GB"

  - name: benchmark_targets
    given: "13B class"
    when: "Targets"
    then: "HumanEval>55%, MMLU>65%"

  - name: compare_llama13b
    given: "Llama 2 13B"
    when: "Comparison"
    then: "Target: exceed significantly"

  - name: phi_13b_harmony
    given: "Architecture"
    when: "Harmony"
    then: "φ-ratio in dimensions"
