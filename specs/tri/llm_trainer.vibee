# LLM Trainer - CPU Training Loop
# φ² + 1/φ² = 3 | PHOENIX = 999

name: llm_trainer
version: "1.0.0"
language: zig
module: llm_trainer

sacred_formula:
  expression: "V = n × 3^k × π^m × φ^p"
  golden_identity: "φ² + 1/φ² = 3"

creation_pattern:
  source: TrainingData
  transformer: CPUTrainer
  result: TrainedModel

types:
  TrainerConfig:
    fields:
      learning_rate: Float  # 3e-4
      batch_size: Int  # 4
      gradient_accumulation: Int  # 8
      max_steps: Int
      warmup_steps: Int
      weight_decay: Float  # 0.1
      grad_clip: Float  # 1.0
      
  TrainerState:
    fields:
      step: Int
      epoch: Int
      loss: Float
      learning_rate: Float
      
  Checkpoint:
    fields:
      model_state: Map<String, Tensor>
      optimizer_state: Map<String, Tensor>
      step: Int
      
behaviors:
  - name: train_step
    given: "Batch, model, optimizer"
    when: "Single training step"
    then: "Return loss, update model"
    
  - name: compute_loss
    given: "Logits, labels"
    when: "Cross-entropy loss"
    then: "Return loss value"
    
  - name: gradient_accumulation
    given: "Micro-batches, accumulation_steps"
    when: "Accumulate gradients"
    then: "Average gradients, step optimizer"
    
  - name: lr_schedule
    given: "Step, warmup, total"
    when: "Calculate LR"
    then: "Return cosine decay with warmup"
    
  - name: checkpoint
    given: "Model, optimizer, step"
    when: "Save checkpoint"
    then: "Write to disk"
    
  - name: resume
    given: "Checkpoint path"
    when: "Resume training"
    then: "Load model, optimizer, step"
