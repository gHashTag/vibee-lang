# VIBEE Specification v14490
# Commonsense Benchmark - Evaluating reasoning
name: commonsense_benchmark_v14490
version: "1.0.0"
language: zig
module: commonsense_benchmark

types:
  BenchmarkDataset:
    fields:
      winograd: String
      copa: String
      hellaswag: String
      piqa: String

  ReasoningTask:
    fields:
      context: String
      question: String
      choices: String
      answer: Int

  BenchmarkMetrics:
    fields:
      accuracy: Float
      per_category: String
      human_baseline: Float

  EvalConfig:
    fields:
      datasets: String
      few_shot: Int
      chain_of_thought: Bool

behaviors:
  - name: load_benchmark
    given: Dataset name
    when: Loading done
    then: Returns benchmark dataset

  - name: evaluate_model
    given: Model and dataset
    when: Evaluation done
    then: Returns benchmark metrics

  - name: analyze_errors
    given: Predictions and labels
    when: Analysis done
    then: Returns error categories

  - name: compare_to_human
    given: Model metrics
    when: Comparison done
    then: Returns human gap
