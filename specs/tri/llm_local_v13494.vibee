# VIBEE Specification v13494
# Local LLM (Ollama)
# KOSCHEI CYCLE 11 - 1000000x SPEEDUP

name: llm_local
version: "13494"
language: zig
module: llm_local

types:
  OllamaConfig:
    fields:
      host: String
      port: Int
      model: String
      num_ctx: Int
      num_gpu: Int

  LocalModel:
    fields:
      name: String
      size: Int
      quantization: String
      parameters: Int
      family: String

  OllamaResponse:
    fields:
      model: String
      response: String
      done: Bool
      context: List<Int>
      total_duration: Int
      eval_count: Int

  ModelPull:
    fields:
      model: String
      status: String
      digest: String
      total: Int
      completed: Int

  LocalMetrics:
    fields:
      requests: Int
      tokens_generated: Int
      avg_tokens_per_sec: Float
      gpu_utilization: Float

  EmbeddingLocal:
    fields:
      model: String
      embedding: List<Float>
      dimensions: Int

behaviors:
  - name: generate
    given: Prompt and model
    When: Generation requested
    then: Local response generated

  - name: chat
    given: Messages and model
    When: Chat requested
    then: Chat response returned

  - name: pull_model
    given: Model name
    When: Model pull requested
    then: Model downloaded

  - name: list_models
    given: Ollama running
    When: List requested
    then: Available models returned

  - name: embeddings
    given: Text to embed
    When: Embedding requested
    then: Local embedding generated

  - name: unload_model
    given: Loaded model
    When: Unload requested
    then: Model unloaded from memory

creation_pattern:
  source: OllamaConfig
  transformer: LocalLLM
  result: OllamaResponse
