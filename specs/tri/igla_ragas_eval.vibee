# VIBEE Specification - IGLA RAGAS Evaluation
# RAG Assessment metrics (Faithfulness, Relevancy, etc.)
# RAG v3 - Evaluation
# φ² + 1/φ² = 3 | PHOENIX = 999

name: igla_ragas_eval
version: "3.0.0"
language: zig
module: igla_ragas_eval

types:
  RAGASConfig:
    fields:
      llm_model: String
      embedding_model: String
      batch_size: Int
      timeout_ms: Int

  EvalSample:
    fields:
      question: String
      answer: String
      contexts: String
      ground_truth: String

  FaithfulnessScore:
    fields:
      score: Float
      claims_total: Int
      claims_supported: Int
      details: String

  RelevancyScore:
    fields:
      score: Float
      relevant_sentences: Int
      total_sentences: Int

  ContextPrecision:
    fields:
      score: Float
      precision_at_k: String

  ContextRecall:
    fields:
      score: Float
      ground_truth_covered: Float

  AnswerSimilarity:
    fields:
      score: Float
      method: String

  RAGASResult:
    fields:
      faithfulness: Float
      answer_relevancy: Float
      context_precision: Float
      context_recall: Float
      overall_score: Float

  EvalReport:
    fields:
      samples_evaluated: Int
      avg_faithfulness: Float
      avg_relevancy: Float
      avg_precision: Float
      avg_recall: Float
      timestamp: Int

  RAGASMetrics:
    fields:
      total_evaluations: Int
      avg_score: Float
      best_score: Float
      worst_score: Float

behaviors:
  - name: evaluate_faithfulness
    given: Answer and contexts
    when: Faithfulness check
    then: Faithfulness score returned

  - name: evaluate_relevancy
    given: Question and answer
    when: Relevancy check
    then: Relevancy score returned

  - name: evaluate_context_precision
    given: Contexts and ground truth
    when: Precision check
    then: Precision score returned

  - name: evaluate_context_recall
    given: Contexts and ground truth
    when: Recall check
    then: Recall score returned

  - name: evaluate_answer_similarity
    given: Answer and ground truth
    when: Similarity check
    then: Similarity score returned

  - name: evaluate_sample
    given: Complete eval sample
    when: Full evaluation
    then: RAGAS result returned

  - name: evaluate_batch
    given: Sample batch
    when: Batch evaluation
    then: Batch results returned

  - name: generate_report
    given: Evaluation results
    when: Report requested
    then: Eval report generated

  - name: compare_runs
    given: Two eval reports
    when: Comparison requested
    then: Comparison analysis returned

  - name: get_metrics
    given: Evaluator
    when: Metrics requested
    then: RAGAS metrics returned

test_cases:
  - name: test_faithfulness
    input: { answer: "Paris is capital", context: "Paris is the capital of France" }
    expected: { score_range: [0.8, 1.0] }

  - name: test_relevancy
    input: { question: "What is capital?", answer: "Paris" }
    expected: { score_range: [0.0, 1.0] }

  - name: test_precision
    input: { contexts: ["a", "b"], ground_truth: "a" }
    expected: { score_range: [0.0, 1.0] }

  - name: test_recall
    input: { contexts: ["a"], ground_truth: "a b" }
    expected: { score_range: [0.0, 1.0] }

  - name: test_full_eval
    input: { question: "Q", answer: "A", contexts: ["C"] }
    expected: { has_scores: true }

  - name: test_report
    input: { samples: 10 }
    expected: { generated: true }

  - name: test_metrics
    input: {}
    expected: { total_evaluations: 0 }
