# iGLA Alignment RLHF
# Reinforcement Learning from Human Feedback
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_alignment_rlhf
version: "1.0.0"
language: zig
module: igla_alignment_rlhf

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  RLHFConfig:
    fields:
      reward_model: String
      ppo_epochs: Int
      kl_coef: Float
      clip_range: Float
      value_coef: Float

  PreferenceData:
    fields:
      prompt: String
      chosen: String
      rejected: String
      margin: Float

  RewardModel:
    fields:
      base_model: String
      reward_head: String
      accuracy: Float

  RLHFMetrics:
    fields:
      reward_mean: Float
      kl_divergence: Float
      policy_loss: Float
      value_loss: Float

behaviors:
  - name: train_reward_model
    given: "Preference data"
    when: "RM training"
    then: "Reward model trained"

  - name: generate_responses
    given: "Prompts"
    when: "Generation"
    then: "Policy responses generated"

  - name: compute_rewards
    given: "Responses"
    when: "Reward computation"
    then: "RM scores computed"

  - name: ppo_update
    given: "Rewards"
    when: "PPO"
    then: "Policy updated via PPO"

  - name: kl_penalty
    given: "Policy, reference"
    when: "KL computation"
    then: "KL divergence penalty"

  - name: clip_rewards
    given: "Raw rewards"
    when: "Clipping"
    then: "Reward clipping for stability"

  - name: phi_rlhf_harmony
    given: "RLHF"
    when: "Harmony"
    then: "φ-optimal KL coefficient"
