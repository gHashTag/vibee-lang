# iGLA RAG Metrics
# Quality metrics for RAG evaluation
# Scientific basis: Voorhees 1999, Järvelin & Kekäläinen 2002
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_rag_metrics
version: "1.0.0"
language: zig
module: igla_rag_metrics

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

scientific_references:
  - "Voorhees 1999: TREC-8 Question Answering Track"
  - "Järvelin & Kekäläinen 2002: NDCG"
  - "Sakai 2007: Alternatives to bpref"
  - "Es et al. 2023: RAGAS"

types:
  MetricsConfig:
    fields:
      k_values: String
      relevance_threshold: Float
      enable_latency: Bool

  RetrievalMetrics:
    fields:
      precision_at_k: Float
      recall_at_k: Float
      mrr: Float
      ndcg: Float
      map: Float

  LatencyMetrics:
    fields:
      p50_ms: Float
      p95_ms: Float
      p99_ms: Float
      avg_ms: Float

  QualityMetrics:
    fields:
      faithfulness: Float
      answer_relevance: Float
      context_precision: Float
      context_recall: Float

  BenchmarkResult:
    fields:
      dataset: String
      metrics: String
      timestamp: Int
      config: String

  RelevanceJudgment:
    fields:
      query_id: Int
      doc_id: Int
      relevance: Int

behaviors:
  - name: compute_precision_at_k
    given: "Retrieved docs, relevant docs, k"
    when: "Precision calculation"
    then: "Precision@K score"

  - name: compute_recall_at_k
    given: "Retrieved docs, relevant docs, k"
    when: "Recall calculation"
    then: "Recall@K score"

  - name: compute_mrr
    given: "Ranked results, relevant docs"
    when: "MRR calculation"
    then: "Mean Reciprocal Rank"

  - name: compute_ndcg
    given: "Ranked results, relevance scores, k"
    when: "NDCG calculation"
    then: "Normalized DCG@K"

  - name: compute_map
    given: "Ranked results, relevant docs"
    when: "MAP calculation"
    then: "Mean Average Precision"

  - name: compute_faithfulness
    given: "Answer, context"
    when: "Faithfulness check"
    then: "Faithfulness score [0,1]"

  - name: compute_latency_percentiles
    given: "Latency samples"
    when: "Latency analysis"
    then: "P50, P95, P99 latencies"

  - name: phi_quality_score
    given: "All metrics"
    when: "Sacred aggregation"
    then: "φ-weighted quality score"
