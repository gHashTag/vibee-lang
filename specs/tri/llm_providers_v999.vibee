# llm_providers_v999.vibee - Multi-Provider LLM Client
# KOSCHEI PATTERN - Together.ai, OpenRouter, Groq, HuggingFace fallback
# φ² + 1/φ² = 3 | PHOENIX = 999

name: llm_providers_v999
version: "999.0.0"
language: zig
module: llm_providers_v999

types:
  ProviderType:
    fields:
      together: String
      openrouter: String
      groq: String
      huggingface: String
      ollama: String

  ProviderConfig:
    fields:
      name: String
      api_key: String
      base_url: String
      model: String
      timeout_ms: Int
      priority: Int

  ChatMessage:
    fields:
      role: String
      content: String

  ChatRequest:
    fields:
      model: String
      messages: List<String>
      max_tokens: Int
      temperature: Float

  ChatResponse:
    fields:
      content: String
      model: String
      provider: String
      tokens_used: Int
      latency_ms: Int

  ProviderError:
    fields:
      provider: String
      code: Int
      message: String
      retryable: Bool

  FallbackResult:
    fields:
      success: Bool
      response: String
      provider_used: String
      attempts: Int
      errors: List<String>

behaviors:
  - name: init_provider
    given: ProviderConfig with API key and URL
    when: Initialize provider client
    then: Return configured provider ready for requests

  - name: send_chat_request
    given: ChatRequest with messages
    when: Send request to provider API
    then: Return ChatResponse or ProviderError

  - name: try_provider_with_fallback
    given: List of providers in priority order
    when: Try each provider until success
    then: Return FallbackResult with used provider

  - name: handle_rate_limit
    given: Rate limit error from provider
    when: Calculate backoff delay
    then: Return delay in milliseconds

  - name: validate_api_key
    given: Provider name and API key
    when: Check if key is valid format
    then: Return Bool indicating validity

  - name: get_model_for_provider
    given: Provider name and requested model
    when: Map model name to provider-specific ID
    then: Return provider model ID string

  - name: build_request_headers
    given: Provider name and API key
    when: Build provider-specific headers
    then: Return headers map

  - name: parse_response
    given: Raw API response JSON
    when: Extract content from response
    then: Return ChatResponse with content

  - name: handle_provider_error
    given: HTTP error code and body
    when: Parse error and determine if retryable
    then: Return ProviderError with details

  - name: calculate_cost
    given: Provider, model, and tokens used
    when: Calculate API cost
    then: Return cost in USD cents

sacred_constants:
  phi: 1.618033988749895
  phi_squared_plus_inverse_squared: 3
  phoenix: 999
  default_timeout_ms: 60000
  max_retries: 3

provider_models:
  together:
    qwen: "Qwen/Qwen2.5-72B-Instruct-Turbo"
    llama: "meta-llama/Llama-3.3-70B-Instruct-Turbo"
    deepseek: "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
  openrouter:
    qwen: "qwen/qwen-2.5-72b-instruct"
    llama: "meta-llama/llama-3.1-70b-instruct"
  groq:
    llama: "llama-3.1-70b-versatile"
    mixtral: "mixtral-8x7b-32768"
  huggingface:
    qwen: "Qwen/Qwen2.5-72B-Instruct"
    gemma: "google/gemma-3-27b-it"
