# v6008 - Attention Layer
# ========================
# Self-attention с backward
# φ² + 1/φ² = 3 | PHOENIX = 999

name: attention_layer
version: "6.0.8"
language: zig
module: attention_layer

constants:
  PHI: 1.618033988749895

types:
  AttentionLayer:
    fields:
      w_q: List
      w_k: List
      w_v: List
      w_o: List
      head_dim: Int
      
  AttentionCache:
    fields:
      q: List
      k: List
      v: List
      attn_weights: List
      
  AttentionGrads:
    fields:
      d_q: List
      d_k: List
      d_v: List
      d_input: List

behaviors:
  - name: attention_forward
    given: Input и layer
    when: Forward pass
    then: Вернуть softmax(QK^T/√d)V
    
  - name: attention_backward
    given: Output grad и cache
    when: Backward pass
    then: Вернуть AttentionGrads
    
  - name: attention_init
    given: hidden_size, head_dim
    when: Инициализация
    then: Init Q,K,V,O weights
