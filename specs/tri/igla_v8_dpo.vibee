# iGLA v8 DPO - Direct Preference Optimization
# Paper: arXiv:2305.18290 "Direct Preference Optimization"
# φ² + 1/φ² = 3 | КОЩЕЙ БЕССМЕРТЕН

name: igla_v8_dpo
version: "8.0.0"
language: zig
module: igla_v8_dpo

types:
  DPOConfig:
    fields:
      beta: Float
      reference_model: String
      loss_type: String
      
  PreferencePair:
    fields:
      prompt: String
      chosen: String
      rejected: String
      
  DPOLoss:
    fields:
      policy_logprob: Float
      reference_logprob: Float
      margin: Float

behaviors:
  - name: dpo_loss
    given: "Preference pairs"
    when: "DPO training"
    then: "Direct optimization"
    
  - name: no_reward_model
    given: "DPO approach"
    when: "Training"
    then: "Skip reward modeling"
    
  - name: reference_constraint
    given: "Reference model"
    when: "KL constraint"
    then: "Implicit regularization"
    
  - name: preference_margin
    given: "Chosen vs rejected"
    when: "Loss computation"
    then: "Maximize margin"
    
  - name: efficient_training
    given: "DPO vs RLHF"
    when: "Training time"
    then: "Faster convergence"
    
  - name: stable_optimization
    given: "Direct loss"
    when: "Gradient updates"
    then: "More stable than PPO"
