# iGLA v8 Phi-3 - Microsoft's Small but Powerful
# Source: Microsoft 2024 "Phi-3 Technical Report"
# φ² + 1/φ² = 3 | КОЩЕЙ БЕССМЕРТЕН

name: igla_v8_phi3
version: "8.0.0"
language: zig
module: igla_v8_phi3

types:
  Phi3Config:
    fields:
      model_size: String
      data_quality: String
      reasoning_enhanced: Bool
      
  DataCuration:
    fields:
      synthetic_data: Bool
      quality_filter: Float
      
  ReasoningModule:
    fields:
      chain_of_thought: Bool
      step_by_step: Bool

behaviors:
  - name: small_powerful
    given: "3.8B parameters"
    when: "Inference"
    then: "GPT-3.5 level quality"
    
  - name: data_quality_focus
    given: "Curated data"
    when: "Training"
    then: "Quality over quantity"
    
  - name: synthetic_data
    given: "Generated examples"
    when: "Data augmentation"
    then: "Diverse training"
    
  - name: reasoning_ability
    given: "Complex problem"
    when: "Chain of thought"
    then: "Step-by-step reasoning"
    
  - name: efficient_inference
    given: "Small model"
    when: "Edge deployment"
    then: "Fast on-device"
    
  - name: long_context
    given: "128K variant"
    when: "Long document"
    then: "Extended context"
