# webarena_benchmark_v22.vibee
# KOSCHEI CYCLE 22 - WebArena Benchmark Runner
# Standardized agent evaluation
# φ² + 1/φ² = 3 | PHOENIX = 999

name: webarena_benchmark_v22
version: "22.0.0"
language: zig
module: webarena_benchmark_v22

types:
  BenchmarkConfig:
    fields:
      task_set: String
      agent_name: String
      model: String
      max_parallel: Int
      output_dir: String

  BenchmarkResult:
    fields:
      total_tasks: Int
      passed_tasks: Int
      failed_tasks: Int
      success_rate: Float
      avg_steps: Float
      avg_time_ms: Float

  TaskScore:
    fields:
      task_id: Int
      passed: Bool
      steps: Int
      time_ms: Int
      error: Option<String>

  CategoryScore:
    fields:
      category: String
      total: Int
      passed: Int
      success_rate: Float

  BenchmarkReport:
    fields:
      agent_name: String
      model: String
      timestamp: Timestamp
      overall_score: Float
      category_scores: String
      task_scores: String

  ComparisonResult:
    fields:
      agent_a: String
      agent_b: String
      score_diff: Float
      better_agent: String

  ProgressInfo:
    fields:
      completed: Int
      total: Int
      current_task: String
      elapsed_ms: Int

  BenchmarkStats:
    fields:
      min_steps: Int
      max_steps: Int
      median_steps: Float
      min_time_ms: Int
      max_time_ms: Int
      median_time_ms: Float

behaviors:
  - name: create_benchmark
    given: BenchmarkConfig
    when: Creating benchmark runner
    then: Return benchmark handle

  - name: run_benchmark
    given: Benchmark and task set
    when: Running full benchmark
    then: Return BenchmarkResult

  - name: run_single_task
    given: Task ID
    when: Running single task
    then: Return TaskScore

  - name: calculate_scores
    given: All task results
    when: Calculating benchmark scores
    then: Return BenchmarkResult

  - name: generate_report
    given: BenchmarkResult
    when: Generating detailed report
    then: Return BenchmarkReport

  - name: compare_agents
    given: Two benchmark results
    when: Comparing agent performance
    then: Return ComparisonResult

  - name: get_progress
    given: Running benchmark
    when: Getting progress info
    then: Return ProgressInfo

  - name: calculate_stats
    given: Task scores
    when: Calculating statistics
    then: Return BenchmarkStats

  - name: export_results
    given: BenchmarkResult and format
    when: Exporting results
    then: Return exported data

  - name: load_baseline
    given: Baseline path
    when: Loading baseline results
    then: Return baseline scores

test_cases:
  - name: run_mini_benchmark
    input: 5 tasks
    expected: result with scores

  - name: calculate_success_rate
    input: 8 passed, 2 failed
    expected: success_rate=0.8

  - name: compare_agents
    input: agent_a=0.7, agent_b=0.8
    expected: better=agent_b

sacred_constants:
  phi: 1.618033988749895
  phi_squared_plus_inverse_squared: 3
  phoenix: 999
