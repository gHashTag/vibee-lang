# v6600 - MiniLM CPU Training
# ============================
# Минимальная языковая модель для CPU тренировки
# MVP: Embedding → FC1 → GELU → FC2 → Softmax
# φ² + 1/φ² = 3 | PHOENIX = 999

name: mini_lm_cpu
version: "6.6.0"
language: zig
module: mini_lm_cpu

constants:
  PHI: 1.618033988749895
  PHOENIX: 999
  DEFAULT_VOCAB: 100
  DEFAULT_HIDDEN: 64
  DEFAULT_BATCH: 4

types:
  MiniLMConfig:
    fields:
      vocab_size: Int
      hidden_size: Int
      ff_size: Int
      max_seq_len: Int
      
  MiniLMModel:
    fields:
      embedding: List
      fc1_weight: List
      fc1_bias: List
      fc2_weight: List
      fc2_bias: List
      config: MiniLMConfig
      
  ForwardCache:
    fields:
      embeddings: List
      fc1_output: List
      gelu_output: List
      logits: List
      
  BackwardGrads:
    fields:
      d_embedding: List
      d_fc1_weight: List
      d_fc1_bias: List
      d_fc2_weight: List
      d_fc2_bias: List

behaviors:
  - name: init_model
    given: Config
    when: Model initialization
    then: Вернуть initialized MiniLMModel with Xavier weights
    
  - name: forward_pass
    given: Input tokens и model
    when: Forward computation
    then: Вернуть logits и cache
    
  - name: backward_pass
    given: Loss gradient и cache
    when: Backward computation
    then: Вернуть gradients для всех параметров
    
  - name: embedding_lookup
    given: Token ids и embedding table
    when: Embedding lookup
    then: Вернуть embedded vectors
    
  - name: embedding_backward
    given: Gradient и token ids
    when: Embedding gradient
    then: Accumulate gradients to embedding table
    
  - name: compute_loss
    given: Logits и targets
    when: Loss computation
    then: Вернуть cross-entropy loss
    
  - name: num_parameters
    given: Model
    when: Parameter count
    then: Вернуть total number of parameters
    
  - name: save_model
    given: Model и path
    when: Saving
    then: Save model weights to file
