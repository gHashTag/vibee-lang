# iGLA v5 GPTQ - Gradient-based Post-Training Quantization
# Paper: arXiv:2210.17323 "GPTQ: Accurate Post-Training Quantization"
# φ² + 1/φ² = 3 | КОЩЕЙ БЕССМЕРТЕН

name: igla_v5_gptq
version: "5.0.0"
language: zig
module: igla_v5_gptq

types:
  GPTQConfig:
    fields:
      bits: Int
      group_size: Int
      act_order: Bool
      
  HessianInfo:
    fields:
      diagonal: String
      inverse: String
      
  QuantizedLayer:
    fields:
      weights_int: String
      scales: String
      zeros: String

behaviors:
  - name: compute_hessian
    given: "Calibration data"
    when: "Hessian approximation"
    then: "Second-order information captured"
    
  - name: optimal_quantization
    given: "Hessian inverse"
    when: "OBS-based quantization"
    then: "Minimal reconstruction error"
    
  - name: column_wise_quant
    given: "Weight matrix"
    when: "Column-by-column quantization"
    then: "Error compensation applied"
    
  - name: act_order_reorder
    given: "Activation order"
    when: "Column reordering"
    then: "Quantize important columns first"
    
  - name: group_quantization
    given: "Group size 128"
    when: "Per-group scales"
    then: "Better accuracy than per-tensor"
    
  - name: gptq_inference
    given: "INT4 weights"
    when: "Dequant + GEMM"
    then: "4x memory reduction, fast inference"
