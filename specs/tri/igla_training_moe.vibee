# iGLA Training MoE
# Mixture of Experts architecture
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_moe
version: "1.0.0"
language: zig
module: igla_training_moe

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  MoEConfig:
    fields:
      num_experts: Int
      num_experts_per_tok: Int
      expert_capacity: Float
      router_type: String
      load_balancing: Bool

  Expert:
    fields:
      expert_id: Int
      gate_proj: List<Float>
      up_proj: List<Float>
      down_proj: List<Float>

  RouterOutput:
    fields:
      expert_indices: List<Int>
      expert_weights: List<Float>
      load_balance_loss: Float

  MoEMetrics:
    fields:
      expert_utilization: List<Float>
      load_balance_score: Float
      routing_entropy: Float
      active_params: Float

behaviors:
  - name: route_tokens
    given: "Hidden states"
    when: "Routing"
    then: "Top-k experts selected per token"

  - name: compute_expert
    given: "Token + expert"
    when: "Expert forward"
    then: "Expert MLP computed"

  - name: combine_experts
    given: "Expert outputs"
    when: "Combination"
    then: "Weighted sum of experts"

  - name: balance_load
    given: "Routing decisions"
    when: "Load balancing"
    then: "Auxiliary loss for balance"

  - name: sparse_forward
    given: "Batch"
    when: "Sparse MoE"
    then: "Only active experts computed"

  - name: compute_metrics
    given: "MoE layer"
    when: "Metrics"
    then: "8x7B = 47B total, 13B active"

  - name: phi_moe_harmony
    given: "Experts"
    when: "Harmony"
    then: "φ-ratio in expert selection"
