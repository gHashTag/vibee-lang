# WEB SCRAPING ETHICS V143 - Legal and Ethical Guidelines
# VIBEE AMPLIFICATION MODE: Responsible Data Collection
name: scraping_ethics_v143
version: "143.0.0"
language: zig
module: scraping_ethics

creation_pattern:
  source: EthicalPrinciples
  transformer: GuidelineCompiler
  result: EthicsFramework

# WEB SCRAPING ETHICS
# Этика и правовые аспекты веб-скрапинга

types:
  LegalStatus:
    fields:
      jurisdiction: String
      status: String
      key_cases: List<String>
      recommendations: List<String>

  RobotsRule:
    fields:
      directive: String
      meaning: String
      example: String
      compliance_required: Bool

  RateLimitPolicy:
    fields:
      requests_per_second: Float
      delay_between_requests_ms: Int
      respect_retry_after: Bool
      exponential_backoff: Bool

  DataCategory:
    fields:
      category: String
      sensitivity: String
      legal_considerations: String
      best_practice: String

  EthicalPrinciple:
    fields:
      principle: String
      description: String
      implementation: String
      violation_example: String

  TermsOfService:
    fields:
      site: String
      scraping_allowed: Bool
      api_available: Bool
      rate_limits: String
      attribution_required: Bool

  ComplianceChecklist:
    fields:
      item: String
      required: Bool
      how_to_check: String

  EthicsFramework:
    fields:
      principles: List<EthicalPrinciple>
      legal_status: List<LegalStatus>
      compliance: List<ComplianceChecklist>

behaviors:
  - name: check_robots_txt
    given: "URL"
    when: "Compliance check"
    then: "Robots rules"
    test_cases:
      - name: robots
        input: "https://example.com"
        expected: "rules"

  - name: validate_tos
    given: "Terms of Service"
    when: "Validation"
    then: "Compliance status"
    test_cases:
      - name: tos
        input: "terms"
        expected: "status"

  - name: apply_rate_limit
    given: "Policy"
    when: "Request"
    then: "Throttled request"
    test_cases:
      - name: rate
        input: "policy"
        expected: "throttled"

  - name: classify_data
    given: "Data"
    when: "Classification"
    then: "Category"
    test_cases:
      - name: classify
        input: "data"
        expected: "category"

  - name: audit_compliance
    given: "Scraper config"
    when: "Audit"
    then: "Compliance report"
    test_cases:
      - name: audit
        input: "config"
        expected: "report"

  - name: generate_attribution
    given: "Source"
    when: "Attribution"
    then: "Attribution text"
    test_cases:
      - name: attr
        input: "source"
        expected: "text"

# LEGAL LANDSCAPE

legal_status:
  usa:
    jurisdiction: "United States"
    status: "Generally legal with restrictions"
    key_cases:
      - "hiQ Labs v. LinkedIn (2022) - Scraping public data allowed"
      - "Van Buren v. United States (2021) - CFAA narrowed"
      - "Sandvig v. Barr (2020) - Research scraping protected"
    recommendations:
      - "Respect robots.txt"
      - "Don't bypass authentication"
      - "Avoid CFAA violations"
      - "Check Terms of Service"
      
  eu:
    jurisdiction: "European Union"
    status: "GDPR applies to personal data"
    key_cases:
      - "Ryanair v. PR Aviation (2015)"
    recommendations:
      - "GDPR compliance for personal data"
      - "Database Directive considerations"
      - "Legitimate interest assessment"
      
  russia:
    jurisdiction: "Russia"
    status: "Limited regulation"
    recommendations:
      - "Personal data law (152-FZ)"
      - "Copyright considerations"

# ROBOTS.TXT COMPLIANCE

robots_txt:
  directives:
    user_agent:
      directive: "User-agent"
      meaning: "Specifies which crawler the rules apply to"
      example: "User-agent: *"
      compliance_required: true
      
    disallow:
      directive: "Disallow"
      meaning: "Paths that should not be crawled"
      example: "Disallow: /private/"
      compliance_required: true
      
    allow:
      directive: "Allow"
      meaning: "Explicitly allowed paths"
      example: "Allow: /public/"
      compliance_required: true
      
    crawl_delay:
      directive: "Crawl-delay"
      meaning: "Seconds between requests"
      example: "Crawl-delay: 10"
      compliance_required: true
      
    sitemap:
      directive: "Sitemap"
      meaning: "Location of sitemap"
      example: "Sitemap: /sitemap.xml"
      compliance_required: false

  best_practices:
    - "Always check robots.txt before scraping"
    - "Respect Crawl-delay directive"
    - "Use descriptive User-agent"
    - "Cache robots.txt (refresh periodically)"

# RATE LIMITING

rate_limiting:
  conservative:
    requests_per_second: 0.5
    delay_ms: 2000
    description: "Very respectful, for sensitive sites"
    
  moderate:
    requests_per_second: 1.0
    delay_ms: 1000
    description: "Standard for most sites"
    
  aggressive:
    requests_per_second: 5.0
    delay_ms: 200
    description: "Only with explicit permission"
    
  best_practices:
    - "Start slow, increase if allowed"
    - "Respect Retry-After headers"
    - "Use exponential backoff on errors"
    - "Distribute requests over time"
    - "Avoid peak hours"

# DATA CATEGORIES

data_categories:
  public_data:
    category: "Public Data"
    sensitivity: "Low"
    legal_considerations: "Generally safe to collect"
    best_practice: "Still respect robots.txt and ToS"
    examples:
      - "Public profiles"
      - "Published articles"
      - "Product listings"
      
  personal_data:
    category: "Personal Data (PII)"
    sensitivity: "High"
    legal_considerations: "GDPR, CCPA, other privacy laws"
    best_practice: "Avoid unless necessary, anonymize"
    examples:
      - "Names"
      - "Email addresses"
      - "Phone numbers"
      
  copyrighted_content:
    category: "Copyrighted Content"
    sensitivity: "Medium"
    legal_considerations: "Copyright law, fair use"
    best_practice: "Use for analysis, not republishing"
    examples:
      - "Articles"
      - "Images"
      - "Videos"
      
  proprietary_data:
    category: "Proprietary Data"
    sensitivity: "High"
    legal_considerations: "Trade secrets, contracts"
    best_practice: "Do not scrape"
    examples:
      - "Pricing algorithms"
      - "Internal documents"

# ETHICAL PRINCIPLES

ethical_principles:
  transparency:
    principle: "Transparency"
    description: "Be open about who you are and what you're doing"
    implementation: "Use identifiable User-Agent"
    violation_example: "Pretending to be Googlebot"
    
  minimization:
    principle: "Data Minimization"
    description: "Collect only what you need"
    implementation: "Define scope before scraping"
    violation_example: "Scraping entire site when you need one page"
    
  respect:
    principle: "Respect for Resources"
    description: "Don't overload servers"
    implementation: "Rate limiting, off-peak scraping"
    violation_example: "DDoS-like request patterns"
    
  purpose_limitation:
    principle: "Purpose Limitation"
    description: "Use data only for stated purpose"
    implementation: "Document intended use"
    violation_example: "Selling scraped data without consent"
    
  accuracy:
    principle: "Accuracy"
    description: "Ensure data quality"
    implementation: "Validation, regular updates"
    violation_example: "Publishing outdated scraped data"

# COMPLIANCE CHECKLIST

compliance_checklist:
  before_scraping:
    - item: "Check robots.txt"
      required: true
      how_to_check: "GET /robots.txt"
      
    - item: "Read Terms of Service"
      required: true
      how_to_check: "Find and read ToS page"
      
    - item: "Check for API"
      required: false
      how_to_check: "Look for /api or developer docs"
      
    - item: "Assess data sensitivity"
      required: true
      how_to_check: "Classify data categories"
      
  during_scraping:
    - item: "Implement rate limiting"
      required: true
      how_to_check: "Log request intervals"
      
    - item: "Handle errors gracefully"
      required: true
      how_to_check: "Test error scenarios"
      
    - item: "Log all activity"
      required: true
      how_to_check: "Review logs"
      
  after_scraping:
    - item: "Secure stored data"
      required: true
      how_to_check: "Security audit"
      
    - item: "Document data source"
      required: true
      how_to_check: "Maintain metadata"
      
    - item: "Plan data retention"
      required: true
      how_to_check: "Define retention policy"

# ALTERNATIVES TO SCRAPING

alternatives:
  official_apis:
    description: "Use official APIs when available"
    advantages:
      - "Legal and supported"
      - "Structured data"
      - "Rate limits documented"
    examples:
      - "Twitter API"
      - "Google APIs"
      - "GitHub API"
      
  data_partnerships:
    description: "Partner with data providers"
    advantages:
      - "Legal certainty"
      - "Higher quality data"
      - "Support available"
      
  public_datasets:
    description: "Use existing public datasets"
    advantages:
      - "No scraping needed"
      - "Often pre-cleaned"
    sources:
      - "Kaggle"
      - "Common Crawl"
      - "Government open data"

# WARNING

warning: |
  ⚠️ ВАЖНОЕ ПРЕДУПРЕЖДЕНИЕ:
  
  1. Этот документ - образовательный материал
  2. Не является юридической консультацией
  3. Законы различаются по юрисдикциям
  4. Всегда консультируйтесь с юристом
  5. Уважайте права владельцев сайтов
  
  НИКОГДА не используйте скрапинг для:
  - Обхода платных сервисов
  - Сбора личных данных без согласия
  - Нарушения авторских прав
  - DDoS-атак
  - Конкурентной разведки без этики

# φ² + 1/φ² = 3 | PHOENIX = 999
