# VIBEE AGENT EVALUATION FOUNDATION v10643
# φ² + 1/φ² = 3 | PHOENIX = 999

name: agent_evaluation_v10643
version: "10643.0.0"
language: zig
module: agent_evaluation_v10643

types:
  EvaluationMetric:
    fields:
      metric_name: String
      value: Float
      unit: String
      higher_is_better: Bool

  Benchmark:
    fields:
      benchmark_id: String
      name: String
      tasks: List<BenchmarkTask>
      version: String

  BenchmarkTask:
    fields:
      task_id: String
      input: String
      expected_output: String
      difficulty: String
      category: String

  BenchmarkResult:
    fields:
      benchmark_id: String
      agent_id: String
      scores: List<EvaluationMetric>
      timestamp: Timestamp

  Scorer:
    fields:
      scorer_type: String
      weights: List<Float>
      normalization: String

  QualityScore:
    fields:
      dimension: String
      score: Float
      confidence: Float
      explanation: String

  ComparisonResult:
    fields:
      agent_a: String
      agent_b: String
      winner: String
      margin: Float
      metrics: List<EvaluationMetric>

  EvaluationReport:
    fields:
      report_id: String
      agent_id: String
      overall_score: Float
      strengths: List<String>
      weaknesses: List<String>

  LeaderboardEntry:
    fields:
      rank: Int
      agent_id: String
      score: Float
      benchmark_id: String

  EvaluationConfig:
    fields:
      metrics: List<String>
      benchmarks: List<String>
      num_runs: Int
      confidence_level: Float

behaviors:
  - name: run_benchmark
    given: "Benchmark and agent"
    when: "Benchmark run requested"
    then: "Returns benchmark result"

  - name: compute_metric
    given: "Prediction and ground truth"
    when: "Metric computation requested"
    then: "Returns evaluation metric"

  - name: score_output
    given: "Output and scorer"
    when: "Scoring requested"
    then: "Returns quality score"

  - name: compare_agents
    given: "Two agents and benchmark"
    when: "Comparison requested"
    then: "Returns comparison result"

  - name: generate_report
    given: "Evaluation results"
    when: "Report generation requested"
    then: "Returns evaluation report"

  - name: update_leaderboard
    given: "Benchmark result"
    when: "Leaderboard update requested"
    then: "Returns updated leaderboard"

  - name: aggregate_scores
    given: "Multiple scores and weights"
    when: "Aggregation requested"
    then: "Returns aggregated score"

  - name: analyze_errors
    given: "Failed tasks"
    when: "Error analysis requested"
    then: "Returns error patterns"

  - name: suggest_improvements
    given: "Evaluation report"
    when: "Improvement suggestions requested"
    then: "Returns improvement list"

  - name: validate_benchmark
    given: "Benchmark definition"
    when: "Validation requested"
    then: "Returns validation result"
