# iGLA Training Positional Encoding
# RoPE and ALiBi position embeddings
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_positional
version: "1.0.0"
language: zig
module: igla_training_positional

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  PositionalConfig:
    fields:
      encoding_type: String
      max_position: Int
      base_frequency: Float
      scaling_factor: Float

  RoPEConfig:
    fields:
      dim: Int
      base: Float
      scaling_type: String
      factor: Float

  PositionalEmbedding:
    fields:
      cos_cached: List<Float>
      sin_cached: List<Float>
      max_seq_len: Int

  PositionalMetrics:
    fields:
      extrapolation_quality: Float
      interpolation_quality: Float
      memory_overhead: Float

behaviors:
  - name: compute_rope_freqs
    given: "Config"
    when: "Frequency computation"
    then: "Rotary frequencies computed"

  - name: apply_rope
    given: "Q, K + positions"
    when: "RoPE application"
    then: "Rotary embeddings applied"

  - name: extend_context
    given: "Base RoPE"
    when: "Context extension"
    then: "NTK-aware scaling for longer context"

  - name: apply_alibi
    given: "Attention scores"
    when: "ALiBi"
    then: "Linear bias added"

  - name: cache_embeddings
    given: "Max length"
    when: "Caching"
    then: "Precomputed embeddings cached"

  - name: extrapolate
    given: "Beyond training length"
    when: "Extrapolation"
    then: "Position extrapolation"

  - name: phi_positional_harmony
    given: "Frequencies"
    when: "Harmony"
    then: "φ-based frequency scaling"
