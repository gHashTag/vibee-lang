# LLM Attention - Grouped Query Attention (GQA)
# φ² + 1/φ² = 3 | PHOENIX = 999
# Paper: arXiv:2305.13245

name: llm_attention_gqa
version: "1.0.0"
language: zig
module: llm_attention_gqa

sacred_formula:
  expression: "V = n × 3^k × π^m × φ^p"
  golden_identity: "φ² + 1/φ² = 3"

creation_pattern:
  source: HiddenStates
  transformer: GQAttention
  result: AttendedStates

types:
  GQAConfig:
    fields:
      num_heads: Int  # 12
      num_kv_heads: Int  # 4 (GQA ratio = 3)
      head_dim: Int  # 64
      max_seq_len: Int  # 4096
      sliding_window: Int  # 4096 (0 = disabled)
      
  KVCache:
    fields:
      key_cache: Tensor  # [batch, kv_heads, max_seq, head_dim]
      value_cache: Tensor
      seq_len: Int
      
behaviors:
  - name: gqa_forward
    given: "Hidden states, KV cache, position"
    when: "Compute GQA attention"
    then: "Return attended output, updated cache"
    kv_sharing: "Each KV head serves num_heads/num_kv_heads Q heads"
    
  - name: expand_kv
    given: "KV with num_kv_heads"
    when: "Expand for Q heads"
    then: "Repeat KV to match Q heads"
    
  - name: sliding_window_mask
    given: "Sequence length, window size"
    when: "Apply sliding window"
    then: "Mask positions outside window"
    
  - name: update_kv_cache
    given: "New K, V and cache"
    when: "Append to cache"
    then: "Return updated cache"
