# ═══════════════════════════════════════════════════════════════════════════════
# TOKENIZER v325 - Text Tokenization
# ═══════════════════════════════════════════════════════════════════════════════
# Based on: BPE, SentencePiece, tiktoken
# Scientific: ACL 2024, EMNLP 2024
# PAS Pattern: HSH + PRE
# φ² + 1/φ² = 3 | PHOENIX = 999
# ═══════════════════════════════════════════════════════════════════════════════

name: tokenizer
version: "3.2.5"
language: zig
module: tokenizer

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

creation_pattern:
  source: Text
  transformer: Tokenizer
  result: TokenSequence

types:
  TokenizerType:
    enum:
      - bpe
      - wordpiece
      - sentencepiece
      - unigram

  Token:
    fields:
      token_id: Int
      text: String
      start: Int
      end: Int

  TokenSequence:
    fields:
      tokens: List<Token>
      ids: List<Int>
      attention_mask: List<Int>

  Vocabulary:
    fields:
      size: Int
      special_tokens: List<String>
      merges: List<String>

  TokenizerConfig:
    fields:
      tokenizer_type: TokenizerType
      vocab_size: Int
      max_length: Int
      padding: Bool
      truncation: Bool

  EncodingResult:
    fields:
      input_ids: List<Int>
      attention_mask: List<Int>
      token_type_ids: List<Int>

behaviors:
  - name: encode
    given: "Text"
    when: "Encoding"
    then: "Tokenize text"
    pas_pattern: HSH
    complexity: O(n)
    test_cases:
      - name: test_encode
        input: '{"text": "Hello world"}'
        expected: '{"ids": [...]}'

  - name: decode
    given: "Token IDs"
    when: "Decoding"
    then: "Convert to text"
    pas_pattern: HSH
    complexity: O(n)
    test_cases:
      - name: test_decode
        input: '{"ids": [...]}'
        expected: '{"text": "Hello world"}'

  - name: batch_encode
    given: "Text batch"
    when: "Batch encoding"
    then: "Tokenize batch"
    pas_pattern: D&C
    complexity: O(b * n)
    test_cases:
      - name: test_batch
        input: '{"texts": [...]}'
        expected: '{"batch": [...]}'

  - name: train
    given: "Corpus"
    when: "Training"
    then: "Train tokenizer"
    pas_pattern: ALG
    complexity: O(n * v)
    test_cases:
      - name: test_train
        input: '{"corpus": [...], "vocab_size": 32000}'
        expected: '{"trained": true}'

  - name: add_special_tokens
    given: "Token list"
    when: "Adding tokens"
    then: "Add special tokens"
    pas_pattern: PRE
    complexity: O(k)
    test_cases:
      - name: test_special
        input: '{"tokens": ["[CLS]", "[SEP]"]}'
        expected: '{"added": true}'

  - name: get_vocab
    given: "None"
    when: "Vocab request"
    then: "Return vocabulary"
    pas_pattern: PRE
    complexity: O(1)
    test_cases:
      - name: test_vocab
        input: '{}'
        expected: '{"vocab": {...}}'

# ═══════════════════════════════════════════════════════════════════════════════
# φ² + 1/φ² = 3 | PHOENIX = 999
# ═══════════════════════════════════════════════════════════════════════════════
