# QML Distiller - Дистилляция знаний
# Knowledge Distillation для компактных моделей
# φ² + 1/φ² = 3 | PHOENIX = 999

name: qml_distiller
version: "1.0.0"
language: zig
module: qml_distiller

sacred_formula:
  expression: "V = n × 3^k × π^m × φ^p"
  golden_identity: "φ² + 1/φ² = 3"

creation_pattern:
  source: TeacherModel
  transformer: DistillationProcess
  result: StudentModel

types:
  DistillConfig:
    fields:
      temperature: Float  # Typically 2-4
      alpha: Float  # Weight for distillation loss
      beta: Float  # Weight for task loss
      distill_type: DistillType
      
  DistillType:
    variants:
      - logits  # Output logits
      - hidden  # Hidden states
      - attention  # Attention maps
      - self_attention  # MiniLM-style
      
  DistillLoss:
    fields:
      kl_loss: Float
      mse_loss: Float
      task_loss: Float
      total: Float

behaviors:
  - name: distill_logits
    given: "Teacher logits, student logits, temperature"
    when: "Standard knowledge distillation"
    then: "Return KL divergence loss"
    paper: "Hinton et al. 2015"
    formula: "KL(softmax(t/T) || softmax(s/T)) * T²"
    
  - name: distill_hidden_states
    given: "Teacher hidden [L_t, D_t], student hidden [L_s, D_s]"
    when: "Hidden state distillation"
    then: "Return MSE loss with projection"
    projection: "Linear(D_s → D_t) if dimensions differ"
    
  - name: distill_self_attention
    given: "Teacher attention, student attention"
    when: "MiniLM-style self-attention distillation"
    then: "Return attention transfer loss"
    paper: "arXiv:2002.10957"
    key_insight: "Transfer self-attention knowledge"
    
  - name: progressive_distillation
    given: "Teacher, student, curriculum"
    when: "Multi-stage distillation"
    then: "Gradually increase task difficulty"
    stages:
      - easy_examples
      - medium_examples
      - hard_examples
      
  - name: layer_mapping
    given: "Teacher layers L_t, student layers L_s"
    when: "Map teacher to student layers"
    then: "Return optimal layer correspondence"
    strategy: "Uniform: student[i] ← teacher[i * L_t / L_s]"
