# VIBEE Specification: MCP LLM Sampling v1318
# YOLO XV - Production Ascension

name: browser_mcp_sampling
version: "1318"
language: zig
module: browser_mcp_sampling

types:
  SamplingRequest:
    fields:
      messages: List
      model_preferences: ModelPreferences
      system_prompt: String
      include_context: String
      temperature: Float
      max_tokens: Int
      stop_sequences: List
      metadata: Map

  ModelPreferences:
    fields:
      hints: List
      cost_priority: Float
      speed_priority: Float
      intelligence_priority: Float

  ModelHint:
    fields:
      name: String

  SamplingMessage:
    fields:
      role: String
      content: SamplingContent

  SamplingContent:
    fields:
      content_type: String
      text: String
      data: String
      mime_type: String

  SamplingResult:
    fields:
      role: String
      content: SamplingContent
      model: String
      stop_reason: String

  SamplingHandler:
    fields:
      handler_id: String
      model_selector: String
      rate_limiter: String

behaviors:
  - name: create_sampling_request
    given: "Messages, preferences"
    when: "Creating request"
    then: "Returns sampling request"

  - name: add_text_message
    given: "Request, role, text"
    when: "Adding text message"
    then: "Appends message to request"

  - name: add_image_message
    given: "Request, role, data, mime type"
    when: "Adding image message"
    then: "Appends image message"

  - name: set_model_preferences
    given: "Request, preferences"
    when: "Setting preferences"
    then: "Updates model selection hints"

  - name: set_temperature
    given: "Request, temperature"
    when: "Setting temperature"
    then: "Updates sampling temperature"

  - name: set_max_tokens
    given: "Request, max tokens"
    when: "Setting max tokens"
    then: "Updates token limit"

  - name: add_stop_sequence
    given: "Request, sequence"
    when: "Adding stop sequence"
    then: "Appends to stop sequences"

  - name: execute_sampling
    given: "Handler, request"
    when: "Executing sampling"
    then: "Returns LLM completion"

  - name: select_model
    given: "Preferences, available models"
    when: "Selecting model"
    then: "Returns best matching model"

  - name: stream_sampling
    given: "Handler, request, callback"
    when: "Streaming completion"
    then: "Streams tokens to callback"

creation_pattern:
  source: SamplingRequest
  transformer: LLMSampling
  result: Completion

test_cases:
  - name: test_request_creation
    input:
      messages: [{role: "user", content: "Hello"}]
    expected:
      request_valid: true

  - name: test_text_message
    input:
      role: "user"
      text: "What is 2+2?"
    expected:
      message_added: true

  - name: test_image_message
    input:
      role: "user"
      data: "base64..."
      mime_type: "image/png"
    expected:
      image_added: true

  - name: test_model_selection
    input:
      hints: [{name: "claude-3-opus"}]
      cost_priority: 0.3
      intelligence_priority: 0.7
    expected:
      model_selected: true

  - name: test_temperature_setting
    input:
      temperature: 0.7
    expected:
      temperature_set: true

  - name: test_stop_sequences
    input:
      sequences: ["```", "END"]
    expected:
      sequences_added: true

  - name: test_sampling_execution
    input:
      messages: [{role: "user", content: "Hi"}]
    expected:
      completion_returned: true

  - name: test_streaming
    input:
      stream: true
    expected:
      tokens_streamed: true
