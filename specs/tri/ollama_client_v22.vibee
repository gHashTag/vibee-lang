# ollama_client_v22.vibee
# KOSCHEI CYCLE 22.1 - Ollama LLM Client
# Native client for Ollama API
# φ² + 1/φ² = 3 | PHOENIX = 999

name: ollama_client_v22
version: "22.1.0"
language: zig
module: ollama_client_v22

types:
  OllamaConfig:
    fields:
      host: String
      port: Int
      model: String
      timeout_ms: Int
      temperature: Float
      max_tokens: Int

  GenerateRequest:
    fields:
      model: String
      prompt: String
      stream: Bool
      options: String

  GenerateResponse:
    fields:
      response: String
      done: Bool
      total_duration_ns: Int
      load_duration_ns: Int
      eval_count: Int
      eval_duration_ns: Int

  ChatMessage:
    fields:
      role: String
      content: String

  ChatRequest:
    fields:
      model: String
      messages: String
      stream: Bool

  ChatResponse:
    fields:
      message: String
      done: Bool
      total_duration_ns: Int

  ModelInfo:
    fields:
      name: String
      size: Int
      modified_at: String
      digest: String

  ClientStatus:
    fields:
      connected: Bool
      version: String
      models_available: Int

  GenerationMetrics:
    fields:
      prompt_tokens: Int
      completion_tokens: Int
      tokens_per_second: Float
      latency_ms: Int

behaviors:
  - name: create_client
    given: OllamaConfig
    when: Creating Ollama client
    then: Return client handle

  - name: check_status
    given: Client handle
    when: Checking Ollama availability
    then: Return ClientStatus

  - name: list_models
    given: Client handle
    when: Listing available models
    then: Return list of ModelInfo

  - name: generate
    given: GenerateRequest
    when: Generating text completion
    then: Return GenerateResponse

  - name: chat
    given: ChatRequest
    when: Making chat completion
    then: Return ChatResponse

  - name: generate_action
    given: Prompt for browser action
    when: Generating Thought/Action/Input
    then: Return parsed action

  - name: set_model
    given: Model name
    when: Switching model
    then: Return success status

  - name: get_metrics
    given: Last generation
    when: Getting generation metrics
    then: Return GenerationMetrics

  - name: abort_generation
    given: Active generation
    when: Aborting generation
    then: Return success status

  - name: warmup_model
    given: Model name
    when: Pre-loading model
    then: Return load time

test_cases:
  - name: check_connection
    input: localhost:11434
    expected: connected=true

  - name: generate_simple
    input: prompt="2+2="
    expected: response contains "4"

  - name: generate_action_format
    input: browser task prompt
    expected: Thought/Action/Input format

sacred_constants:
  phi: 1.618033988749895
  phi_squared_plus_inverse_squared: 3
  phoenix: 999
