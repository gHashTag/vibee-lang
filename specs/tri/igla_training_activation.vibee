# iGLA Training Activation
# SwiGLU and GELU activations
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_activation
version: "1.0.0"
language: zig
module: igla_training_activation

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  ActivationConfig:
    fields:
      activation_type: String
      hidden_size: Int
      intermediate_size: Int
      bias: Bool

  MLPWeights:
    fields:
      gate_proj: List<Float>
      up_proj: List<Float>
      down_proj: List<Float>

  ActivationOutput:
    fields:
      output: List<Float>
      gate_values: Option<List<Float>>

  ActivationMetrics:
    fields:
      sparsity: Float
      gradient_flow: Float
      compute_cost: Float

behaviors:
  - name: swiglu
    given: "Input"
    when: "SwiGLU"
    then: "Swish(gate) * up projection"

  - name: gelu
    given: "Input"
    when: "GELU"
    then: "Gaussian error linear unit"

  - name: silu
    given: "Input"
    when: "SiLU/Swish"
    then: "x * sigmoid(x)"

  - name: relu
    given: "Input"
    when: "ReLU"
    then: "max(0, x)"

  - name: compute_mlp
    given: "Hidden states"
    when: "MLP forward"
    then: "gate_proj, up_proj, down_proj"

  - name: fused_activation
    given: "Input"
    when: "Fused"
    then: "Fused gate + activation"

  - name: phi_activation_harmony
    given: "Activation"
    when: "Harmony"
    then: "φ-optimal intermediate size"
