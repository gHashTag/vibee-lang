# VIBEE Specification - IGLA ONNX Runtime
# ONNX Runtime integration for ML inference
# RAG v3 - Real Embeddings
# φ² + 1/φ² = 3 | PHOENIX = 999

name: igla_onnx_runtime
version: "3.0.0"
language: zig
module: igla_onnx_runtime

types:
  ONNXRuntime:
    fields:
      id: String
      version: String
      providers: String
      initialized: Bool

  ONNXSession:
    fields:
      id: String
      model_path: String
      input_names: String
      output_names: String
      loaded: Bool

  ONNXTensor:
    fields:
      data: String
      shape: String
      dtype: String
      device: String

  ONNXConfig:
    fields:
      num_threads: Int
      use_gpu: Bool
      gpu_device_id: Int
      memory_limit_mb: Int
      optimization_level: Int

  InferenceResult:
    fields:
      outputs: String
      latency_ms: Float
      memory_used_mb: Float

  ModelInfo:
    fields:
      name: String
      version: String
      input_shapes: String
      output_shapes: String
      opset_version: Int

  ExecutionProvider:
    fields:
      name: String
      available: Bool
      priority: Int

  ONNXMetrics:
    fields:
      total_inferences: Int
      avg_latency_ms: Float
      peak_memory_mb: Float
      cache_hits: Int

behaviors:
  - name: init_runtime
    given: ONNX config
    when: Runtime initialization
    then: Runtime ready for inference

  - name: load_model
    given: Model path
    when: Model loading
    then: Session created with model

  - name: create_tensor
    given: Data and shape
    when: Tensor creation
    then: ONNX tensor returned

  - name: run_inference
    given: Session and inputs
    when: Inference executed
    then: Output tensors returned

  - name: get_model_info
    given: Loaded session
    when: Info requested
    then: Model metadata returned

  - name: list_providers
    given: Runtime
    when: Providers listed
    then: Available providers returned

  - name: set_provider
    given: Provider name
    when: Provider set
    then: Execution provider configured

  - name: optimize_model
    given: Model session
    when: Optimization requested
    then: Model optimized for inference

  - name: warmup
    given: Session
    when: Warmup requested
    then: Model warmed up with dummy input

  - name: get_metrics
    given: Runtime
    when: Metrics requested
    then: Performance metrics returned

test_cases:
  - name: test_init_runtime
    input: { num_threads: 4 }
    expected: { initialized: true }

  - name: test_create_tensor
    input: { shape: "[1, 384]", dtype: "float32" }
    expected: { valid: true }

  - name: test_model_info
    input: { model: "minilm" }
    expected: { has_info: true }

  - name: test_providers
    input: {}
    expected: { cpu_available: true }

  - name: test_metrics
    input: {}
    expected: { total_inferences: 0 }
