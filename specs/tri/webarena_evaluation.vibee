# WebArena Evaluation Module
# Programmatic correctness validation
# 134 unique evaluation functions

name: webarena_evaluation
version: "1.0.0"
language: zig
module: webarena_evaluation

types:
  Evaluation:
    fields:
      eval_id: String
      task_id: String
      eval_type: String
      expected: String
      actual: String
      passed: Bool

  EvaluationFunction:
    fields:
      function_id: String
      function_name: String
      function_type: String
      parameters: Map<String,String>

  EvaluationResult:
    fields:
      result_id: String
      task_id: String
      success: Bool
      score: Float
      details: String

  BenchmarkRun:
    fields:
      run_id: String
      total_tasks: Int
      passed_tasks: Int
      failed_tasks: Int
      accuracy: Float

  ErrorAnalysis:
    fields:
      analysis_id: String
      error_type: String
      frequency: Int
      example_tasks: List<String>

behaviors:
  - name: evaluate_task
    given: Task result and expected outcome
    when: Evaluation needed
    then: Returns evaluation result

  - name: run_benchmark
    given: Task set and agent
    when: Full benchmark needed
    then: Returns benchmark run results

  - name: compare_outputs
    given: Expected and actual outputs
    when: Output comparison needed
    then: Returns comparison result

  - name: analyze_errors
    given: Failed evaluations
    when: Error analysis needed
    then: Returns error analysis

  - name: compute_metrics
    given: Benchmark run
    when: Metrics computation needed
    then: Returns detailed metrics

  - name: generate_report
    given: Benchmark results
    when: Report generation needed
    then: Returns formatted report
