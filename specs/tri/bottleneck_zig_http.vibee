name: bottleneck_zig_http
version: "1.0.0"
language: zig
module: bottleneck_zig_http

# BOTTLENECK: HTTP Client Performance
# Zig HTTP client for LLM API calls
# Target: Lower latency than Node.js fetch

types:
  HttpRequest:
    fields:
      method: String
      url: String
      headers: Map<String,String>
      body: Option<String>
      timeout_ms: Int

  HttpResponse:
    fields:
      status_code: Int
      headers: Map<String,String>
      body: String
      latency_ms: Int
      bytes_received: Int

  StreamChunk:
    fields:
      data: String
      is_final: Bool
      chunk_index: Int

  ConnectionPool:
    fields:
      host: String
      max_connections: Int
      active_connections: Int
      idle_connections: Int

behaviors:
  - name: send_request
    given: HttpRequest with method, url, headers, body
    when: Making API call to LLM provider
    then: Return HttpResponse with timing metrics

  - name: stream_request
    given: HttpRequest for streaming endpoint
    when: Calling streaming LLM API
    then: Yield StreamChunks as they arrive

  - name: create_pool
    given: Host and max connections
    when: Initializing connection pool
    then: Return ConnectionPool handle

  - name: post_json
    given: URL, JSON body, auth header
    when: Making LLM API call
    then: Return parsed JSON response

  - name: benchmark_latency
    given: URL and iteration count
    when: Measuring HTTP latency
    then: Return p50, p95, p99 latencies
