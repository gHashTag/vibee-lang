# iGLA v5 RWKV-6 - Linear Attention RNN
# Paper: arXiv:2305.13048 "RWKV: Reinventing RNNs for the Transformer Era"
# φ² + 1/φ² = 3 | КОЩЕЙ БЕССМЕРТЕН

name: igla_v5_rwkv
version: "5.0.0"
language: zig
module: igla_v5_rwkv

types:
  RWKVConfig:
    fields:
      num_layers: Int
      hidden_size: Int
      time_mix_ratio: Float
      
  RWKVState:
    fields:
      state_vector: String
      time_decay: String
      channel_mix: String
      
  WKVCompute:
    fields:
      w: String
      k: String
      v: String
      output: String

behaviors:
  - name: rwkv_forward
    given: "Input token"
    when: "RWKV processing"
    then: "O(1) per token, O(n) total"
    
  - name: time_mixing
    given: "Current and previous states"
    when: "Time mix applied"
    then: "Temporal information blended"
    
  - name: channel_mixing
    given: "Hidden states"
    when: "Channel mix applied"
    then: "Feature transformation"
    
  - name: wkv_kernel
    given: "W, K, V tensors"
    when: "WKV computation"
    then: "Linear attention equivalent"
    
  - name: state_update
    given: "New token processed"
    when: "State update"
    then: "Constant memory, streaming capable"
    
  - name: parallel_training
    given: "Training mode"
    when: "Parallel scan"
    then: "O(n log n) training, O(n) inference"
