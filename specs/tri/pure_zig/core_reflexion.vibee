name: core_reflexion
version: "11.0.0"
language: zig
module: core_reflexion

# CORE: Reflexion - Verbal Reinforcement Learning
# Based on arXiv:2303.11366
# 91% pass@1 on HumanEval (vs 80% GPT-4)
# Learning from linguistic feedback without weight updates

types:
  Episode:
    fields:
      episode_id: Int
      task: String
      trajectory: List<String>
      outcome: String
      success: Bool

  Reflection:
    fields:
      episode_id: Int
      what_went_wrong: String
      what_to_improve: String
      lessons_learned: List<String>
      confidence: Float

  ReflectionMemory:
    fields:
      reflections: List<String>
      max_size: Int
      retrieval_k: Int

  TrialResult:
    fields:
      trial_number: Int
      success: Bool
      steps_taken: Int
      reflection: Option<String>

  ReflexionConfig:
    fields:
      max_trials: Int
      memory_size: Int
      reflection_prompt: String
      enable_self_eval: Bool

behaviors:
  - name: execute_trial
    given: Task and ReflectionMemory
    when: Running one trial attempt
    then: Return TrialResult

  - name: generate_reflection
    given: Episode with failure
    when: Analyzing what went wrong
    then: Return Reflection

  - name: store_reflection
    given: ReflectionMemory and Reflection
    when: Adding to episodic memory
    then: Return updated ReflectionMemory

  - name: retrieve_relevant
    given: ReflectionMemory and current_task
    when: Getting relevant past reflections
    then: Return list of relevant Reflections

  - name: self_evaluate
    given: Episode trajectory
    when: Internally simulating feedback
    then: Return evaluation score

  - name: run_reflexion
    given: Task and ReflexionConfig
    when: Running full reflexion loop
    then: Return final result after trials

  - name: improve_from_feedback
    given: Reflection and current_strategy
    when: Updating decision making
    then: Return improved strategy
