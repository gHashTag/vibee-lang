name: zig_llm_unified
version: "13.0.0"
language: zig
module: zig_llm_unified

# IMPLEMENTATION: Unified LLM Interface
# Abstract interface for OpenAI, Anthropic, Local
# φ² + 1/φ² = 3

types:
  LLMProvider:
    fields:
      provider_type: String
      config: String

  LLMRequest:
    fields:
      messages: List<String>
      max_tokens: Int
      temperature: Float
      stop_sequences: List<String>

  LLMResponse:
    fields:
      content: String
      provider: String
      model: String
      tokens_used: Int
      latency_ms: Int

  LLMConfig:
    fields:
      provider: String
      api_key: String
      model: String
      base_url: Option<String>

behaviors:
  - name: create_llm
    given: LLMConfig
    when: Creating LLM client
    then: Return provider-specific client

  - name: complete
    given: LLMRequest
    when: Making completion request
    then: Return LLMResponse

  - name: complete_simple
    given: Single prompt string
    when: Quick completion
    then: Return content string

  - name: with_system
    given: System prompt
    when: Setting system context
    then: Return configured client

  - name: estimate_tokens
    given: Text
    when: Estimating token count
    then: Return approximate count
