name: llm_client_local
version: "11.0.0"
language: zig
module: llm_client_local

# LLM: Local Model Client
# Ollama, llama.cpp, vLLM

types:
  LocalConfig:
    fields:
      backend: String
      model_path: String
      host: String
      port: Int

  LocalRequest:
    fields:
      prompt: String
      max_tokens: Int
      temperature: Float
      stop_sequences: List<String>

  LocalResponse:
    fields:
      content: String
      tokens_generated: Int
      generation_time_ms: Int

  ModelInfo:
    fields:
      name: String
      size_bytes: Int
      quantization: String
      context_length: Int

behaviors:
  - name: create_client
    given: LocalConfig
    when: Connecting to local model
    then: Return client handle

  - name: generate
    given: LocalRequest
    when: Generating text
    then: Return LocalResponse

  - name: generate_stream
    given: LocalRequest
    when: Streaming generation
    then: Yield tokens

  - name: list_models
    given: Nothing
    when: Getting available models
    then: Return list of ModelInfo

  - name: load_model
    given: Model name
    when: Loading model into memory
    then: Return load status

  - name: unload_model
    given: Model name
    when: Unloading model
    then: Return unload status
