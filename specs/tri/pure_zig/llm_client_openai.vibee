name: llm_client_openai
version: "11.0.0"
language: zig
module: llm_client_openai

# LLM: OpenAI Client
# GPT-4, GPT-4o, o1 models

types:
  OpenAIConfig:
    fields:
      api_key: String
      model: String
      base_url: String
      organization: Option<String>

  ChatMessage:
    fields:
      role: String
      content: String
      name: Option<String>

  ChatRequest:
    fields:
      messages: List<String>
      model: String
      temperature: Float
      max_tokens: Int
      stream: Bool

  ChatResponse:
    fields:
      content: String
      finish_reason: String
      usage_prompt: Int
      usage_completion: Int
      model: String

  FunctionCall:
    fields:
      name: String
      arguments: String

behaviors:
  - name: create_client
    given: OpenAIConfig
    when: Initializing client
    then: Return client handle

  - name: chat
    given: ChatRequest
    when: Making chat completion
    then: Return ChatResponse

  - name: chat_stream
    given: ChatRequest
    when: Streaming chat
    then: Yield tokens

  - name: with_functions
    given: ChatRequest and functions
    when: Using function calling
    then: Return response with function calls

  - name: embed
    given: Text
    when: Getting embeddings
    then: Return embedding vector

  - name: count_tokens
    given: Messages
    when: Counting tokens
    then: Return token count
