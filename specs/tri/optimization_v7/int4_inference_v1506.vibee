# INT4 GPTQ/AWQ - Frantar 2023
# φ² + 1/φ² = 3 | PHOENIX = 999

name: int4_inference_v1506
version: "1.0.0"
language: zig
module: int4_inference_v1506

types:
  ShardingConfig:
    fields:
      shard_size: Int
      num_shards: Int
      overlap: Bool

  PrecisionConfig:
    fields:
      dtype: String
      loss_scale: Float
      grad_scale: Float

  MemoryStats:
    fields:
      peak_memory: Float
      saved_memory: Float
      efficiency: Float

behaviors:
  - name: shard
    given: Model parameters
    when: Sharding
    then: Returns sharded model

  - name: quantize
    given: FP32 weights
    when: Quantization
    then: Returns quantized weights

  - name: checkpoint
    given: Activations
    when: Checkpointing
    then: Returns memory-efficient forward

  - name: phi_constants
    given: Sacred values
    when: Constants needed
    then: Returns φ-based shard sizes
