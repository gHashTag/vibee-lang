# v7000 - Linear Attention (Mamba/S4)
# ====================================
# O(n) линейное внимание вместо O(n²)
# Based on: Mamba (Gu & Dao 2023), S4 (Gu et al. 2021)
# φ² + 1/φ² = 3 | PHOENIX = 999

name: linear_attention
version: "7.0.0"
language: zig
module: linear_attention

constants:
  PHI: 1.618033988749895
  PHOENIX: 999
  DEFAULT_D_STATE: 16
  DEFAULT_D_CONV: 4

types:
  MambaConfig:
    fields:
      d_model: Int
      d_state: Int
      d_conv: Int
      expand: Int
      dt_rank: String
      
  S4Config:
    fields:
      d_model: Int
      d_state: Int
      channels: Int
      bidirectional: Bool
      
  SSMState:
    fields:
      h: List
      conv_state: List
      ssm_state: List
      
  LinearAttnOutput:
    fields:
      output: List
      state: SSMState

behaviors:
  - name: mamba_forward
    given: Input sequence и config
    when: Forward pass
    then: Вернуть O(n) output
    
  - name: s4_forward
    given: Input и S4 config
    when: S4 forward
    then: Вернуть state space output
    
  - name: selective_scan
    given: Input, A, B, C, D, delta
    when: Selective SSM scan
    then: Вернуть scanned output
    
  - name: discretize_ssm
    given: A, B, delta
    when: Discretization
    then: Вернуть discrete A_bar, B_bar
    
  - name: causal_conv1d
    given: Input и kernel
    when: Causal convolution
    then: Вернуть conv output
    
  - name: init_ssm_state
    given: Batch size и config
    when: State initialization
    then: Вернуть initial SSMState
    
  - name: update_ssm_state
    given: Current state и input
    when: State update
    then: Вернуть updated state
    
  - name: mamba_backward
    given: Output grad и cache
    when: Backward pass
    then: Вернуть input gradients
