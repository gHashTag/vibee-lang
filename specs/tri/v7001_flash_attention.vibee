# v7001 - FlashAttention v2/v3
# =============================
# Memory-efficient attention with tiling
# Based on: Dao et al. 2022, 2023
# φ² + 1/φ² = 3 | PHOENIX = 999

name: flash_attention
version: "7.0.1"
language: zig
module: flash_attention

constants:
  PHI: 1.618033988749895
  BLOCK_M: 128
  BLOCK_N: 64
  BLOCK_K: 64

types:
  FlashConfig:
    fields:
      block_m: Int
      block_n: Int
      causal: Bool
      softmax_scale: Float
      
  FlashOutput:
    fields:
      output: List
      lse: List
      
  TileInfo:
    fields:
      m_block: Int
      n_block: Int
      k_block: Int

behaviors:
  - name: flash_attention_forward
    given: Q, K, V и config
    when: Forward pass
    then: Вернуть tiled attention output
    
  - name: flash_attention_backward
    given: dO, Q, K, V, O, lse
    when: Backward pass
    then: Вернуть dQ, dK, dV
    
  - name: compute_tile
    given: Q_tile, K_tile, V_tile
    when: Single tile computation
    then: Вернуть tile output
    
  - name: online_softmax
    given: Scores и running max/sum
    when: Online softmax
    then: Вернуть normalized scores
    
  - name: rescale_output
    given: Output, old_lse, new_lse
    when: Rescaling after merge
    then: Вернуть rescaled output
    
  - name: apply_causal_mask
    given: Scores и position
    when: Causal masking
    then: Вернуть masked scores
    
  - name: compute_lse
    given: Scores
    when: Log-sum-exp
    then: Вернуть lse for numerical stability
    
  - name: fuse_qkv_projection
    given: Input и W_qkv
    when: Fused projection
    then: Вернуть Q, K, V efficiently
