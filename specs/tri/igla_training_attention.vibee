# iGLA Training Attention
# Ring Attention + GQA + FlashAttention
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_attention
version: "1.0.0"
language: zig
module: igla_training_attention

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  AttentionConfig:
    fields:
      attention_type: String
      num_heads: Int
      num_kv_heads: Int
      head_dim: Int
      use_flash: Bool
      use_ring: Bool

  AttentionWeights:
    fields:
      q_proj: List<Float>
      k_proj: List<Float>
      v_proj: List<Float>
      o_proj: List<Float>

  AttentionOutput:
    fields:
      hidden_states: List<Float>
      attention_weights: Option<List<Float>>
      kv_cache: Option<Object>

  AttentionMetrics:
    fields:
      memory_usage_gb: Float
      compute_flops: Float
      context_length: Int
      throughput: Float

behaviors:
  - name: compute_qkv
    given: "Hidden states"
    when: "QKV projection"
    then: "Q, K, V computed with GQA"

  - name: apply_rotary
    given: "Q, K"
    when: "RoPE"
    then: "Rotary position embeddings applied"

  - name: flash_attention
    given: "Q, K, V"
    when: "FlashAttention"
    then: "IO-aware exact attention"

  - name: ring_attention
    given: "Q, K, V distributed"
    when: "Ring Attention"
    then: "Infinite context via ring"

  - name: grouped_query
    given: "Q, K, V"
    when: "GQA"
    then: "Grouped query attention (8 KV heads)"

  - name: output_projection
    given: "Attention output"
    when: "Output proj"
    then: "Final projection applied"

  - name: phi_attention_harmony
    given: "Attention"
    when: "Harmony"
    then: "φ-ratio in head dimensions"
