# VIBEE v68 Multi-Model Router Specification
# Dynamic Model Selection & Routing
# φ² + 1/φ² = 3 | PHOENIX = 999

name: multi_model_router_v68
version: "68.0.0"
language: zig
module: multi_model_router_v68

# === SACRED CONSTANTS ===
constants:
  PHI: 1.618033988749895
  TRINITY: 3
  PHOENIX: 999

# === MODEL PROVIDERS ===
types:
  Provider:
    values:
      - anthropic
      - openai
      - google
      - deepseek
      - local
      - ollama

  ModelTier:
    values:
      - flagship    # Best quality, highest cost
      - standard    # Good balance
      - fast        # Speed optimized
      - reasoning   # Extended thinking
      - code        # Code specialized

  Model:
    fields:
      id: String
      provider: String
      tier: String
      context_window: Int
      max_output: Int
      input_cost_per_mtok: Float   # $ per million tokens
      output_cost_per_mtok: Float
      speed_toks_per_sec: Int
      swe_bench: Float
      supports_vision: Bool
      supports_tools: Bool

  RoutingStrategy:
    values:
      - cost_optimized
      - quality_optimized
      - speed_optimized
      - balanced
      - task_specific

  TaskType:
    values:
      - code_generation
      - code_review
      - debugging
      - documentation
      - refactoring
      - testing
      - chat
      - reasoning

  RouterConfig:
    fields:
      strategy: String
      max_cost_per_request: Float
      min_quality_score: Float
      max_latency_ms: Int
      fallback_model: String

# === MODEL CATALOG ===
models:
  # Anthropic
  - id: claude-4-opus
    provider: anthropic
    tier: flagship
    context_window: 200000
    max_output: 32000
    input_cost_per_mtok: 15.0
    output_cost_per_mtok: 75.0
    speed_toks_per_sec: 80
    swe_bench: 72.7
    supports_vision: true
    supports_tools: true

  - id: claude-4-sonnet
    provider: anthropic
    tier: standard
    context_window: 200000
    max_output: 16000
    input_cost_per_mtok: 3.0
    output_cost_per_mtok: 15.0
    speed_toks_per_sec: 120
    swe_bench: 65.0
    supports_vision: true
    supports_tools: true

  - id: claude-4-haiku
    provider: anthropic
    tier: fast
    context_window: 200000
    max_output: 8000
    input_cost_per_mtok: 0.25
    output_cost_per_mtok: 1.25
    speed_toks_per_sec: 200
    swe_bench: 45.0
    supports_vision: true
    supports_tools: true

  # OpenAI
  - id: gpt-5
    provider: openai
    tier: flagship
    context_window: 128000
    max_output: 16000
    input_cost_per_mtok: 10.0
    output_cost_per_mtok: 30.0
    speed_toks_per_sec: 100
    swe_bench: 68.0
    supports_vision: true
    supports_tools: true

  - id: o3
    provider: openai
    tier: reasoning
    context_window: 200000
    max_output: 100000
    input_cost_per_mtok: 15.0
    output_cost_per_mtok: 60.0
    speed_toks_per_sec: 50
    swe_bench: 71.7
    supports_vision: false
    supports_tools: true

  - id: gpt-4o-mini
    provider: openai
    tier: fast
    context_window: 128000
    max_output: 16000
    input_cost_per_mtok: 0.15
    output_cost_per_mtok: 0.60
    speed_toks_per_sec: 180
    swe_bench: 42.0
    supports_vision: true
    supports_tools: true

  # Google
  - id: gemini-2-ultra
    provider: google
    tier: flagship
    context_window: 1000000
    max_output: 32000
    input_cost_per_mtok: 12.0
    output_cost_per_mtok: 36.0
    speed_toks_per_sec: 90
    swe_bench: 62.0
    supports_vision: true
    supports_tools: true

  - id: gemini-2-flash
    provider: google
    tier: fast
    context_window: 1000000
    max_output: 8000
    input_cost_per_mtok: 0.075
    output_cost_per_mtok: 0.30
    speed_toks_per_sec: 250
    swe_bench: 40.0
    supports_vision: true
    supports_tools: true

  # DeepSeek
  - id: deepseek-v3
    provider: deepseek
    tier: code
    context_window: 64000
    max_output: 8000
    input_cost_per_mtok: 0.14
    output_cost_per_mtok: 0.28
    speed_toks_per_sec: 150
    swe_bench: 48.0
    supports_vision: false
    supports_tools: true

  - id: deepseek-r1
    provider: deepseek
    tier: reasoning
    context_window: 64000
    max_output: 8000
    input_cost_per_mtok: 0.55
    output_cost_per_mtok: 2.19
    speed_toks_per_sec: 80
    swe_bench: 55.0
    supports_vision: false
    supports_tools: true

  # Local
  - id: llama-3.3-70b
    provider: local
    tier: standard
    context_window: 128000
    max_output: 8000
    input_cost_per_mtok: 0.0
    output_cost_per_mtok: 0.0
    speed_toks_per_sec: 40
    swe_bench: 35.0
    supports_vision: false
    supports_tools: true

  - id: qwen-2.5-coder-32b
    provider: local
    tier: code
    context_window: 32000
    max_output: 8000
    input_cost_per_mtok: 0.0
    output_cost_per_mtok: 0.0
    speed_toks_per_sec: 60
    swe_bench: 40.0
    supports_vision: false
    supports_tools: true

# === ROUTING RULES ===
routing_rules:
  code_generation:
    preferred: [claude-4-sonnet, deepseek-v3, gpt-5]
    fallback: qwen-2.5-coder-32b
    
  code_review:
    preferred: [claude-4-opus, o3]
    fallback: claude-4-sonnet
    
  debugging:
    preferred: [claude-4-opus, deepseek-r1]
    fallback: claude-4-sonnet
    
  documentation:
    preferred: [claude-4-haiku, gpt-4o-mini]
    fallback: gemini-2-flash
    
  refactoring:
    preferred: [claude-4-sonnet, deepseek-v3]
    fallback: llama-3.3-70b
    
  testing:
    preferred: [claude-4-sonnet, gpt-5]
    fallback: deepseek-v3
    
  reasoning:
    preferred: [o3, deepseek-r1, claude-4-opus]
    fallback: claude-4-sonnet

# === BEHAVIORS ===
behaviors:
  - name: select_model
    given: "Task type and constraints"
    when: "Model selection requested"
    then: "Return optimal model for task"
    
  - name: estimate_cost
    given: "Model and token counts"
    when: "Cost estimation requested"
    then: "Return estimated cost in USD"
    
  - name: check_availability
    given: "Model ID"
    when: "Availability check"
    then: "Return model status"
    
  - name: fallback
    given: "Primary model failure"
    when: "Request fails"
    then: "Route to fallback model"

# === TEST CASES ===
test_cases:
  - name: select_for_code_gen
    input: {task: "code_generation", strategy: "balanced"}
    expected: {model: "claude-4-sonnet"}
    
  - name: select_for_speed
    input: {task: "documentation", strategy: "speed_optimized"}
    expected: {model: "gemini-2-flash"}
    
  - name: select_for_quality
    input: {task: "code_review", strategy: "quality_optimized"}
    expected: {model: "claude-4-opus"}
    
  - name: select_free
    input: {task: "code_generation", max_cost: 0}
    expected: {model: "qwen-2.5-coder-32b"}
