# VIBEE Specification - IGLA BEIR Benchmark
# Benchmarking Information Retrieval (BEIR) suite
# RAG v3 - Evaluation
# φ² + 1/φ² = 3 | PHOENIX = 999

name: igla_beir_benchmark
version: "3.0.0"
language: zig
module: igla_beir_benchmark

types:
  BEIRDataset:
    fields:
      name: String
      task_type: String
      corpus_size: Int
      query_count: Int
      loaded: Bool

  BEIRQuery:
    fields:
      id: String
      text: String
      relevant_docs: String

  BEIRCorpus:
    fields:
      id: String
      title: String
      text: String

  RetrievalResult:
    fields:
      query_id: String
      doc_ids: String
      scores: String

  NDCGScore:
    fields:
      ndcg_at_1: Float
      ndcg_at_3: Float
      ndcg_at_5: Float
      ndcg_at_10: Float
      ndcg_at_100: Float

  MAPScore:
    fields:
      map_at_1: Float
      map_at_10: Float
      map_at_100: Float

  RecallScore:
    fields:
      recall_at_1: Float
      recall_at_10: Float
      recall_at_100: Float
      recall_at_1000: Float

  BEIRResult:
    fields:
      dataset: String
      ndcg: String
      map: String
      recall: String
      mrr: Float
      latency_ms: Float

  BenchmarkConfig:
    fields:
      datasets: String
      batch_size: Int
      max_queries: Int
      k_values: String

  BEIRMetrics:
    fields:
      datasets_evaluated: Int
      avg_ndcg_10: Float
      avg_recall_100: Float
      total_queries: Int

behaviors:
  - name: load_dataset
    given: Dataset name
    when: Dataset loading
    then: BEIR dataset ready

  - name: index_corpus
    given: Corpus documents
    when: Indexing
    then: Corpus indexed

  - name: run_queries
    given: Query set
    when: Retrieval
    then: Results for all queries

  - name: compute_ndcg
    given: Results and relevance
    when: NDCG computation
    then: NDCG scores returned

  - name: compute_map
    given: Results and relevance
    when: MAP computation
    then: MAP scores returned

  - name: compute_recall
    given: Results and relevance
    when: Recall computation
    then: Recall scores returned

  - name: compute_mrr
    given: Results and relevance
    when: MRR computation
    then: MRR score returned

  - name: evaluate_dataset
    given: Dataset and retriever
    when: Full evaluation
    then: BEIR result returned

  - name: compare_retrievers
    given: Multiple results
    when: Comparison
    then: Comparison report

  - name: export_results
    given: Benchmark results
    when: Export requested
    then: Results exported

  - name: get_metrics
    given: Benchmark
    when: Metrics requested
    then: BEIR metrics returned

test_cases:
  - name: test_load_dataset
    input: { name: "msmarco" }
    expected: { loaded: true }

  - name: test_ndcg
    input: { results: ["d1", "d2"], relevant: ["d1"] }
    expected: { ndcg_at_1: 1.0 }

  - name: test_map
    input: { results: ["d1"], relevant: ["d1"] }
    expected: { map_at_1: 1.0 }

  - name: test_recall
    input: { results: ["d1", "d2"], relevant: ["d1"], k: 10 }
    expected: { recall_at_10: 1.0 }

  - name: test_mrr
    input: { results: ["d1"], relevant: ["d1"] }
    expected: { mrr: 1.0 }

  - name: test_evaluate
    input: { dataset: "test" }
    expected: { has_scores: true }

  - name: test_metrics
    input: {}
    expected: { datasets_evaluated: 0 }
