# iGLA Training Mixed Precision
# BF16/FP16 training
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_mixed_precision
version: "1.0.0"
language: zig
module: igla_training_mixed_precision

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  MixedPrecisionConfig:
    fields:
      dtype: String
      loss_scale: String
      grad_scaler: Bool
      keep_fp32: List<String>

  PrecisionState:
    fields:
      current_scale: Float
      growth_interval: Int
      backoff_factor: Float
      growth_factor: Float

  CastOperation:
    fields:
      input_dtype: String
      output_dtype: String
      operation: String

  MixedPrecisionMetrics:
    fields:
      memory_savings: Float
      speedup: Float
      overflow_count: Int
      effective_scale: Float

behaviors:
  - name: cast_to_bf16
    given: "FP32 tensor"
    when: "Forward cast"
    then: "BF16 for compute"

  - name: cast_to_fp32
    given: "BF16 tensor"
    when: "Backward cast"
    then: "FP32 for accumulation"

  - name: scale_loss
    given: "Loss"
    when: "Loss scaling"
    then: "Scaled for FP16 gradients"

  - name: unscale_gradients
    given: "Scaled gradients"
    when: "Unscaling"
    then: "Original scale restored"

  - name: check_overflow
    given: "Gradients"
    when: "Overflow check"
    then: "Inf/NaN detection"

  - name: update_scale
    given: "Overflow status"
    when: "Scale update"
    then: "Dynamic loss scaling"

  - name: phi_precision_harmony
    given: "Precision"
    when: "Harmony"
    then: "φ-optimal precision mix"
