# iGLA Training Hyperparameters
# Hyperparameter configurations
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_hyperparams
version: "1.0.0"
language: zig
module: igla_training_hyperparams

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  HyperparamsConfig:
    fields:
      learning_rate: Float
      batch_size: Int
      warmup_steps: Int
      weight_decay: Float
      gradient_clip: Float

  LlamaHyperparams:
    fields:
      lr: Float
      beta1: Float
      beta2: Float
      eps: Float
      weight_decay: Float

  ScalingHyperparams:
    fields:
      lr_7b: Float
      lr_13b: Float
      lr_34b: Float
      lr_70b: Float
      batch_scaling: String

  HyperparamsMetrics:
    fields:
      optimal_lr: Float
      optimal_batch: Int
      convergence_speed: Float
      final_loss: Float

behaviors:
  - name: llama_defaults
    given: "Model size"
    when: "Llama defaults"
    then: "lr=3e-4, wd=0.1, β=(0.9,0.95)"

  - name: scale_learning_rate
    given: "Batch size"
    when: "LR scaling"
    then: "Linear or sqrt scaling"

  - name: scale_batch_size
    given: "Model size"
    when: "Batch scaling"
    then: "Larger models = larger batch"

  - name: warmup_schedule
    given: "Total steps"
    when: "Warmup"
    then: "2000 steps warmup typical"

  - name: tune_hyperparams
    given: "Validation loss"
    when: "Tuning"
    then: "Grid/random search"

  - name: apply_muP
    given: "Architecture"
    when: "μP scaling"
    then: "Maximal update parameterization"

  - name: phi_hyperparams_harmony
    given: "Hyperparams"
    when: "Harmony"
    then: "φ-optimal learning rate"
