# v3203 - Sparse Attention
# =========================
# Sub-quadratic attention patterns
# φ² + 1/φ² = 3 | PHOENIX = 999

name: sparse_attention
version: "3.2.3"
language: zig
module: sparse_attention

constants:
  PHI: 1.618033988749895
  LOCAL_WINDOW: 256
  GLOBAL_TOKENS: 64
  STRIDE: 512

types:
  SparsePattern:
    fields:
      pattern_type: String
      local_window: Int
      global_tokens: Int
      stride: Int
      
  AttentionMask:
    fields:
      row_indices: List
      col_indices: List
      num_nonzero: Int
      
  SparseConfig:
    fields:
      pattern: SparsePattern
      num_heads: Int
      head_dim: Int

behaviors:
  - name: local_attention
    given: Q, K, V and window size
    when: Computing sliding window attention
    then: Return output with O(N * window) complexity
    
  - name: strided_attention
    given: Q, K, V and stride
    when: Computing strided sparse attention
    then: Return output attending to every stride-th token
    
  - name: global_local_attention
    given: Q, K, V and global token indices
    when: Combining global and local patterns
    then: Return Longformer-style attention
    
  - name: bigbird_attention
    given: Q, K, V and config
    when: Computing BigBird pattern
    then: Return random + local + global attention
    
  - name: generate_sparse_mask
    given: Sequence length and pattern
    when: Creating attention mask
    then: Return sparse mask indices
    
  - name: sparse_matmul
    given: Dense Q and sparse K indices
    when: Computing sparse attention scores
    then: Return scores only for non-masked positions
    
  - name: estimate_flops
    given: Sequence length and pattern
    when: Estimating computation
    then: Return FLOPs (should be sub-quadratic)
