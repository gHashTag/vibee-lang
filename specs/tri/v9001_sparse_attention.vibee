# v9001 - Sparse Attention
# =========================
# Efficient sparse attention patterns
# Based on: Longformer, BigBird, Sparse Transformers
# φ² + 1/φ² = 3 | PHOENIX = 999

name: sparse_attention
version: "9.0.1"
language: zig
module: sparse_attention

constants:
  PHI: 1.618033988749895

types:
  SparseConfig:
    fields:
      pattern: String
      window_size: Int
      global_tokens: Int
      random_tokens: Int
      
  AttentionMask:
    fields:
      local_mask: List
      global_mask: List
      random_mask: List

behaviors:
  - name: sparse_attention_forward
    given: Q, K, V и config
    when: Sparse attention
    then: Вернуть sparse attention output
    
  - name: sliding_window
    given: Q, K, V и window_size
    when: Local attention
    then: Вернуть windowed attention
    
  - name: global_attention
    given: Q, K, V и global_indices
    when: Global tokens
    then: Вернуть global attention
    
  - name: random_attention
    given: Q, K, V и random_indices
    when: Random attention
    then: Вернуть random attention
    
  - name: combine_patterns
    given: Local, global, random outputs
    when: Combining patterns
    then: Вернуть combined output
    
  - name: create_sparse_mask
    given: Seq_len и config
    when: Creating mask
    then: Вернуть sparse attention mask
