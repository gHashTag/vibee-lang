# iGLA Alignment DPO
# Direct Preference Optimization
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_alignment_dpo
version: "1.0.0"
language: zig
module: igla_alignment_dpo

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  DPOConfig:
    fields:
      beta: Float
      loss_type: String
      label_smoothing: Float
      reference_free: Bool

  DPOSample:
    fields:
      prompt: String
      chosen: String
      rejected: String

  DPOLoss:
    fields:
      loss: Float
      chosen_reward: Float
      rejected_reward: Float
      margin: Float

  DPOMetrics:
    fields:
      accuracy: Float
      reward_margin: Float
      chosen_prob: Float
      rejected_prob: Float

behaviors:
  - name: compute_log_probs
    given: "Responses"
    when: "Log prob computation"
    then: "Policy and reference log probs"

  - name: dpo_loss
    given: "Log probs"
    when: "DPO loss"
    then: "-log σ(β(log π/π_ref))"

  - name: ipo_loss
    given: "Log probs"
    when: "IPO loss"
    then: "Identity preference optimization"

  - name: train_dpo
    given: "Preference data"
    when: "DPO training"
    then: "Direct preference learning"

  - name: compute_implicit_reward
    given: "Policy"
    when: "Implicit reward"
    then: "β log(π/π_ref)"

  - name: evaluate_preferences
    given: "Test pairs"
    when: "Evaluation"
    then: "Preference accuracy"

  - name: phi_dpo_harmony
    given: "DPO"
    when: "Harmony"
    then: "φ-optimal beta"
