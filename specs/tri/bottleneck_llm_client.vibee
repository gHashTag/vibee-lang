name: bottleneck_llm_client
version: "1.0.0"
language: zig
module: bottleneck_llm_client

# BOTTLENECK: LLM API Client
# Unified client for OpenAI, Anthropic, Local models
# Mirrors existing llm-interface.js but in Zig

types:
  LLMConfig:
    fields:
      provider: String
      api_key: String
      model: String
      base_url: Option<String>
      max_tokens: Int
      temperature: Float

  Message:
    fields:
      role: String
      content: String

  ChatRequest:
    fields:
      messages: List<String>
      system_prompt: Option<String>
      max_tokens: Int
      temperature: Float
      stream: Bool

  ChatResponse:
    fields:
      content: String
      finish_reason: String
      usage_prompt_tokens: Int
      usage_completion_tokens: Int
      latency_ms: Int

  StreamToken:
    fields:
      token: String
      is_final: Bool
      index: Int

behaviors:
  - name: create_client
    given: LLMConfig with provider and credentials
    when: Initializing LLM client
    then: Return client handle

  - name: chat_completion
    given: ChatRequest with messages
    when: Making non-streaming LLM call
    then: Return ChatResponse with content

  - name: chat_stream
    given: ChatRequest with stream=true
    when: Making streaming LLM call
    then: Yield StreamTokens as they arrive

  - name: extract_action
    given: ChatResponse content
    when: Parsing agent action from response
    then: Return action name and parameters

  - name: format_messages
    given: List of Message objects
    when: Preparing API request
    then: Return formatted messages array

  - name: count_tokens
    given: Text string
    when: Estimating token usage
    then: Return approximate token count
