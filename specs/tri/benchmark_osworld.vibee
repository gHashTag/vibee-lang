# OSWorld Benchmark Specification
# Real computer environment tasks
# 369 tasks across Ubuntu, Windows, macOS

name: benchmark_osworld
version: "1.0.0"
language: zig
module: benchmark_osworld

types:
  OSWorldTask:
    fields:
      task_id: String
      description: String
      operating_system: String
      application_domain: String
      difficulty: Int
      expected_steps: Int

  ExecutionEnvironment:
    fields:
      os_type: String
      os_version: String
      installed_apps: List<String>
      initial_state: String
      network_access: Bool

  EvaluationFunction:
    fields:
      function_id: String
      evaluation_type: String
      success_criteria: String
      partial_credit: Bool

  TaskResult:
    fields:
      task_id: String
      success: Bool
      steps_taken: Int
      time_elapsed_ms: Int
      error_message: Option<String>

  BenchmarkScore:
    fields:
      agent_name: String
      total_tasks: Int
      tasks_passed: Int
      accuracy_percentage: Float
      human_baseline: Float

behaviors:
  - name: load_task_suite
    given: OSWorld benchmark configuration
    when: Task loader initializes
    then: Returns 369 real computer tasks

  - name: setup_environment
    given: Task specification with OS requirements
    when: Virtual environment is provisioned
    then: Returns ready execution environment

  - name: execute_task
    given: Agent and task in prepared environment
    when: Agent attempts to complete task
    then: Returns execution trace and final state

  - name: evaluate_result
    given: Task result and evaluation function
    when: One of 134 unique evaluation functions runs
    then: Returns success/failure with detailed feedback

  - name: compute_benchmark_score
    given: All task results for an agent
    when: Aggregation across all tasks
    then: Returns overall accuracy percentage

  - name: compare_to_human
    given: Agent score and human baseline of 72%
    when: Performance gap analysis runs
    then: Returns relative performance metrics
