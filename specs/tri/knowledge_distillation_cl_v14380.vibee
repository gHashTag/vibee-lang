# VIBEE Specification v14380
# Knowledge Distillation for CL - Preserving old knowledge
name: knowledge_distillation_cl_v14380
version: "1.0.0"
language: zig
module: knowledge_distillation_cl

types:
  DistillationType:
    fields:
      logit: String
      feature: String
      attention: String
      relation: String

  TeacherStudent:
    fields:
      teacher: String
      student: String
      temperature: Float

  DistillLoss:
    fields:
      kd_loss: Float
      task_loss: Float
      total_loss: Float

  KDCLConfig:
    fields:
      distill_type: String
      temperature: Float
      alpha: Float

behaviors:
  - name: compute_soft_targets
    given: Teacher and inputs
    when: Computation done
    then: Returns soft targets

  - name: distill_features
    given: Teacher and student features
    when: Distillation done
    then: Returns feature loss

  - name: train_with_distill
    given: New data and teacher
    when: Training done
    then: Returns distill loss

  - name: update_teacher
    given: Trained student
    when: Update done
    then: Returns new teacher
