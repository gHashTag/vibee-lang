# QML Matryoshka - Вложенные эмбеддинги
# Matryoshka Representation Learning для адаптивных размеров
# φ² + 1/φ² = 3 | PHOENIX = 999

name: qml_matryoshka
version: "1.0.0"
language: zig
module: qml_matryoshka

sacred_formula:
  expression: "V = n × 3^k × π^m × φ^p"
  golden_identity: "φ² + 1/φ² = 3"
  matryoshka_sizes: [768, 512, 256, 128, 64, 32, 16, 8]

creation_pattern:
  source: FullEmbedding
  transformer: MatryoshkaEncoder
  result: NestedEmbeddings

types:
  MatryoshkaConfig:
    fields:
      full_dim: Int  # 768 for MiniLM
      nesting_dims: List<Int>  # [512, 256, 128, 64, 32, 16, 8]
      loss_weights: List<Float>  # PHI-weighted
      
  NestedEmbedding:
    fields:
      full: Tensor  # [B, 768]
      nested: List<Tensor>  # [[B,512], [B,256], ...]
      
  MatryoshkaLoss:
    fields:
      contrastive_loss: Float
      nested_losses: List<Float>
      total_loss: Float

behaviors:
  - name: encode_matryoshka
    given: "Input tokens"
    when: "Forward pass through encoder"
    then: "Return nested embeddings at all granularities"
    paper: "arXiv:2205.13147"
    
  - name: truncate_embedding
    given: "Full embedding [768] and target dim"
    when: "Need smaller representation"
    then: "Return first target_dim dimensions"
    speedup: "14x for dim=64 vs dim=768"
    
  - name: compute_matryoshka_loss
    given: "Nested embeddings, labels"
    when: "Training with MRL"
    then: "Return weighted sum of losses at each granularity"
    weights: "PHI-based: [1.0, φ⁻¹, φ⁻², φ⁻³, ...]"
    
  - name: adaptive_retrieval
    given: "Query embedding, corpus, latency_budget"
    when: "Retrieval with time constraint"
    then: "Use smallest embedding that meets accuracy threshold"
    strategy: "Start with dim=64, increase if needed"
    
  - name: phi_optimal_dims
    given: "Base dimension"
    when: "Calculate optimal nesting dimensions"
    then: "Return dims following golden ratio"
    formula: "dim_k = round(base * φ^(-k))"
