# v3603 - Constitutional AI
# ==========================
# Self-improvement through principles
# φ² + 1/φ² = 3 | PHOENIX = 999

name: constitutional_ai
version: "3.6.3"
language: zig
module: constitutional_ai

constants:
  PHI: 1.618033988749895
  MAX_REVISIONS: 3
  HARMLESSNESS_THRESHOLD: 0.9
  HELPFULNESS_THRESHOLD: 0.8

types:
  Constitution:
    fields:
      principles: List
      critique_prompts: List
      revision_prompts: List
      
  Principle:
    fields:
      name: String
      description: String
      priority: Int
      
  CritiqueResult:
    fields:
      original_response: String
      critique: String
      violated_principles: List
      severity: Float
      
  RevisionResult:
    fields:
      revised_response: String
      improvements: List
      revision_count: Int
      
  SafetyScore:
    fields:
      harmlessness: Float
      helpfulness: Float
      honesty: Float
      overall: Float
      
  RLHFConfig:
    fields:
      reward_model: String
      kl_coeff: Float
      clip_range: Float
      
  PreferenceData:
    fields:
      prompt: String
      chosen: String
      rejected: String
      
  ConstitutionalConfig:
    fields:
      constitution: Constitution
      max_revisions: Int
      safety_threshold: Float

behaviors:
  - name: load_constitution
    given: Constitution file path
    when: Loading principles
    then: Return parsed constitution
    
  - name: generate_initial
    given: Prompt and model
    when: Generating first response
    then: Return initial response
    
  - name: critique_response
    given: Response and constitution
    when: Evaluating against principles
    then: Return critique with violations
    
  - name: revise_response
    given: Response, critique, and constitution
    when: Improving response
    then: Return revised response
    
  - name: constitutional_loop
    given: Prompt and config
    when: Running full CAI loop
    then: Return final safe response
    
  - name: compute_safety_score
    given: Response
    when: Evaluating safety metrics
    then: Return safety scores
    
  - name: generate_preference_pairs
    given: Prompts and constitution
    when: Creating RLHF data
    then: Return preference pairs
    
  - name: train_reward_model
    given: Preference data
    when: Training reward model
    then: Return trained model
