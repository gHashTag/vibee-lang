# iGLA v5 LoRA+ - Improved Low-Rank Adaptation
# Paper: arXiv:2402.12354 "LoRA+: Efficient Low Rank Adaptation"
# φ² + 1/φ² = 3 | КОЩЕЙ БЕССМЕРТЕН

name: igla_v5_lora_plus
version: "5.0.0"
language: zig
module: igla_v5_lora_plus

types:
  LoraPlusConfig:
    fields:
      rank: Int
      alpha: Float
      lr_ratio: Float
      
  LoraPlusLayer:
    fields:
      lora_A: String
      lora_B: String
      lr_A: Float
      lr_B: Float
      
  LoraPlusStats:
    fields:
      accuracy_gain: Float
      convergence_speed: Float
      memory_usage: Float

behaviors:
  - name: asymmetric_lr
    given: "LoRA A and B matrices"
    when: "Different learning rates"
    then: "lr_B = 16 × lr_A optimal"
    
  - name: lora_plus_init
    given: "Pre-trained weights"
    when: "LoRA+ initialization"
    then: "A: small random, B: zero"
    
  - name: efficient_update
    given: "Gradient computed"
    when: "LoRA+ update"
    then: "Only 0.1% params updated"
    
  - name: feature_learning
    given: "Asymmetric LR"
    when: "Training"
    then: "B learns features, A learns projection"
    
  - name: accuracy_boost
    given: "LoRA+ vs LoRA"
    when: "Same compute budget"
    then: "+2% accuracy improvement"
    
  - name: merge_weights
    given: "Trained LoRA+"
    when: "Inference mode"
    then: "Merged weights, no overhead"
