# iGLA Training Optimizer
# AdamW and LAMB optimizers
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_training_optimizer
version: "1.0.0"
language: zig
module: igla_training_optimizer

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  OptimizerConfig:
    fields:
      optimizer_type: String
      learning_rate: Float
      beta1: Float
      beta2: Float
      eps: Float
      weight_decay: Float

  OptimizerState:
    fields:
      step: Int
      exp_avg: List<Float>
      exp_avg_sq: List<Float>

  OptimizerUpdate:
    fields:
      param_update: List<Float>
      grad_norm: Float
      lr_used: Float

  OptimizerMetrics:
    fields:
      grad_norm: Float
      param_norm: Float
      update_ratio: Float
      memory_gb: Float

behaviors:
  - name: adamw_step
    given: "Gradients"
    when: "AdamW step"
    then: "Adam with decoupled weight decay"

  - name: lamb_step
    given: "Gradients"
    when: "LAMB step"
    then: "Layer-wise adaptive large batch"

  - name: compute_moments
    given: "Gradients"
    when: "Moment computation"
    then: "First and second moments"

  - name: apply_weight_decay
    given: "Parameters"
    when: "Weight decay"
    then: "Decoupled L2 regularization"

  - name: clip_gradients
    given: "Gradients"
    when: "Gradient clipping"
    then: "Max norm clipping (1.0)"

  - name: zero_grad
    given: "Model"
    when: "Gradient reset"
    then: "Gradients zeroed"

  - name: phi_optimizer_harmony
    given: "Optimizer"
    when: "Harmony"
    then: "φ-based learning rate"
