# v6607 - GELU Activation with Backward
# =======================================
# GELU(x) = 0.5 * x * (1 + tanh(√(2/π) * (x + 0.044715 * x³)))
# φ² + 1/φ² = 3 | PHOENIX = 999

name: gelu_backward
version: "6.6.7"
language: zig
module: gelu_backward

constants:
  PHI: 1.618033988749895
  SQRT_2_PI: 0.7978845608
  GELU_COEFF: 0.044715

types:
  ActivationCache:
    fields:
      input: List
      
  ActivationGrad:
    fields:
      input_grad: List

behaviors:
  - name: gelu_forward
    given: Input
    when: GELU activation
    then: Вернуть GELU(input)
    
  - name: gelu_backward
    given: Input, output_grad
    when: GELU gradient
    then: Вернуть input_grad = output_grad * GELU'(input)
    
  - name: gelu_derivative
    given: X
    when: GELU derivative computation
    then: Вернуть d/dx GELU(x)
    
  - name: relu_forward
    given: Input
    when: ReLU activation
    then: Вернуть max(0, input)
    
  - name: relu_backward
    given: Input, output_grad
    when: ReLU gradient
    then: Вернуть output_grad * (input > 0)
    
  - name: tanh_forward
    given: Input
    when: Tanh activation
    then: Вернуть tanh(input)
    
  - name: tanh_backward
    given: Output, output_grad
    when: Tanh gradient
    then: Вернуть output_grad * (1 - output²)
