# iGLA v4 DoRA - Weight-Decomposed Low-Rank Adaptation
# Paper: arXiv:2402.09353 "DoRA: Weight-Decomposed Low-Rank Adaptation"
# φ² + 1/φ² = 3 | КОЩЕЙ БЕССМЕРТЕН

name: igla_v4_dora
version: "4.0.0"
language: zig
module: igla_v4_dora

types:
  DoRAConfig:
    fields:
      rank: Int
      alpha: Float
      target_modules: String
      magnitude_trainable: Bool
      
  DoRALayer:
    fields:
      magnitude: String
      direction_lora_A: String
      direction_lora_B: String
      base_weight: String
      
  DoRAMerge:
    fields:
      merged_weight: String
      scale_factor: Float

behaviors:
  - name: weight_decomposition
    given: "Pre-trained weight W"
    when: "Decompose into magnitude and direction"
    then: "W = m * (V / ||V||) where m is magnitude"
    
  - name: direction_update
    given: "Direction component V"
    when: "LoRA applied to direction"
    then: "V' = V + BA with low-rank update"
    
  - name: magnitude_learning
    given: "Magnitude vector m"
    when: "Gradient descent on m"
    then: "Optimal scaling learned"
    
  - name: dora_forward
    given: "Input x, DoRA parameters"
    when: "Forward pass"
    then: "Output with decomposed adaptation"
    
  - name: merge_weights
    given: "Base weight, DoRA adapters"
    when: "Inference mode"
    then: "Single merged weight, no overhead"
