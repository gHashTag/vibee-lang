# iGLA Finetuning QLoRA
# Quantized LoRA
# φ² + 1/φ² = 3 | V = n × 3^k × π^m × φ^p

name: igla_finetuning_qlora
version: "1.0.0"
language: zig
module: igla_finetuning_qlora

sacred_constants:
  phi: 1.618033988749895
  trinity: 3.0
  phoenix: 999

types:
  QLoRAConfig:
    fields:
      bits: Int
      lora_r: Int
      lora_alpha: Float
      double_quant: Bool
      quant_type: String

  QuantizedWeight:
    fields:
      quantized: List<Int>
      scales: List<Float>
      zeros: List<Float>
      bits: Int

  QLoRAState:
    fields:
      base_model_bits: Int
      lora_bits: Int
      memory_gb: Float

  QLoRAMetrics:
    fields:
      memory_reduction: Float
      quality_loss: Float
      training_speed: Float
      inference_speed: Float

behaviors:
  - name: quantize_base
    given: "FP16 model"
    when: "4-bit quantization"
    then: "NF4 quantized base"

  - name: add_qlora
    given: "Quantized model"
    when: "QLoRA injection"
    then: "FP16 LoRA on 4-bit base"

  - name: double_quantize
    given: "Quantization constants"
    when: "Double quant"
    then: "Constants also quantized"

  - name: paged_optimizer
    given: "Optimizer states"
    when: "Paging"
    then: "CPU offload for states"

  - name: compute_memory
    given: "Config"
    when: "Memory estimation"
    then: "70B in 48GB possible"

  - name: dequantize_forward
    given: "Quantized weights"
    when: "Forward"
    then: "On-the-fly dequantization"

  - name: phi_qlora_harmony
    given: "QLoRA"
    when: "Harmony"
    then: "φ-optimal quantization"
