# AskUI Vision Agent Architecture
# #1 OSWorld Benchmark - 66.2% (2025)
# Composite multi-model architecture for UI automation

name: agent_vision_askui
version: "1.0.0"
language: zig
module: agent_vision_askui

types:
  UIElement:
    fields:
      element_id: String
      element_type: String
      bounding_box: String
      confidence: Float
      text_content: Option<String>
      is_interactive: Bool

  ScreenState:
    fields:
      screenshot_data: String
      detected_elements: List<UIElement>
      current_focus: Option<String>
      timestamp: Timestamp

  VisionModel:
    fields:
      model_name: String
      model_type: String
      specialization: String
      accuracy: Float

  CompositeArchitecture:
    fields:
      localization_model: String
      text_to_coords_model: String
      query_model: String
      action_model: String
      ensemble_strategy: String

  ActionPlan:
    fields:
      action_type: String
      target_element: String
      coordinates: String
      parameters: Map<String,String>
      confidence: Float

behaviors:
  - name: detect_ui_elements
    given: Raw screenshot image data
    when: UIDT-1 model processes the image
    then: Returns list of detected UI elements with bounding boxes

  - name: text_to_coordinates
    given: Natural language description of target element
    when: PTA-1 model interprets the description
    then: Returns precise screen coordinates for the target

  - name: query_screen_state
    given: Current screen state and user intent
    when: GPT-4 or Gemini 2.5 analyzes context
    then: Returns structured understanding of current state

  - name: plan_action_sequence
    given: User goal and current screen state
    when: Action model generates execution plan
    then: Returns ordered list of actions to achieve goal

  - name: execute_composite_pipeline
    given: User request and screen capture
    when: All models work in ensemble
    then: Returns coordinated action with high accuracy

  - name: validate_action_result
    given: Expected outcome and actual screen state
    when: Vision model compares states
    then: Returns success status and error details if failed
