# Neural Eye Tracking - Gaze Estimation and Attention Prediction
# PAS Analysis for appearance-based gaze tracking
# arXiv: 2506.11932, 2411.18064, 2407.00315, 2402.01555, 2308.09593

name: neural_eye_tracking
version: "1.0.0"
language: 999
module: ⲛⲉⲩⲣⲁⲗ_ⲉⲩⲉ_ⲧⲣⲁⲕⲕⲓⲛⲅ

creation_pattern:
  source: FacialImageStream
  transformer: AppearanceBasedGazeEstimator
  result: GazeVectorWithConfidence

pas_analysis:
  current_algorithm: "Infrared-based eye tracking"
  current_complexity: "O(pupil_detection + calibration)"
  theoretical_lower_bound: "Ω(image_features)"
  gap: "O(calibration_overhead)"
  applicable_patterns:
    - pattern: MLS
      reason: "Deep learning appearance-based estimation"
      success_rate: 0.06
    - pattern: PRE
      reason: "Precomputed facial feature embeddings"
      success_rate: 0.16
    - pattern: D&C
      reason: "Separate face/eye feature extraction"
      success_rate: 0.31
    - pattern: ALG
      reason: "Self-supervised contrastive learning"
      success_rate: 0.22
  confidence: 0.80
  predicted_improvement: "Calibration-free, hardware-agnostic tracking"
  timeline: "2024-2026"

components:
  - name: fgi_net
    description: "Lightweight gaze estimation with global information fusion"
    paper: "arXiv:2411.18064"
    complexity: "O(features)"
    features:
      - global_information_fusion
      - 87_percent_param_reduction
      - 79_percent_flops_reduction
      - fast_convergence
      - 3_74_deg_mpii_error

  - name: eye_mask_ib
    description: "Unsupervised gaze via eye mask information bottleneck"
    paper: "arXiv:2407.00315"
    complexity: "O(patches)"
    features:
      - self_supervised_pretraining
      - eye_attended_masking
      - injection_bottleneck
      - contrastive_loss
      - no_gaze_annotations

  - name: slyk_latent
    description: "Deep facial feature learning for gaze"
    paper: "arXiv:2402.01555"
    complexity: "O(features)"
    features:
      - self_supervised_facial_learning
      - patch_based_tri_branch
      - inverse_variance_weighted_loss
      - 10_9_percent_improvement_gaze360

  - name: saze
    description: "Stochastic subject-wise adversarial gaze learning"
    paper: "arXiv:2401.13865"
    complexity: "O(subjects × features)"
    features:
      - face_generalization_network
      - adversarial_appearance_loss
      - subject_reselection_training
      - 3_89_deg_mpii_error

  - name: resnet_gaze
    description: "Optimized ResNet architecture for gaze"
    paper: "arXiv:2308.09593"
    complexity: "O(resolution²)"
    features:
      - stride_optimization
      - input_resolution_tuning
      - multi_region_architecture
      - 3_64_deg_eth_xgaze

behaviors:
  - name: estimate_gaze_calibration_free
    given: "Facial image from webcam"
    when: "Gaze estimation without calibration"
    then: "3D gaze vector with confidence"
    test_cases:
      - name: webcam_gaze
        input:
          image_resolution: [640, 480]
          lighting: "variable"
          calibration: false
        expected:
          angular_error_deg: "<5"
          confidence: "provided"

  - name: predict_attention
    given: "Video stream with gaze data"
    when: "Predicting visual attention"
    then: "Attention heatmap over time"
    test_cases:
      - name: attention_map
        input:
          video_fps: 30
          gaze_samples: 900
        expected:
          attention_accuracy: "high"
          temporal_smoothness: true

  - name: hardware_agnostic_tracking
    given: "Any RGB camera"
    when: "Cross-device gaze estimation"
    then: "Consistent accuracy across devices"
    test_cases:
      - name: cross_device
        input:
          devices: ["webcam", "smartphone", "tablet"]
        expected:
          accuracy_variance: "<10%"
          no_recalibration: true

  - name: self_supervised_gaze
    given: "Unlabeled facial images"
    when: "Training without gaze annotations"
    then: "Learned gaze representation"
    test_cases:
      - name: unsupervised_training
        input:
          labeled_data: 0
          unlabeled_images: 100000
        expected:
          downstream_accuracy: "competitive"
          transfer_learning: "effective"

test_generation:
  strategy: property_based
  properties:
    - "Gaze estimation works without calibration"
    - "Accuracy degrades gracefully with lighting"
    - "Cross-device generalization maintained"
    - "Self-supervised pretraining improves accuracy"
    - "Confidence correlates with actual error"
