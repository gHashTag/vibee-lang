# Layer 31: Neural Screen-Space Effects
# Deep Shading and neural post-processing
# Author: Dmitrii Vasilev

name: layer_31_screen_space
version: "1.0.0"
language: 999
module: neural_screen_space

creation_pattern:
  source: GBuffer
  transformer: NeuralScreenSpaceProcessor
  result: ShadedImage

# ═══════════════════════════════════════════════════════════════════════════════
# DEEP SHADING (1603.06078) - CGF 2017
# ═══════════════════════════════════════════════════════════════════════════════

deep_shading:
  paper: "Deep Shading: Convolutional Neural Networks for Screen-Space Shading"
  arxiv: "1603.06078"
  venue: "Computer Graphics Forum 2017"
  authors: ["Nalbach", "Arabadzhiyska", "Mehta", "Seidel", "Ritschel"]
  
  concept:
    description: "CNN learns screen-space effects from G-buffer attributes"
    paradigm: "Inverse of computer vision - attributes → appearance"
    
  input_gbuffer:
    - positions: "World-space positions"
    - normals: "Surface normals"
    - albedo: "Diffuse reflectance"
    - depth: "Depth buffer"
    - motion_vectors: "Per-pixel motion"
    
  learned_effects:
    ambient_occlusion:
      description: "Screen-space ambient occlusion"
      quality: "Comparable to ray-traced AO"
      
    indirect_light:
      description: "One-bounce indirect illumination"
      quality: "Approximates global illumination"
      
    subsurface_scattering:
      description: "Screen-space SSS"
      quality: "Skin-like translucency"
      
    depth_of_field:
      description: "Bokeh and blur"
      quality: "Cinematic DoF"
      
    motion_blur:
      description: "Per-object motion blur"
      quality: "Film-quality blur"
      
    anti_aliasing:
      description: "Temporal anti-aliasing"
      quality: "Reduces jaggies"

  architecture:
    type: "U-Net style encoder-decoder"
    input_channels: 12  # G-buffer attributes
    output_channels: 3  # RGB
    
  training:
    data: "Rendered image pairs (G-buffer, ground truth)"
    loss: "L1 + perceptual loss"
    
  performance:
    speed: "Real-time on GPU"
    quality: "Competitive with traditional methods"

pas_prediction:
  target: "Neural Screen-Space Effects"
  current: "Traditional SSAO, SSR"
  predicted: "Unified neural screen-space"
  speedup: "5x"
  confidence: 0.80
  timeline: "2025-2026"
  patterns: [MLS, PRE]

behaviors:
  - name: apply_neural_ssao
    given: G-buffer with positions and normals
    when: Neural SSAO is applied
    then: Ambient occlusion computed
    
  - name: apply_neural_gi
    given: G-buffer with full attributes
    when: Neural indirect light is applied
    then: One-bounce GI approximated
    
  - name: apply_neural_dof
    given: G-buffer with depth
    when: Neural DoF is applied
    then: Cinematic depth of field rendered
