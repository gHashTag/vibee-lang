# VIBEE LLM Providers v1.1.0
# OpenAI, Anthropic, Local LLM integration

name: vibee_llm_providers
version: "1.1.0"
language: zig
module: vibee_llm_providers

types:
  LLMProvider:
    fields:
      name: String
      api_key: Option<String>
      base_url: String
      default_model: String

  LLMRequest:
    fields:
      model: String
      messages: List<Object>
      temperature: Float
      max_tokens: Int
      stream: Bool

  LLMResponse:
    fields:
      content: String
      finish_reason: String
      usage: Object

  StreamEvent:
    fields:
      type: String
      content: Option<String>
      done: Bool

  ModelInfo:
    fields:
      id: String
      name: String
      context_length: Int
      supports_vision: Bool

behaviors:
  - name: create_provider
    given: "Provider config"
    when: "Create"
    then: "Initialize provider"

  - name: complete
    given: "Request"
    when: "Complete"
    then: "Return response"

  - name: stream_complete
    given: "Request"
    when: "Stream"
    then: "Yield chunks"

  - name: openai_complete
    given: "Request"
    when: "Call OpenAI"
    then: "Return GPT response"

  - name: anthropic_complete
    given: "Request"
    when: "Call Anthropic"
    then: "Return Claude response"

  - name: local_complete
    given: "Request"
    when: "Call local"
    then: "Return Ollama/LMStudio response"

  - name: list_models
    given: "Provider"
    when: "List"
    then: "Return available models"

  - name: count_tokens
    given: "Text"
    when: "Count"
    then: "Return token count"

  - name: validate_api_key
    given: "Provider and key"
    when: "Validate"
    then: "Return validity"

test_cases:
  - name: test_openai_provider
    input: { provider: "openai" }
    expected: { created: true }

  - name: test_anthropic_provider
    input: { provider: "anthropic" }
    expected: { created: true }

  - name: test_local_provider
    input: { provider: "local", url: "http://localhost:11434" }
    expected: { created: true }

  - name: test_streaming
    input: { stream: true }
    expected: { chunks_received: true }

  - name: test_token_count
    input: { text: "Hello world" }
    expected: { tokens_gt: 0 }
