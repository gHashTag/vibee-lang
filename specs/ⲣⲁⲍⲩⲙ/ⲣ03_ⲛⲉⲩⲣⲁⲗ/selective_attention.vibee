# SelectiveAttention - Saliency Prediction and Attention Mechanisms
# Source: Visual attention and saliency research
# PAS Analysis: D&C (multi-scale), MLS (saliency learning), PRE (feature maps)

name: selective_attention
version: "1.0.0"
language: 999
module: ⲤⲈⲖⲈⲔⲦⲒⲂⲈ_ⲀⲦⲦⲈⲚⲦⲒⲞⲚ

pas_analysis:
  source_paper: "Visual attention research"
  current_complexity: "O(n²) full attention"
  theoretical_lower_bound: "O(n) sparse attention"
  gap: "Quadratic to linear via sparsity"
  patterns_applicable:
    - symbol: D&C
      name: "Divide-and-Conquer"
      success_rate: 0.31
      rationale: "Multi-scale attention"
    - symbol: MLS
      name: "ML-Guided Search"
      success_rate: 0.06
      rationale: "Learn saliency maps"
    - symbol: PRE
      name: "Precomputation"
      success_rate: 0.16
      rationale: "Pre-compute feature maps"
    - symbol: HSH
      name: "Hashing"
      success_rate: 0.12
      rationale: "Locality-sensitive hashing"
  confidence: 0.75
  predicted_improvement: "Human-like attention patterns"

creation_pattern:
  source: VisualScene
  transformer: AttentionModel
  result: SaliencyMap

behaviors:
  - name: bottom_up_saliency
    given: "Visual input"
    when: "Compute feature contrast"
    then: "Generate saliency map"
    test_cases:
      - name: itti_koch
        input:
          features: ["color", "intensity", "orientation"]
          scales: 3
        expected:
          auc: 0.85
          nss: 2.5

  - name: top_down_attention
    given: "Visual input and task"
    when: "Apply task-driven attention"
    then: "Focus on task-relevant regions"
    test_cases:
      - name: task_attention
        input:
          task: "find_red_object"
          scene: "cluttered"
        expected:
          target_fixation: 0.9
          search_efficiency: 0.8

  - name: scanpath_prediction
    given: "Image"
    when: "Predict eye movement sequence"
    then: "Generate scanpath"
    test_cases:
      - name: scanpath_model
        input:
          fixations: 10
          duration_ms: 3000
        expected:
          sequence_score: 0.7
          human_like: true

  - name: attention_gating
    given: "Multi-modal input"
    when: "Apply attention gates"
    then: "Select relevant modalities"
    test_cases:
      - name: cross_modal_attention
        input:
          modalities: ["vision", "audio"]
          task: "speech_recognition"
        expected:
          audio_weight: 0.7
          vision_weight: 0.3

algorithms:
  itti_koch:
    features: ["Color", "Intensity", "Orientation"]
    center_surround: "Difference of Gaussians"
    normalization: "Iterative"
    combination: "Linear sum"
    
  transformer_attention:
    formula: "Attention(Q,K,V) = softmax(QK^T/√d_k)V"
    multi_head: "Concat(head_1, ..., head_h)W^O"
    
  sparse_attention:
    patterns: ["Local", "Strided", "Random"]
    complexity: "O(n√n) or O(n log n)"

attention_types:
  spatial: "Where to look"
  feature: "What to look for"
  temporal: "When to attend"
  object: "Which object"

metrics:
  auc_judd: 0.87
  nss: 2.5
  cc: 0.75
  kl_divergence: 0.5
