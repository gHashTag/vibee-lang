# VIBEE Specification: TAXAL - Triadic Alignment for Explainability
# Based on arXiv:2509.05199
# Author: Dmitrii Vasilev

name: taxal_explainability
version: "1.0.0"
language: "999"
module: ‚≤ß‚≤Å‚≤ù‚≤Å‚≤ó_‚≤ü‚≤É‚≤Å‚≤•‚≤õ‚≤â‚≤õ‚≤ì‚≤â

creation_pattern:
  source: AIDecision
  transformer: TriadicFusion
  result: MultiDimensionalExplanation

description: |
  TAXAL provides triadic fusion of explainability:
  1. Cognitive: User understanding (can they comprehend?)
  2. Functional: Practical utility (does it help?)
  3. Causal: Faithful reasoning (why this decision?)
  
  Role-sensitive explanations for different stakeholders.

pas_analysis:
  current_algorithm: "Single-dimension explanations"
  current_complexity: "O(1) explanation, limited trust"
  predicted_improvement: "Triadic fusion (3x trust dimensions)"
  patterns_applied: [D&C, ALG]
  confidence: 0.72
  scientific_reference: "arXiv:2509.05199"

behaviors:
  - name: generate_cognitive_explanation
    given: AI decision
    when: User needs to understand
    then: Comprehensible explanation generated
    test_cases:
      - name: explain_to_novice
        input: { 
          decision: "recommend_async_await",
          user_expertise: "beginner"
        }
        expected: { 
          explanation: "This makes your code wait for data without freezing",
          analogy: "Like ordering food and doing other things while waiting",
          complexity: "simple"
        }
      - name: explain_to_expert
        input: { 
          decision: "recommend_async_await",
          user_expertise: "senior"
        }
        expected: { 
          explanation: "Non-blocking I/O improves throughput under concurrent load",
          technical_details: true,
          complexity: "advanced"
        }

  - name: generate_functional_explanation
    given: AI decision
    when: User needs practical value
    then: Utility-focused explanation generated
    test_cases:
      - name: explain_refactor_benefit
        input: { decision: "extract_method" }
        expected: { 
          benefit: "Reduces code duplication by 40%",
          impact: "Easier maintenance",
          effort: "5 minutes"
        }

  - name: generate_causal_explanation
    given: AI decision
    when: User needs reasoning trace
    then: Faithful causal chain provided
    test_cases:
      - name: explain_bug_detection
        input: { decision: "flag_null_pointer_risk" }
        expected: { 
          cause: "Variable 'user' may be null at line 42",
          evidence: "No null check before access",
          chain: ["input_validation_missing", "null_propagation", "crash_risk"]
        }

  - name: triadic_fusion
    given: All three explanation types
    when: Comprehensive explanation needed
    then: Unified multi-dimensional explanation
    test_cases:
      - name: full_explanation
        input: { decision: "suggest_caching" }
        expected: { 
          cognitive: "Store results to avoid recalculating",
          functional: "Reduces response time by 80%",
          causal: "Same query executed 50 times/minute",
          unified: true
        }

  - name: role_sensitive_explanation
    given: User role
    when: Generating explanation
    then: Explanation adapted to role
    test_cases:
      - name: developer_role
        input: { role: "developer", decision: "security_warning" }
        expected: { 
          focus: "technical_fix",
          includes: ["code_example", "vulnerability_type"]
        }
      - name: manager_role
        input: { role: "manager", decision: "security_warning" }
        expected: { 
          focus: "business_impact",
          includes: ["risk_level", "compliance_status"]
        }
      - name: auditor_role
        input: { role: "auditor", decision: "security_warning" }
        expected: { 
          focus: "evidence_trail",
          includes: ["decision_log", "data_sources"]
        }

explanation_dimensions:
  cognitive:
    coptic: ‚≤î‚≤û‚≤Ñ‚≤ö
    icon: "üß†"
    color: "rgba(100, 200, 255, 0.8)"
    question: "Can the user understand?"
    
  functional:
    coptic: ‚≤™‚≤®‚≤ö‚≤î
    icon: "‚öôÔ∏è"
    color: "rgba(100, 255, 150, 0.8)"
    question: "Does it help the user?"
    
  causal:
    coptic: ‚≤î‚≤Ä‚≤®‚≤å
    icon: "üîó"
    color: "rgba(255, 200, 100, 0.8)"
    question: "Why this decision?"

user_roles:
  - developer
  - manager
  - auditor
  - end_user
  - researcher

exports:
  - generate_cognitive_explanation
  - generate_functional_explanation
  - generate_causal_explanation
  - triadic_fusion
  - role_sensitive_explanation
