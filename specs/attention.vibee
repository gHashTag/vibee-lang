# Attention - Механизм внимания
# PAS: Flash Attention O(n) memory
# Author: Dmitrii Vasilev
# Version: 1.0.0

name: attention
version: "1.0.0"
language: 999
module: ⲙⲗ.ⲃⲛⲓⲙⲁⲛⲓⲉ

creation_pattern:
  source: QKV_Tensors
  transformer: AttentionMechanism
  result: AttentionOutput

pas_analysis:
  current: "O(n²) memory standard attention"
  target: "O(n) memory Flash Attention"
  speedup: "n/block_size memory, 2x speed"
  confidence: 0.90
  patterns: ["D&C", "TEN"]

components:
  query_proj:
    name: "ⲠⲢⲞⲈⲔⲤⲒⲀ.Ⲩ"
    description: "Query проекция"
  key_proj:
    name: "ⲠⲢⲞⲈⲔⲤⲒⲀ.Ⲕ"
    description: "Key проекция"
  value_proj:
    name: "ⲠⲢⲞⲈⲔⲤⲒⲀ.Ⲃ"
    description: "Value проекция"
  softmax:
    name: "ⲤⲞⲪⲦⲘⲀⲔⲤ"
    description: "Softmax нормализация"
  flash_kernel:
    name: "ⲪⲖⲀⲰ.ⲔⲈⲢⲚⲈⲖ"
    description: "Flash Attention kernel"

behaviors:
  - name: compute_attention
    given: "Q, K, V тензоры"
    when: "Forward pass"
    then: "Attention output"
  - name: flash_forward
    given: "Q, K, V + block_size"
    when: "Flash Attention"
    then: "Output с O(n) памятью"
  - name: multi_head
    given: "Input + num_heads"
    when: "Multi-head attention"
    then: "Concatenated heads output"
