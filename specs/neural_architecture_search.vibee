# NeuralArchitectureSearch - Self-Improving AutoML Engine
# Source: arXiv:2511.20333 - NNGPT, arXiv:2505.16561 - Auto-nnU-Net
# PAS Analysis: MLS (architecture synthesis), PRE (NN-RAG), D&C (hierarchical NAS)

name: neural_architecture_search
version: "1.0.0"
language: 999
module: ⲚⲈⲨⲢⲀⲖ_ⲀⲢⲬⲒⲦⲈⲔⲦⲨⲢⲈ_ⲤⲈⲀⲢⲬ

pas_analysis:
  source_paper: "arXiv:2511.20333, arXiv:2505.16561"
  current_complexity: "O(n^2) exhaustive search"
  theoretical_lower_bound: "O(n log n) guided search"
  gap: "Factor of n via ML guidance"
  patterns_applicable:
    - symbol: MLS
      name: "ML-Guided Search"
      success_rate: 0.06
      rationale: "LLM-based architecture synthesis"
    - symbol: PRE
      name: "Precomputation"
      success_rate: 0.16
      rationale: "NN-RAG retrieval-augmented synthesis"
    - symbol: D&C
      name: "Divide-and-Conquer"
      success_rate: 0.31
      rationale: "Hierarchical NAS decomposition"
  confidence: 0.71
  predicted_improvement: "5K+ validated models autonomously"

creation_pattern:
  source: TaskSpecification
  transformer: NASEngine
  result: OptimalArchitecture

behaviors:
  - name: zero_shot_synthesis
    given: "Task description prompt"
    when: "Generate architecture from scratch"
    then: "Produce valid PyTorch network"
    test_cases:
      - name: cifar_classifier
        input:
          task: "Image classification 32x32"
          constraints: "< 10M params"
        expected:
          architecture: "ResNet-like"
          params: 8500000

  - name: nn_rag_retrieval
    given: "Target architecture requirements"
    when: "Query NN-RAG database"
    then: "Retrieve and adapt similar blocks"
    test_cases:
      - name: block_retrieval
        input:
          target: "attention_block"
          scope: "closed"
        expected:
          executability: 0.73
          candidates: 5

  - name: hyperparameter_optimization
    given: "Architecture and dataset"
    when: "Run HPO with Bayesian optimization"
    then: "Find optimal hyperparameters"
    test_cases:
      - name: lemur_hpo
        input:
          architecture: "CNN"
          search_space: "lr, batch_size, dropout"
        expected:
          rmse: 0.60
          better_than_optuna: true

  - name: one_shot_prediction
    given: "Architecture specification"
    when: "Predict accuracy without training"
    then: "Estimate performance"
    test_cases:
      - name: accuracy_prediction
        input:
          architecture: "MobileNet-v2"
          dataset: "ImageNet"
        expected:
          predicted_accuracy: 0.72
          rmse: 0.14

algorithms:
  darts:
    name: "Differentiable Architecture Search"
    formula: "alpha = softmax(architecture_params)"
    bi_level: "min_w L_val(w*, alpha) s.t. w* = argmin_w L_train(w, alpha)"
    
  nn_rag:
    retrieval: "Cosine similarity on architecture embeddings"
    synthesis: "Scope-closed PyTorch block generation"
    executability: 0.73
    
  hash_deduplication:
    purpose: "Avoid redundant architecture evaluations"
    savings: "Hundreds of training runs"

metrics:
  models_generated: 5000
  executability_rate: 0.73
  hpo_rmse: 0.60
  predictor_pearson_r: 0.78
