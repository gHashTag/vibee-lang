# Руководство по Тренировке LLM VIBEE

**Автор**: Дмитрий Васильев | **Версия**: v1610

```
V = n × 3^k × π^m × φ^p × e^q
φ² + 1/φ² = 3 | PHOENIX = 999
```

---

## 1. ЧТО ТАКОЕ ТРЕНИРОВКА LLM?

### 1.1 Базовые Концепции

**LLM (Large Language Model)** - это нейронная сеть, которая учится предсказывать следующий токен (слово/символ) на основе предыдущих.

```
Вход: "Кошка сидит на"
Выход: "крыше" (с вероятностью 0.3)
        "диване" (с вероятностью 0.25)
        "окне" (с вероятностью 0.2)
        ...
```

### 1.2 Этапы Создания LLM

```
┌─────────────────────────────────────────────────────────────────┐
│                    ПАЙПЛАЙН СОЗДАНИЯ LLM                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. PRE-TRAINING (Предобучение)                                 │
│     └── Обучение на ОГРОМНОМ корпусе текстов                    │
│         └── Триллионы токенов (интернет, книги, код)            │
│         └── Цель: предсказание следующего токена                │
│         └── Результат: "сырая" модель с общими знаниями         │
│                                                                 │
│  2. SUPERVISED FINE-TUNING (SFT)                                │
│     └── Дообучение на размеченных данных                        │
│         └── Пары (инструкция, ответ)                            │
│         └── Цель: следовать инструкциям                         │
│         └── Результат: модель понимает задачи                   │
│                                                                 │
│  3. RLHF / DPO / KTO (Выравнивание)                             │
│     └── Обучение на человеческих предпочтениях                  │
│         └── Пары (хороший ответ, плохой ответ)                  │
│         └── Цель: генерировать полезные ответы                  │
│         └── Результат: безопасная, полезная модель              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2. ВАРИАНТЫ ТРЕНИРОВКИ

### 2.1 Pre-training (Предобучение с нуля)

**Что это**: Обучение модели с нуля на огромном корпусе текстов.

**Требования**:
- 1000+ GPU (A100/H100)
- Месяцы обучения
- Триллионы токенов данных
- $1-100 миллионов

**Кто делает**: OpenAI, Anthropic, Google, Meta

```python
# Псевдокод pre-training
for batch in trillion_tokens:
    # Предсказываем следующий токен
    logits = model(input_tokens)
    loss = cross_entropy(logits, target_tokens)
    loss.backward()
    optimizer.step()
```

**VIBEE модули**: `transformer_block_v1157`, `attention_mechanism_v1156`

---

### 2.2 Supervised Fine-Tuning (SFT)

**Что это**: Дообучение на парах (инструкция → ответ).

**Требования**:
- 1-8 GPU
- Часы-дни обучения
- 10K-1M примеров
- $100-10,000

**Данные**:
```json
{
  "instruction": "Напиши функцию сортировки на Python",
  "output": "def sort(arr):\n    return sorted(arr)"
}
```

**VIBEE модули**: `instruction_tuning_v1356`, `training_data_generator_v1150`

---

### 2.3 RLHF (Reinforcement Learning from Human Feedback)

**Что это**: Обучение с подкреплением на основе человеческих предпочтений.

**Этапы**:
```
┌─────────────────────────────────────────────────────────────────┐
│                         RLHF ПАЙПЛАЙН                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. Собираем предпочтения людей:                                │
│     Вопрос: "Как приготовить кофе?"                             │
│     Ответ A: "Насыпьте кофе, залейте водой" ← ЛУЧШЕ             │
│     Ответ B: "Кофе - это напиток"           ← ХУЖЕ              │
│                                                                 │
│  2. Обучаем Reward Model (модель вознаграждения):               │
│     reward_model(ответ) → score                                 │
│     Учится предсказывать, какой ответ лучше                     │
│                                                                 │
│  3. Оптимизируем LLM через PPO:                                 │
│     - LLM генерирует ответ                                      │
│     - Reward Model оценивает ответ                              │
│     - PPO обновляет веса LLM                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**VIBEE модули**: 
- `rlhf_reward_model_v1281`
- `rlhf_ppo_trainer_v1282`

---

### 2.4 DPO (Direct Preference Optimization)

**Что это**: Упрощённая альтернатива RLHF без отдельной Reward Model.

**Преимущества**:
- Не нужна Reward Model
- Проще в реализации
- Стабильнее обучение

**Формула**:
```
L_DPO = -log(σ(β * (log π(y_w|x) - log π(y_l|x))))

где:
  y_w = предпочтительный ответ (winner)
  y_l = непредпочтительный ответ (loser)
  β = температура
```

**VIBEE модули**: `rlhf_dpo_v1283`

---

### 2.5 LoRA / QLoRA (Эффективное дообучение)

**Что это**: Дообучение только малой части параметров.

**Как работает**:
```
┌─────────────────────────────────────────────────────────────────┐
│                         LoRA                                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Оригинальные веса W (заморожены):                              │
│  ┌─────────────────────────────────┐                            │
│  │  W (d × d) = миллиарды парам.   │                            │
│  └─────────────────────────────────┘                            │
│                                                                 │
│  LoRA адаптеры (обучаемые):                                     │
│  ┌─────────┐     ┌─────────┐                                    │
│  │ A (d×r) │  ×  │ B (r×d) │  где r << d (например r=8)         │
│  └─────────┘     └─────────┘                                    │
│                                                                 │
│  Итого: W' = W + A × B                                          │
│  Обучаем только A и B (0.1% параметров!)                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**QLoRA**: LoRA + квантизация (4-bit)
- Можно обучать 65B модель на 1 GPU!

**VIBEE модули**: `lora_adapter_v1190`, `qlora_v1191`

---

## 3. ЧЕМ VIBEE ОТЛИЧАЕТСЯ?

### 3.1 Уникальность VIBEE

```
┌─────────────────────────────────────────────────────────────────┐
│              VIBEE vs ОБЫЧНЫЕ LLM                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ОБЫЧНЫЕ LLM:                                                   │
│  ├── Обучаются на тексте → генерируют текст                     │
│  ├── Код - просто ещё один текст                                │
│  └── Нет понимания структуры кода                               │
│                                                                 │
│  VIBEE LLM:                                                     │
│  ├── Обучается на СПЕЦИФИКАЦИЯХ (.vibee)                        │
│  ├── Понимает СЕМАНТИКУ кода (типы, поведения)                  │
│  ├── Генерирует в 91+ язык программирования                     │
│  ├── Понимает 20 человеческих языков                            │
│  └── Использует священную математику (φ, π, 999)                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 Данные для Тренировки VIBEE

**Формат данных**:
```yaml
# Вход: .vibee спецификация
name: user_service
types:
  User:
    fields:
      id: Int
      name: String
      email: String

behaviors:
  - name: create_user
    given: Valid user data
    when: Create called
    then: Returns new user
```

```zig
// Выход: сгенерированный код (Zig, Python, Rust, Go, ...)
const User = struct {
    id: i64,
    name: []const u8,
    email: []const u8,
};

pub fn create_user(data: UserData) !User {
    // implementation
}
```

### 3.3 Пайплайн Тренировки VIBEE

```
┌─────────────────────────────────────────────────────────────────┐
│                    VIBEE TRAINING PIPELINE                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ЭТАП 1: Сбор данных                                            │
│  ├── 4,517 .vibee спецификаций                                  │
│  ├── 4,153 сгенерированных .zig файлов                          │
│  ├── Пары (spec → code) для каждого языка                       │
│  └── training_data_generator_v1150                              │
│                                                                 │
│  ЭТАП 2: Аугментация                                            │
│  ├── spec_augmentation_v1152 - вариации спецификаций            │
│  ├── back_translation_v1254 - обратный перевод                  │
│  ├── code_style_transfer_v1255 - перенос стиля                  │
│  └── Увеличение данных в 10-100x                                │
│                                                                 │
│  ЭТАП 3: Pre-training (опционально)                             │
│  ├── Берём базовую модель (Llama, Mistral, CodeLlama)           │
│  ├── Или обучаем с нуля на коде                                 │
│  └── transformer_block_v1157, attention_mechanism_v1156         │
│                                                                 │
│  ЭТАП 4: SFT на спецификациях                                   │
│  ├── instruction_tuning_v1356                                   │
│  ├── Формат: "Сгенерируй {язык} код из спецификации: {spec}"    │
│  └── Ответ: сгенерированный код                                 │
│                                                                 │
│  ЭТАП 5: RLHF/DPO выравнивание                                  │
│  ├── rlhf_dpo_v1283 или rlhf_ppo_trainer_v1282                  │
│  ├── Предпочтения: корректный код > некорректный                │
│  └── code_quality_scorer_v1153 для оценки                       │
│                                                                 │
│  ЭТАП 6: Оценка                                                 │
│  ├── pass_at_k_v1277 - выполнение кода                          │
│  ├── code_bleu_v1276 - качество кода                            │
│  ├── human_eval_v1278, mbpp_eval_v1279                          │
│  └── Тесты на всех 91 языках                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 4. ПРАКТИЧЕСКИЕ ВАРИАНТЫ

### 4.1 Вариант 1: Fine-tuning существующей модели (РЕКОМЕНДУЕТСЯ)

**Базовая модель**: CodeLlama-7B, DeepSeek-Coder, StarCoder2

**Что делаем**:
1. Берём готовую модель
2. Делаем LoRA/QLoRA fine-tuning на наших данных
3. Получаем VIBEE-специализированную модель

**Требования**:
- 1 GPU (RTX 3090/4090 или A100)
- 10-100 часов обучения
- $100-1000

**Команды**:
```bash
# Генерируем training data
vibee gen --all --format jsonl > training_data.jsonl

# Fine-tune с LoRA
python train.py \
  --base_model codellama/CodeLlama-7b \
  --data training_data.jsonl \
  --lora_r 8 \
  --epochs 3
```

### 4.2 Вариант 2: Обучение с нуля (для исследований)

**Что делаем**:
1. Собираем огромный корпус кода
2. Обучаем трансформер с нуля
3. Fine-tune на VIBEE спецификациях

**Требования**:
- 8-64 GPU
- Недели-месяцы обучения
- $10,000-100,000

### 4.3 Вариант 3: Prompt Engineering (без обучения)

**Что делаем**:
1. Используем готовую модель (GPT-4, Claude)
2. Даём промпт с примерами VIBEE
3. Модель генерирует код

**Пример промпта**:
```
Ты - VIBEE компилятор. Преобразуй спецификацию в код.

Спецификация:
name: calculator
types:
  Operation:
    fields:
      a: Int
      b: Int
behaviors:
  - name: add
    given: Two numbers
    when: Add called
    then: Returns sum

Сгенерируй код на Python:
```

---

## 5. СВЯЩЕННАЯ МАТЕМАТИКА В ТРЕНИРОВКЕ

### 5.1 φ-based Learning Rate

```python
# Обычный cosine annealing
lr = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(π * t / T))

# VIBEE sacred φ-warmup
phi = 1.618033988749895
lr = lr_0 * phi ** (step / warmup_steps)  # Золотой рост
```

**VIBEE модули**: `sacred_phi_warmup_v1270`

### 5.2 Trinity Batch Scheduling

```python
# Размер батча растёт по степеням 3
batch_size = base_batch * (3 ** k)  # 32 → 96 → 288 → 864
```

**VIBEE модули**: `trinity_batch_scheduler_v1271`

### 5.3 Phoenix Checkpointing

```python
# Сохраняем чекпоинты каждые 999 шагов
if step % 999 == 0:
    save_checkpoint(model, f"phoenix_{step}.pt")
```

**VIBEE модули**: `phoenix_checkpoint_v1272`

---

## 6. ИТОГОВАЯ АРХИТЕКТУРА

```
┌─────────────────────────────────────────────────────────────────┐
│                    VIBEE LLM ARCHITECTURE                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ВХОДЫ:                                                         │
│  ├── .vibee спецификация (YAML)                                 │
│  ├── Целевой язык (91 вариант)                                  │
│  └── Человеческий язык (20 вариантов)                           │
│                                                                 │
│  МОДЕЛЬ:                                                        │
│  ├── Encoder: понимание спецификации                            │
│  │   ├── universal_language_detector_v1201                      │
│  │   ├── polyglot_tokenizer_v1202                               │
│  │   └── semantic_mapper_v1205                                  │
│  │                                                              │
│  ├── Transformer: преобразование                                │
│  │   ├── transformer_block_v1157                                │
│  │   ├── attention_mechanism_v1156                              │
│  │   └── mamba_v1300 / rwkv_v1301 (альтернативы)                │
│  │                                                              │
│  └── Decoder: генерация кода                                    │
│      ├── codegen_* (91 генератор)                               │
│      └── syntax_tree_universal_v1204                            │
│                                                                 │
│  ВЫХОДЫ:                                                        │
│  ├── Код на целевом языке                                       │
│  ├── Тесты                                                      │
│  └── Документация                                               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 7. СРАВНЕНИЕ С КОНКУРЕНТАМИ

| Модель | Языков | Спецификации | RLHF | Мультимодальность |
|--------|--------|--------------|------|-------------------|
| GPT-4 | ~20 | ❌ | ✅ | ✅ |
| Claude | ~20 | ❌ | ✅ | ✅ |
| CodeLlama | ~15 | ❌ | ❌ | ❌ |
| StarCoder | ~80 | ❌ | ❌ | ❌ |
| **VIBEE** | **91** | **✅** | **✅** | **✅** |

**Уникальность VIBEE**:
1. **Specification-first**: понимает структуру, а не просто текст
2. **91 язык**: больше всех конкурентов
3. **Священная математика**: φ, π, 999 в оптимизации
4. **Полный пайплайн**: от спецификации до тестов

---

**φ² + 1/φ² = 3 | VIBEE LLM | PHOENIX = 999**
