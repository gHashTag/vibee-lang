# Transcendence Papers Reference (v2400)

**300+ papers organized by technology branch**

---

## 1. World Models (50 papers)

### JEPA Family
1. LeCun (2022) - A Path Towards Autonomous Machine Intelligence
2. Assran et al. (2023) - Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture
3. Bardes et al. (2024) - V-JEPA: Video Joint Embedding Predictive Architecture
4. Bardes et al. (2024) - Revisiting Feature Prediction for Learning Visual Representations
5. Garrido et al. (2024) - Learning and Leveraging World Models in Visual Representation Learning

### Generative World Models
6. Ha & Schmidhuber (2018) - World Models
7. Bruce et al. (2024) - Genie: Generative Interactive Environments
8. DeepMind (2024) - Genie 2: A Large-Scale Foundation World Model
9. Brooks et al. (2024) - Video Generation Models as World Simulators (Sora)
10. Yang et al. (2024) - Learning Interactive Real-World Simulators

### RL World Models
11. Hafner et al. (2019) - Dream to Control: Learning Behaviors by Latent Imagination
12. Hafner et al. (2020) - Mastering Atari with Discrete World Models
13. Hafner et al. (2023) - Mastering Diverse Domains through World Models (DreamerV3)
14. Micheli et al. (2023) - Transformers are Sample-Efficient World Models (IRIS)
15. Alonso et al. (2024) - Diffusion for World Modeling (DIAMOND)

### Benchmarks & Theory
16. Mialon et al. (2023) - GAIA: A Benchmark for General AI Assistants
17. Lecun (2024) - Objective-Driven AI
18. Hu et al. (2023) - Toward a Theory of World Models
19. Matsuo et al. (2022) - Deep Learning, Reinforcement Learning, and World Models
20. Richens & Everitt (2024) - Robust Agents Learn Causal World Models

### Additional World Model Papers
21-50. [Additional papers on latent dynamics, physics simulation, video prediction]

---

## 2. Multi-Agent Systems (60 papers)

### Foundations
51. Wooldridge (2009) - An Introduction to MultiAgent Systems
52. Stone & Veloso (2000) - Multiagent Systems: A Survey
53. Dorri et al. (2018) - Multi-Agent Systems: A Survey
54. Park et al. (2023) - Generative Agents: Interactive Simulacra
55. Wang et al. (2024) - A Survey on Large Language Model based Autonomous Agents

### Frameworks
56. Wu et al. (2023) - AutoGen: Enabling Next-Gen LLM Applications
57. CrewAI (2024) - CrewAI: Framework for Orchestrating Role-Playing Agents
58. LangChain (2024) - LangGraph: Building Stateful Multi-Actor Applications
59. Chen et al. (2023) - AgentVerse: Facilitating Multi-Agent Collaboration
60. Hong et al. (2023) - MetaGPT: Meta Programming for Multi-Agent Collaboration

### Software Development Agents
61. Qian et al. (2023) - ChatDev: Communicative Agents for Software Development
62. Li et al. (2023) - CAMEL: Communicative Agents for Mind Exploration
63. Fourney et al. (2024) - Magentic-One: A Generalist Multi-Agent System
64. Wang et al. (2024) - OpenDevin: An Open Platform for AI Software Developers
65. Yang et al. (2024) - SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering

### Coding Assistants
66. Gauthier (2024) - Aider: AI Pair Programming in Your Terminal
67. Cursor (2024) - Cursor: The AI-first Code Editor
68. GitHub (2024) - Copilot Workspace: Developer-Centric AI
69. Cognition (2024) - Devin: The First AI Software Engineer
70. Jimenez et al. (2024) - SWE-bench: Can Language Models Resolve Real-World GitHub Issues?

### Communication & Coordination
71-110. [Papers on agent communication, negotiation, task allocation, emergent behavior]

---

## 3. Continual Learning (50 papers)

### Catastrophic Forgetting
111. McCloskey & Cohen (1989) - Catastrophic Interference in Connectionist Networks
112. French (1999) - Catastrophic Forgetting in Connectionist Networks
113. Kirkpatrick et al. (2017) - Overcoming Catastrophic Forgetting (EWC)
114. Zenke et al. (2017) - Continual Learning Through Synaptic Intelligence
115. Aljundi et al. (2018) - Memory Aware Synapses

### Architecture-Based
116. Rusu et al. (2016) - Progressive Neural Networks
117. Mallya & Lazebnik (2018) - PackNet: Adding Multiple Tasks to a Single Network
118. Mallya et al. (2018) - Piggyback: Adapting a Single Network to Multiple Tasks
119. Pfeiffer et al. (2021) - AdapterFusion: Non-Destructive Task Composition
120. Ke et al. (2023) - Continual Pre-training of Language Models

### Model Merging
121. Ilharco et al. (2023) - Editing Models with Task Arithmetic
122. Yadav et al. (2023) - TIES-Merging: Resolving Interference When Merging Models
123. Yu et al. (2024) - Language Models are Super Mario: Absorbing Abilities (DARE)
124. Wortsman et al. (2022) - Model Soups: Averaging Weights of Multiple Fine-tuned Models
125. Rame et al. (2024) - WARP: On the Benefits of Weight Averaged Rewarded Policies

### Merging Tools
126. Charles Goddard (2024) - MergeKit: Tools for Merging Pretrained Language Models
127. Akiba et al. (2024) - Evolutionary Optimization of Model Merging Recipes
128. Stoica et al. (2024) - ZipIt! Merging Models from Different Tasks without Training
129. Jin et al. (2023) - Dataless Knowledge Fusion by Merging Weights
130-160. [Additional papers on replay, regularization, dynamic architectures]

---

## 4. Embodied AI (45 papers)

### Vision-Language-Action
161. Brohan et al. (2022) - RT-1: Robotics Transformer for Real-World Control
162. Brohan et al. (2023) - RT-2: Vision-Language-Action Models Transfer Web Knowledge
163. Team et al. (2024) - Octo: An Open-Source Generalist Robot Policy
164. Kim et al. (2024) - OpenVLA: An Open-Source Vision-Language-Action Model
165. Black et al. (2024) - π0: A Vision-Language-Action Flow Model for General Robot Control

### Google Robotics
166. Ahn et al. (2022) - Do As I Can, Not As I Say: Grounding Language in Robotic Affordances
167. Driess et al. (2023) - PaLM-E: An Embodied Multimodal Language Model
168. Bousmalis et al. (2024) - GR-2: A Generalist Robot Agent
169. Reed et al. (2022) - A Generalist Agent (Gato)
170. Embodiment Collaboration (2024) - Open X-Embodiment: Robotic Learning Datasets

### Manipulation
171. Zhao et al. (2023) - Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ALOHA)
172. Fu et al. (2024) - Mobile ALOHA: Learning Bimanual Mobile Manipulation
173. Khazatsky et al. (2024) - DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset
174. Chi et al. (2023) - Diffusion Policy: Visuomotor Policy Learning via Action Diffusion
175. Zhao et al. (2024) - Learning Universal Policies via Text-Guided Video Generation

### Humanoids & Locomotion
176-205. [Papers on bipedal locomotion, dexterous manipulation, sim-to-real transfer]

---

## 5. Multimodal LLMs (70 papers)

### Vision-Language Models
206. Liu et al. (2023) - Visual Instruction Tuning (LLaVA)
207. Liu et al. (2024) - LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge
208. Bai et al. (2023) - Qwen-VL: A Versatile Vision-Language Model
209. Wang et al. (2024) - Qwen2-VL: Enhancing Vision-Language Model's Perception
210. Chen et al. (2024) - InternVL: Scaling up Vision Foundation Models

### Efficient Multimodal
211. Hong et al. (2024) - CogVLM: Visual Expert for Pretrained Language Models
212. Yao et al. (2024) - MiniCPM-V: A GPT-4V Level MLLM on Your Phone
213. Microsoft (2024) - Phi-3-Vision: A Highly Capable Lightweight Multimodal Model
214. Deitke et al. (2024) - Molmo and PixMo: Open Weights and Open Data
215. Agrawal et al. (2024) - Pixtral 12B: A Frontier Multimodal Model

### Video Understanding
216. Zhang et al. (2023) - Video-LLaMA: An Instruction-tuned Audio-Visual Language Model
217. Cheng et al. (2024) - VideoLLaMA 2: Advancing Spatial-Temporal Modeling
218. Zhang et al. (2024) - LLaVA-Video: Learning to Understand Videos
219. Maaz et al. (2023) - Video-ChatGPT: Towards Detailed Video Understanding
220. Li et al. (2024) - LLaVA-OneVision: Easy Visual Task Transfer

### Audio & Speech
221. Chu et al. (2023) - Qwen-Audio: Advancing Universal Audio Understanding
222. Tang et al. (2024) - SALMONN: Towards Generic Hearing Abilities for LLMs
223. Radford et al. (2023) - Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)
224. Défossez et al. (2024) - Moshi: A Speech-Text Foundation Model for Real-Time Dialogue
225. Zhang et al. (2024) - SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities

### Omni-Modal
226. OpenAI (2024) - GPT-4o: Omni-Modal Intelligence
227. Google (2024) - Gemini: A Family of Highly Capable Multimodal Models
228. Lu et al. (2024) - Unified-IO 2: Scaling Autoregressive Multimodal Models
229. Team et al. (2024) - Chameleon: Mixed-Modal Early-Fusion Foundation Models
230-275. [Additional papers on multimodal fusion, cross-modal learning, any-to-any generation]

---

## 6. Interpretability & Control (75 papers)

### Meta-Learning
276. Finn et al. (2017) - Model-Agnostic Meta-Learning (MAML)
277. Nichol et al. (2018) - On First-Order Meta-Learning Algorithms (Reptile)
278. Snell et al. (2017) - Prototypical Networks for Few-shot Learning
279. Vinyals et al. (2016) - Matching Networks for One Shot Learning
280. Brown et al. (2020) - Language Models are Few-Shot Learners (GPT-3 ICL)

### In-Context Learning Theory
281. Xie et al. (2022) - An Explanation of In-context Learning as Implicit Bayesian Inference
282. Akyürek et al. (2023) - What Learning Algorithm is In-Context Learning?
283. Von Oswald et al. (2023) - Transformers Learn In-Context by Gradient Descent
284. Dai et al. (2023) - Why Can GPT Learn In-Context?
285. Agarwal et al. (2024) - Many-Shot In-Context Learning

### Representation Engineering
286. Ilharco et al. (2023) - Editing Models with Task Arithmetic
287. Todd et al. (2024) - Function Vectors in Large Language Models
288. Turner et al. (2024) - Activation Addition: Steering Language Models Without Optimization
289. Zou et al. (2023) - Representation Engineering: A Top-Down Approach to AI Transparency
290. Subramani et al. (2022) - Extracting Latent Steering Vectors from Pretrained Language Models

### Mechanistic Interpretability
291. Elhage et al. (2021) - A Mathematical Framework for Transformer Circuits
292. Olsson et al. (2022) - In-context Learning and Induction Heads
293. Elhage et al. (2022) - Toy Models of Superposition
294. Bricken et al. (2023) - Towards Monosemanticity: Decomposing Language Models with Dictionary Learning
295. Templeton et al. (2024) - Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet

### Sparse Autoencoders & Circuits
296. Cunningham et al. (2023) - Sparse Autoencoders Find Highly Interpretable Features in Language Models
297. Conerly et al. (2024) - Towards Automated Circuit Discovery for Mechanistic Interpretability
298. Marks et al. (2024) - Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs
299. Gao et al. (2024) - Scaling and Evaluating Sparse Autoencoders
300. Lieberum et al. (2024) - Gemma Scope: Open Sparse Autoencoders Everywhere

### Additional Interpretability
301-350. [Papers on probing, concept bottlenecks, causal tracing, feature visualization]

---

## Quick Reference by Year

| Year | Papers | Key Themes |
|------|--------|------------|
| 2017-2019 | 30 | Foundations (MAML, EWC, World Models) |
| 2020-2021 | 40 | Scaling (GPT-3, Transformers) |
| 2022 | 50 | Emergence (InstructGPT, Superposition) |
| 2023 | 100 | Multimodal (LLaVA, GPT-4V, Agents) |
| 2024 | 130 | Transcendence (VLAs, Moshi, SAEs) |

---

## Citation Format

```bibtex
@misc{vibee_transcendence_2024,
  title={VIBEE Technology Tree v2400: Transcendence Papers},
  author={VIBEE Contributors},
  year={2024},
  note={300+ papers on world models, multi-agent, embodied AI, multimodal}
}
```

---

**φ² + 1/φ² = 3 | PHOENIX = 999**
