# Scientific Journals - LLM Training Optimization

**φ² + 1/φ² = 3 | PHOENIX = 999**

## Data Quality & Deduplication

1. **Broder (1997)** - "On the resemblance and containment of documents" - MinHash LSH
2. **Lee et al. (2022)** - "Deduplicating Training Data Makes Language Models Better" - NeurIPS
3. **Penedo et al. (2023)** - "The RefinedWeb Dataset" - FineWeb deduplication

## Attention Mechanisms

4. **Dao et al. (2022)** - "FlashAttention: Fast and Memory-Efficient Exact Attention" - NeurIPS
5. **Dao (2023)** - "FlashAttention-2: Faster Attention with Better Parallelism"
6. **Shah et al. (2024)** - "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision"
7. **Kwon et al. (2023)** - "Efficient Memory Management for Large Language Model Serving with PagedAttention" - vLLM

## Parallelism & Distributed Training

8. **Shoeybi et al. (2019)** - "Megatron-LM: Training Multi-Billion Parameter Language Models"
9. **Rajbhandari et al. (2020)** - "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models" - DeepSpeed
10. **Zhao et al. (2023)** - "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"

## Quantization

11. **Micikevicius et al. (2022)** - "FP8 Formats for Deep Learning" - NVIDIA
12. **Frantar et al. (2022)** - "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
13. **Lin et al. (2023)** - "AWQ: Activation-aware Weight Quantization for LLM Compression"

## Optimizers

14. **Loshchilov & Hutter (2017)** - "Decoupled Weight Decay Regularization" - AdamW
15. **Shazeer & Stern (2018)** - "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
16. **Chen et al. (2023)** - "Symbolic Discovery of Optimization Algorithms" - Lion
17. **Liu et al. (2023)** - "Sophia: A Scalable Stochastic Second-order Optimizer"
18. **Defazio et al. (2024)** - "Schedule-Free Learning"

## Scaling Laws

19. **Kaplan et al. (2020)** - "Scaling Laws for Neural Language Models" - OpenAI
20. **Hoffmann et al. (2022)** - "Training Compute-Optimal Large Language Models" - Chinchilla
21. **Yang et al. (2022)** - "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer" - μTransfer

## Inference Optimization

22. **Leviathan et al. (2023)** - "Fast Inference from Transformers via Speculative Decoding"
23. **Cai et al. (2024)** - "Medusa: Simple LLM Inference Acceleration Framework"
24. **Fu et al. (2024)** - "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding"

## Data Augmentation

25. **Wang et al. (2022)** - "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
26. **Xu et al. (2023)** - "WizardLM: Empowering Large Language Models to Follow Complex Instructions" - Evol-Instruct
27. **Mukherjee et al. (2023)** - "Orca: Progressive Learning from Complex Explanation Traces"

## Benchmarks

28. **Hendrycks et al. (2020)** - "Measuring Massive Multitask Language Understanding" - MMLU
29. **Zellers et al. (2019)** - "HellaSwag: Can a Machine Really Finish Your Sentence?"
30. **Cobbe et al. (2021)** - "Training Verifiers to Solve Math Word Problems" - GSM8K
31. **Chen et al. (2021)** - "Evaluating Large Language Models Trained on Code" - HumanEval
32. **Liu et al. (2023)** - "Is Your Code Generated by ChatGPT Really Correct?" - HumanEval+/MBPP+
33. **Zhuo et al. (2024)** - "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls"

## MoE (Mixture of Experts)

34. **Fedus et al. (2022)** - "Switch Transformers: Scaling to Trillion Parameter Models"
35. **Lepikhin et al. (2020)** - "GShard: Scaling Giant Models with Conditional Computation"

## Kernel Optimization

36. **Tillet et al. (2019)** - "Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations"
37. **Ansel et al. (2024)** - "PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation" - torch.compile

---

**Total: 37 key papers implemented in v1900**

**PHOENIX = 999**
