"""
VIBEE BitNet Benchmark Comparison

–ú–æ–¥—É–ª—å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–π.

–°–≤—è—â–µ–Ω–Ω–∞—è –§–æ—Ä–º—É–ª–∞: V = n √ó 3^k √ó œÄ^m √ó œÜ^p √ó e^q
–ó–æ–ª–æ—Ç–æ–µ –¢–æ–∂–¥–µ—Å—Ç–≤–æ: œÜ¬≤ + 1/œÜ¬≤ = 3 | PHOENIX = 999

Copyright (c) 2024 VIBEE Project
"""

import json
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
from enum import Enum
from datetime import datetime


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Enums
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ComparisonStatus(Enum):
    """–°—Ç–∞—Ç—É—Å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏"""
    IMPROVED = "improved"       # –£–ª—É—á—à–µ–Ω–∏–µ
    REGRESSION = "regression"   # –†–µ–≥—Ä–µ—Å—Å–∏—è
    UNCHANGED = "unchanged"     # –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    NEW = "new"                 # –ù–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞
    REMOVED = "removed"         # –£–¥–∞–ª—ë–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Data Classes
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class ThresholdConfig:
    """–ü–æ—Ä–æ–≥–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–π (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)"""
    latency_mean: float = 5.0
    latency_p95: float = 10.0
    latency_p99: float = 15.0
    throughput: float = 5.0
    memory_bandwidth: float = 10.0
    default: float = 5.0
    
    def get_threshold(self, metric_name: str) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Ä–æ–≥ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏"""
        mapping = {
            "mean": self.latency_mean,
            "p95": self.latency_p95,
            "p99": self.latency_p99,
            "tokens_per_second": self.throughput,
            "bandwidth_gbps": self.memory_bandwidth,
        }
        return mapping.get(metric_name, self.default)


@dataclass
class ComparisonResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–¥–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏"""
    metric_name: str
    baseline_value: float
    current_value: float
    absolute_diff: float
    percent_diff: float
    status: ComparisonStatus
    threshold: float = 5.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "metric_name": self.metric_name,
            "baseline_value": self.baseline_value,
            "current_value": self.current_value,
            "absolute_diff": self.absolute_diff,
            "percent_diff": self.percent_diff,
            "status": self.status.value,
            "threshold": self.threshold,
        }


@dataclass
class BenchmarkComparison:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö benchmark —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    benchmark_name: str
    comparisons: List[ComparisonResult] = field(default_factory=list)
    
    @property
    def has_regression(self) -> bool:
        return any(c.status == ComparisonStatus.REGRESSION for c in self.comparisons)
    
    @property
    def has_improvement(self) -> bool:
        return any(c.status == ComparisonStatus.IMPROVED for c in self.comparisons)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "benchmark_name": self.benchmark_name,
            "comparisons": [c.to_dict() for c in self.comparisons],
            "has_regression": self.has_regression,
            "has_improvement": self.has_improvement,
        }


@dataclass
class RegressionReport:
    """–ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –æ —Ä–µ–≥—Ä–µ—Å—Å–∏—è—Ö"""
    baseline_timestamp: str
    current_timestamp: str
    baseline_model: str
    current_model: str
    benchmark_comparisons: List[BenchmarkComparison] = field(default_factory=list)
    
    @property
    def total_metrics(self) -> int:
        return sum(len(bc.comparisons) for bc in self.benchmark_comparisons)
    
    @property
    def improved_count(self) -> int:
        return sum(
            1 for bc in self.benchmark_comparisons
            for c in bc.comparisons
            if c.status == ComparisonStatus.IMPROVED
        )
    
    @property
    def regression_count(self) -> int:
        return sum(
            1 for bc in self.benchmark_comparisons
            for c in bc.comparisons
            if c.status == ComparisonStatus.REGRESSION
        )
    
    @property
    def unchanged_count(self) -> int:
        return sum(
            1 for bc in self.benchmark_comparisons
            for c in bc.comparisons
            if c.status == ComparisonStatus.UNCHANGED
        )
    
    @property
    def has_regressions(self) -> bool:
        return self.regression_count > 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "baseline_timestamp": self.baseline_timestamp,
            "current_timestamp": self.current_timestamp,
            "baseline_model": self.baseline_model,
            "current_model": self.current_model,
            "total_metrics": self.total_metrics,
            "improved_count": self.improved_count,
            "regression_count": self.regression_count,
            "unchanged_count": self.unchanged_count,
            "has_regressions": self.has_regressions,
            "benchmark_comparisons": [bc.to_dict() for bc in self.benchmark_comparisons],
        }
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict(), indent=2)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Comparison Functions
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def compare_metric(
    metric_name: str,
    baseline_value: float,
    current_value: float,
    threshold: float = 5.0,
    higher_is_better: bool = False
) -> ComparisonResult:
    """
    –°—Ä–∞–≤–Ω–∏—Ç—å –¥–≤–µ –º–µ—Ç—Ä–∏–∫–∏.
    
    Args:
        metric_name: –ò–º—è –º–µ—Ç—Ä–∏–∫–∏
        baseline_value: –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        current_value: –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        threshold: –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (%)
        higher_is_better: True –¥–ª—è throughput, False –¥–ª—è latency
    
    Returns:
        ComparisonResult
    """
    if baseline_value == 0:
        percent_diff = 100.0 if current_value > 0 else 0.0
    else:
        percent_diff = ((current_value - baseline_value) / baseline_value) * 100
    
    absolute_diff = current_value - baseline_value
    
    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç–∞—Ç—É—Å
    if higher_is_better:
        # –î–ª—è throughput: –±–æ–ª—å—à–µ = –ª—É—á—à–µ
        if percent_diff > threshold:
            status = ComparisonStatus.IMPROVED
        elif percent_diff < -threshold:
            status = ComparisonStatus.REGRESSION
        else:
            status = ComparisonStatus.UNCHANGED
    else:
        # –î–ª—è latency: –º–µ–Ω—å—à–µ = –ª—É—á—à–µ
        if percent_diff < -threshold:
            status = ComparisonStatus.IMPROVED
        elif percent_diff > threshold:
            status = ComparisonStatus.REGRESSION
        else:
            status = ComparisonStatus.UNCHANGED
    
    return ComparisonResult(
        metric_name=metric_name,
        baseline_value=baseline_value,
        current_value=current_value,
        absolute_diff=absolute_diff,
        percent_diff=percent_diff,
        status=status,
        threshold=threshold,
    )


def is_higher_better(metric_name: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –±–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ª—É—á—à–∏–º"""
    higher_better_metrics = {
        "tokens_per_second",
        "throughput",
        "bandwidth_gbps",
        "total_tokens",
    }
    return metric_name in higher_better_metrics


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# BenchmarkComparator Class
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class BenchmarkComparator:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤.
    
    –ü—Ä–∏–º–µ—Ä:
        >>> comparator = BenchmarkComparator()
        >>> comparator.load_baseline("baseline.json")
        >>> comparator.load_current("current.json")
        >>> report = comparator.compare()
        >>> print(report.to_markdown())
    """
    
    def __init__(self, thresholds: Optional[ThresholdConfig] = None):
        """
        Args:
            thresholds: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ—Ä–æ–≥–æ–≤
        """
        self._thresholds = thresholds or ThresholdConfig()
        self._baseline: Optional[Dict[str, Any]] = None
        self._current: Optional[Dict[str, Any]] = None
    
    def load_baseline(self, path: Union[str, Path]) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å baseline —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ JSON"""
        with open(path, 'r') as f:
            self._baseline = json.load(f)
    
    def load_current(self, path: Union[str, Path]) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ JSON"""
        with open(path, 'r') as f:
            self._current = json.load(f)
    
    def set_baseline(self, data: Dict[str, Any]) -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å baseline –∏–∑ dict"""
        self._baseline = data
    
    def set_current(self, data: Dict[str, Any]) -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å current –∏–∑ dict"""
        self._current = data
    
    def compare(self) -> RegressionReport:
        """
        –°—Ä–∞–≤–Ω–∏—Ç—å baseline –∏ current —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
        
        Returns:
            RegressionReport
        """
        if not self._baseline or not self._current:
            raise ValueError("Baseline –∏ current –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        
        report = RegressionReport(
            baseline_timestamp=self._baseline.get("timestamp", "unknown"),
            current_timestamp=self._current.get("timestamp", "unknown"),
            baseline_model=self._baseline.get("model_path", "unknown"),
            current_model=self._current.get("model_path", "unknown"),
        )
        
        # –°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å baseline —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∏–º–µ–Ω–∏
        baseline_results = {
            r["name"]: r for r in self._baseline.get("results", [])
        }
        
        # –°—Ä–∞–≤–Ω–∏—Ç—å –∫–∞–∂–¥—ã–π —Ç–µ–∫—É—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        for current_result in self._current.get("results", []):
            name = current_result["name"]
            baseline_result = baseline_results.get(name)
            
            if baseline_result:
                comparison = self._compare_results(baseline_result, current_result)
                report.benchmark_comparisons.append(comparison)
        
        return report
    
    def _compare_results(
        self,
        baseline: Dict[str, Any],
        current: Dict[str, Any]
    ) -> BenchmarkComparison:
        """–°—Ä–∞–≤–Ω–∏—Ç—å –¥–≤–∞ benchmark —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        comparison = BenchmarkComparison(benchmark_name=baseline["name"])
        
        baseline_metrics = baseline.get("metrics", {})
        current_metrics = current.get("metrics", {})
        
        # –°—Ä–∞–≤–Ω–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
        all_metrics = set(baseline_metrics.keys()) | set(current_metrics.keys())
        
        for metric_name in all_metrics:
            baseline_value = baseline_metrics.get(metric_name)
            current_value = current_metrics.get(metric_name)
            
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –Ω–µ-—á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            if not isinstance(baseline_value, (int, float)) or \
               not isinstance(current_value, (int, float)):
                continue
            
            threshold = self._thresholds.get_threshold(metric_name)
            higher_better = is_higher_better(metric_name)
            
            result = compare_metric(
                metric_name=metric_name,
                baseline_value=float(baseline_value),
                current_value=float(current_value),
                threshold=threshold,
                higher_is_better=higher_better,
            )
            
            comparison.comparisons.append(result)
        
        return comparison
    
    def save_baseline(self, data: Dict[str, Any], path: Union[str, Path]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –Ω–æ–≤—ã–π baseline"""
        with open(path, 'w') as f:
            json.dump(data, f, indent=2)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Report Generators
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def generate_text_report(report: RegressionReport) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞"""
    lines = []
    
    lines.append("=" * 70)
    lines.append("VIBEE BitNet Benchmark Comparison Report")
    lines.append("=" * 70)
    lines.append("")
    lines.append(f"Baseline: {report.baseline_timestamp} ({report.baseline_model})")
    lines.append(f"Current:  {report.current_timestamp} ({report.current_model})")
    lines.append("")
    lines.append(f"Total metrics:  {report.total_metrics}")
    lines.append(f"Improved:       {report.improved_count} ‚úÖ")
    lines.append(f"Regressions:    {report.regression_count} ‚ùå")
    lines.append(f"Unchanged:      {report.unchanged_count}")
    lines.append("")
    
    for bc in report.benchmark_comparisons:
        lines.append("-" * 70)
        lines.append(f"Benchmark: {bc.benchmark_name}")
        lines.append("-" * 70)
        
        for c in bc.comparisons:
            status_icon = {
                ComparisonStatus.IMPROVED: "‚úÖ",
                ComparisonStatus.REGRESSION: "‚ùå",
                ComparisonStatus.UNCHANGED: "‚ûñ",
                ComparisonStatus.NEW: "üÜï",
                ComparisonStatus.REMOVED: "üóëÔ∏è",
            }.get(c.status, "?")
            
            sign = "+" if c.percent_diff > 0 else ""
            lines.append(
                f"  {status_icon} {c.metric_name}: "
                f"{c.baseline_value:.3f} ‚Üí {c.current_value:.3f} "
                f"({sign}{c.percent_diff:.1f}%)"
            )
        
        lines.append("")
    
    lines.append("=" * 70)
    
    if report.has_regressions:
        lines.append("‚ö†Ô∏è  REGRESSIONS DETECTED!")
    else:
        lines.append("‚úÖ No regressions detected")
    
    lines.append("=" * 70)
    
    return "\n".join(lines)


def generate_markdown_report(report: RegressionReport) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –æ—Ç—á—ë—Ç–∞ –¥–ª—è GitHub"""
    lines = []
    
    lines.append("# Benchmark Comparison Report")
    lines.append("")
    lines.append(f"**Baseline:** {report.baseline_timestamp}")
    lines.append(f"**Current:** {report.current_timestamp}")
    lines.append("")
    
    # Summary
    lines.append("## Summary")
    lines.append("")
    lines.append("| Metric | Count |")
    lines.append("|--------|-------|")
    lines.append(f"| Total | {report.total_metrics} |")
    lines.append(f"| Improved | {report.improved_count} |")
    lines.append(f"| Regressions | {report.regression_count} |")
    lines.append(f"| Unchanged | {report.unchanged_count} |")
    lines.append("")
    
    # Status badge
    if report.has_regressions:
        lines.append("**Status:** ‚ùå Regressions detected")
    else:
        lines.append("**Status:** ‚úÖ No regressions")
    lines.append("")
    
    # Details
    lines.append("## Details")
    lines.append("")
    
    for bc in report.benchmark_comparisons:
        lines.append(f"### {bc.benchmark_name.title()}")
        lines.append("")
        lines.append("| Metric | Baseline | Current | Diff | Status |")
        lines.append("|--------|----------|---------|------|--------|")
        
        for c in bc.comparisons:
            status_icon = {
                ComparisonStatus.IMPROVED: "‚úÖ",
                ComparisonStatus.REGRESSION: "‚ùå",
                ComparisonStatus.UNCHANGED: "‚ûñ",
            }.get(c.status, "?")
            
            sign = "+" if c.percent_diff > 0 else ""
            lines.append(
                f"| {c.metric_name} | {c.baseline_value:.3f} | "
                f"{c.current_value:.3f} | {sign}{c.percent_diff:.1f}% | {status_icon} |"
            )
        
        lines.append("")
    
    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CLI Integration
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def compare_files(
    baseline_path: Union[str, Path],
    current_path: Union[str, Path],
    output_format: str = "text",
    fail_on_regression: bool = False
) -> int:
    """
    –°—Ä–∞–≤–Ω–∏—Ç—å –¥–≤–∞ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.
    
    Args:
        baseline_path: –ü—É—Ç—å –∫ baseline JSON
        current_path: –ü—É—Ç—å –∫ current JSON
        output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ (text, markdown, json)
        fail_on_regression: –í–µ—Ä–Ω—É—Ç—å exit code 1 –ø—Ä–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—è—Ö
    
    Returns:
        Exit code (0 = success, 1 = regressions)
    """
    comparator = BenchmarkComparator()
    comparator.load_baseline(baseline_path)
    comparator.load_current(current_path)
    
    report = comparator.compare()
    
    if output_format == "json":
        print(report.to_json())
    elif output_format == "markdown":
        print(generate_markdown_report(report))
    else:
        print(generate_text_report(report))
    
    if fail_on_regression and report.has_regressions:
        return 1
    
    return 0
