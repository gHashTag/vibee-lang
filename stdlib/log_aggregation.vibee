// =============================================================================
// Vibee OS â€” Log Aggregation Module
// Centralized log collection, processing, and forwarding
// =============================================================================

use datetime.{DateTime, Duration}
use logger.{Level, LogRecord}

// -----------------------------------------------------------------------------
// Core Types
// -----------------------------------------------------------------------------

/// Aggregated log entry
struct LogEntry {
    id: String
    timestamp: DateTime
    level: Level
    message: String
    source: LogSource
    fields: Map<String, Any>
    tags: Map<String, String>
}

impl LogEntry {
    fn new(level: Level, message: String) -> Self {
        LogEntry {
            id: Uuid.v4().to_string(),
            timestamp: DateTime.now(),
            level: level,
            message: message,
            source: LogSource.default(),
            fields: Map.new(),
            tags: Map.new()
        }
    }
    
    fn from_record(record: LogRecord) -> Self {
        LogEntry {
            id: Uuid.v4().to_string(),
            timestamp: record.timestamp,
            level: record.level,
            message: record.message,
            source: LogSource.default(),
            fields: record.fields,
            tags: Map.new()
        }
    }
    
    fn with_field(key: String, value: Any) -> Self {
        self.fields.insert(key, value)
        self
    }
    
    fn with_tag(key: String, value: String) -> Self {
        self.tags.insert(key, value)
        self
    }
    
    fn with_source(source: LogSource) -> Self {
        self.source = source
        self
    }
}

/// Log source information
struct LogSource {
    service: String
    host: String
    environment: String
    version: Option<String>
    instance_id: Option<String>
}

impl LogSource {
    fn default() -> Self {
        LogSource {
            service: Env.get("SERVICE_NAME").unwrap_or("unknown"),
            host: System.hostname(),
            environment: Env.get("ENVIRONMENT").unwrap_or("production"),
            version: Env.get("SERVICE_VERSION"),
            instance_id: Env.get("INSTANCE_ID")
        }
    }
    
    fn new(service: String) -> Self {
        LogSource {
            service: service,
            host: System.hostname(),
            environment: "production",
            version: None,
            instance_id: None
        }
    }
}

// -----------------------------------------------------------------------------
// Log Aggregator
// -----------------------------------------------------------------------------

/// Main log aggregation agent
actor LogAggregator {
    state config: AggregatorConfig
    state buffer: [LogEntry]
    state exporters: [Box<dyn LogExporter>]
    state processors: [Box<dyn LogProcessor>]
    state running: Bool
    
    fn new(config: AggregatorConfig) -> Self {
        LogAggregator {
            config: config,
            buffer: [],
            exporters: [],
            processors: [],
            running: false
        }
    }
    
    fn add_exporter<E: LogExporter>(exporter: E) -> Self {
        self.exporters.push(Box.new(exporter))
        self
    }
    
    fn add_processor<P: LogProcessor>(processor: P) -> Self {
        self.processors.push(Box.new(processor))
        self
    }
    
    async fn start() {
        self.running = true
        self.start_flush_loop().await
    }
    
    async fn stop() {
        self.running = false
        self.flush().await
    }
    
    fn ingest(entry: LogEntry) {
        // Apply processors
        var processed = entry
        for processor in self.processors.iter() {
            if let Some(p) = processor.process(processed) {
                processed = p
            } else {
                return // Entry filtered out
            }
        }
        
        self.buffer.push(processed)
        
        if self.buffer.len() >= self.config.batch_size {
            self.flush_sync()
        }
    }
    
    fn ingest_record(record: LogRecord) {
        self.ingest(LogEntry.from_record(record))
    }
    
    async fn flush() {
        if self.buffer.is_empty() { return }
        
        let batch = self.buffer.drain(..).collect()
        
        for exporter in self.exporters.iter() {
            if let Err(e) = exporter.export(batch.clone()).await {
                Logger.error("Log export failed: \(e)")
            }
        }
    }
    
    fn flush_sync() {
        Runtime.block_on(self.flush())
    }
    
    async fn start_flush_loop() {
        while self.running {
            Timer.sleep(self.config.flush_interval).await
            self.flush().await
        }
    }
}

struct AggregatorConfig {
    batch_size: Int
    flush_interval: Duration
    max_buffer_size: Int
    source: LogSource
}

impl AggregatorConfig {
    fn new() -> Self {
        AggregatorConfig {
            batch_size: 100,
            flush_interval: Duration.seconds(5),
            max_buffer_size: 10000,
            source: LogSource.default()
        }
    }
    
    fn with_batch_size(size: Int) -> Self {
        self.batch_size = size
        self
    }
    
    fn with_flush_interval(interval: Duration) -> Self {
        self.flush_interval = interval
        self
    }
}

// -----------------------------------------------------------------------------
// Log Processors
// -----------------------------------------------------------------------------

trait LogProcessor {
    fn process(entry: LogEntry) -> Option<LogEntry>
    fn name() -> String
}

/// Filter logs by level
struct LevelFilter { min_level: Level }

impl LevelFilter {
    fn new(min_level: Level) -> Self {
        LevelFilter { min_level: min_level }
    }
}

impl LogProcessor for LevelFilter {
    fn process(entry: LogEntry) -> Option<LogEntry> {
        if entry.level >= self.min_level { Some(entry) } else { None }
    }
    fn name() -> String { "level_filter" }
}

/// Add fields to all logs
struct FieldEnricher { fields: Map<String, Any> }

impl FieldEnricher {
    fn new() -> Self { FieldEnricher { fields: Map.new() } }
    fn add(key: String, value: Any) -> Self { self.fields.insert(key, value); self }
}

impl LogProcessor for FieldEnricher {
    fn process(entry: LogEntry) -> Option<LogEntry> {
        var e = entry
        for (k, v) in self.fields.iter() {
            e.fields.insert(k.clone(), v.clone())
        }
        Some(e)
    }
    fn name() -> String { "field_enricher" }
}

/// Redact sensitive data
struct SensitiveDataRedactor { patterns: [Regex], replacement: String }

impl SensitiveDataRedactor {
    fn new() -> Self {
        SensitiveDataRedactor {
            patterns: [
                Regex.new(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b"),
                Regex.new(r"\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b"),
                Regex.new(r"\b\d{3}-\d{2}-\d{4}\b")
            ],
            replacement: "[REDACTED]"
        }
    }
    
    fn add_pattern(pattern: String) -> Self {
        self.patterns.push(Regex.new(pattern))
        self
    }
}

impl LogProcessor for SensitiveDataRedactor {
    fn process(entry: LogEntry) -> Option<LogEntry> {
        var e = entry
        for pattern in self.patterns.iter() {
            e.message = pattern.replace_all(e.message, self.replacement.clone())
        }
        Some(e)
    }
    fn name() -> String { "sensitive_data_redactor" }
}

/// Sample logs
struct LogSampler { rate: Float, counter: Int }

impl LogSampler {
    fn new(rate: Float) -> Self {
        LogSampler { rate: rate.clamp(0.0, 1.0), counter: 0 }
    }
}

impl LogProcessor for LogSampler {
    fn process(entry: LogEntry) -> Option<LogEntry> {
        // Always pass errors
        if entry.level >= Level.Error { return Some(entry) }
        
        self.counter += 1
        if Random.float() < self.rate { Some(entry) } else { None }
    }
    fn name() -> String { "log_sampler" }
}

// -----------------------------------------------------------------------------
// Log Exporters
// -----------------------------------------------------------------------------

trait LogExporter {
    async fn export(entries: [LogEntry]) -> Result<(), ExportError>
    fn name() -> String
}

/// Export to Elasticsearch
struct ElasticsearchExporter {
    url: String
    index_pattern: String
    client: HttpClient
}

impl ElasticsearchExporter {
    fn new(url: String) -> Self {
        ElasticsearchExporter {
            url: url,
            index_pattern: "logs-%Y.%m.%d",
            client: HttpClient.new()
        }
    }
    
    fn with_index_pattern(pattern: String) -> Self {
        self.index_pattern = pattern
        self
    }
}

impl LogExporter for ElasticsearchExporter {
    async fn export(entries: [LogEntry]) -> Result<(), ExportError> {
        let index = DateTime.now().format(self.index_pattern)
        var bulk_body = String.new()
        
        for entry in entries {
            bulk_body.push_str("{\"index\":{\"_index\":\"\(index)\"}}\n")
            bulk_body.push_str(Json.encode(entry))
            bulk_body.push_str("\n")
        }
        
        self.client.post("\(self.url)/_bulk")
            .header("Content-Type", "application/x-ndjson")
            .body(bulk_body)
            .send()
            .await?
        
        Ok(())
    }
    fn name() -> String { "elasticsearch" }
}

/// Export to Loki
struct LokiExporter { url: String, client: HttpClient }

impl LokiExporter {
    fn new(url: String) -> Self {
        LokiExporter { url: url, client: HttpClient.new() }
    }
}

impl LogExporter for LokiExporter {
    async fn export(entries: [LogEntry]) -> Result<(), ExportError> {
        let streams = self.group_by_labels(entries)
        let payload = {"streams": streams}
        
        self.client.post("\(self.url)/loki/api/v1/push")
            .header("Content-Type", "application/json")
            .body(Json.encode(payload))
            .send()
            .await?
        
        Ok(())
    }
    fn name() -> String { "loki" }
}

impl LokiExporter {
    fn group_by_labels(entries: [LogEntry]) -> [Map<String, Any>] {
        var groups: Map<String, [LogEntry]> = Map.new()
        
        for entry in entries {
            let key = self.labels_key(entry.tags.clone())
            groups.entry(key).or_insert([]).push(entry)
        }
        
        groups.iter().map(|(_, entries)| {
            let labels = entries.first().map(|e| e.tags.clone()).unwrap_or(Map.new())
            let values: [[String; 2]] = entries.iter().map(|e| {
                [e.timestamp.unix_nano().to_string(), e.message.clone()]
            }).collect()
            
            {"stream": labels, "values": values}
        }).collect()
    }
    
    fn labels_key(labels: Map<String, String>) -> String {
        labels.iter().map(|(k, v)| "\(k)=\(v)").collect::<Vec<_>>().sort().join(",")
    }
}

/// Export to file
struct FileExporter { path: String, format: LogFormat }

impl FileExporter {
    fn new(path: String) -> Self {
        FileExporter { path: path, format: LogFormat.Json }
    }
    
    fn with_format(format: LogFormat) -> Self {
        self.format = format
        self
    }
}

impl LogExporter for FileExporter {
    async fn export(entries: [LogEntry]) -> Result<(), ExportError> {
        var file = File.open_append(self.path.clone())?
        
        for entry in entries {
            let line = match self.format {
                LogFormat.Json => Json.encode(entry),
                LogFormat.Text => "\(entry.timestamp) [\(entry.level)] \(entry.message)"
            }
            file.write_line(line)?
        }
        
        Ok(())
    }
    fn name() -> String { "file" }
}

enum LogFormat { Json, Text }

/// Export to stdout (for container environments)
struct StdoutExporter { format: LogFormat }

impl StdoutExporter {
    fn new() -> Self { StdoutExporter { format: LogFormat.Json } }
}

impl LogExporter for StdoutExporter {
    async fn export(entries: [LogEntry]) -> Result<(), ExportError> {
        for entry in entries {
            println(Json.encode(entry))
        }
        Ok(())
    }
    fn name() -> String { "stdout" }
}

enum ExportError {
    NetworkError(String)
    SerializationError(String)
    IoError(String)
}

// -----------------------------------------------------------------------------
// Query Interface
// -----------------------------------------------------------------------------

struct LogQuery {
    start_time: Option<DateTime>
    end_time: Option<DateTime>
    levels: [Level]
    services: [String]
    search: Option<String>
    limit: Int
    offset: Int
}

impl LogQuery {
    fn new() -> Self {
        LogQuery {
            start_time: None,
            end_time: None,
            levels: [],
            services: [],
            search: None,
            limit: 100,
            offset: 0
        }
    }
    
    fn since(time: DateTime) -> Self { self.start_time = Some(time); self }
    fn until(time: DateTime) -> Self { self.end_time = Some(time); self }
    fn with_level(level: Level) -> Self { self.levels.push(level); self }
    fn with_service(service: String) -> Self { self.services.push(service); self }
    fn search(query: String) -> Self { self.search = Some(query); self }
    fn limit(n: Int) -> Self { self.limit = n; self }
    fn offset(n: Int) -> Self { self.offset = n; self }
}

// -----------------------------------------------------------------------------
// Global Instance
// -----------------------------------------------------------------------------

static GLOBAL_AGGREGATOR: Mutex<Option<LogAggregator>> = Mutex.new(None)

fn init(config: AggregatorConfig) {
    *GLOBAL_AGGREGATOR.lock() = Some(LogAggregator.new(config))
}

fn aggregator() -> Option<&LogAggregator> {
    GLOBAL_AGGREGATOR.lock().as_ref()
}

fn ingest(entry: LogEntry) {
    if let Some(a) = aggregator() { a.ingest(entry) }
}

fn ingest_record(record: LogRecord) {
    if let Some(a) = aggregator() { a.ingest_record(record) }
}

// -----------------------------------------------------------------------------
// Tests
// -----------------------------------------------------------------------------

test "log entry creation" {
    let entry = LogEntry.new(Level.Info, "test message")
        .with_field("user_id", "123")
        .with_tag("env", "test")
    
    assert_eq(entry.level, Level.Info)?
    assert_eq(entry.fields.get("user_id"), Some("123"))?
}

test "level filter" {
    let filter = LevelFilter.new(Level.Warn)
    
    let info = LogEntry.new(Level.Info, "info")
    let error = LogEntry.new(Level.Error, "error")
    
    assert(filter.process(info).is_none())?
    assert(filter.process(error).is_some())?
}

test "field enricher" {
    let enricher = FieldEnricher.new().add("version", "1.0.0")
    let entry = LogEntry.new(Level.Info, "test")
    
    let result = enricher.process(entry).unwrap()
    assert_eq(result.fields.get("version"), Some("1.0.0"))?
}

test "log query builder" {
    let query = LogQuery.new()
        .with_level(Level.Error)
        .with_service("api")
        .limit(50)
    
    assert_eq(query.levels.len(), 1)?
    assert_eq(query.limit, 50)?
}
