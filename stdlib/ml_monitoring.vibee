// =============================================================================
// Vibee OS â€” ML Monitoring Module
// Model monitoring, drift detection, and observability
// =============================================================================

use datetime::{DateTime, Duration}
use uuid::{UUID}
use result::{Result, Ok, Err}
use tensor::{Tensor}
use stats::{mean, std, percentile}
use metrics::{Counter, Histogram, Gauge}

// -----------------------------------------------------------------------------
// Drift Types
// -----------------------------------------------------------------------------

/// Type of drift detected
enum DriftType {
    DataDrift
    ConceptDrift
    PredictionDrift
    FeatureDrift(String)
    LabelDrift
    
    fn to_string() -> String {
        match self {
            DataDrift => "data_drift",
            ConceptDrift => "concept_drift",
            PredictionDrift => "prediction_drift",
            FeatureDrift(name) => "feature_drift:\(name)",
            LabelDrift => "label_drift"
        }
    }
}

/// Drift severity level
enum DriftSeverity {
    None
    Low
    Medium
    High
    Critical
    
    fn from_score(score: Float, thresholds: DriftThresholds) -> Self {
        if score >= thresholds.critical { DriftSeverity.Critical }
        else if score >= thresholds.high { DriftSeverity.High }
        else if score >= thresholds.medium { DriftSeverity.Medium }
        else if score >= thresholds.low { DriftSeverity.Low }
        else { DriftSeverity.None }
    }
}

/// Drift detection thresholds
struct DriftThresholds {
    low: Float
    medium: Float
    high: Float
    critical: Float
    
    fn default() -> Self {
        DriftThresholds { low: 0.1, medium: 0.3, high: 0.5, critical: 0.7 }
    }
}

// -----------------------------------------------------------------------------
// Drift Detection Results
// -----------------------------------------------------------------------------

/// Result of drift detection
struct DriftResult {
    drift_type: DriftType
    score: Float
    severity: DriftSeverity
    p_value: Option<Float>
    details: Map<String, Float>
    detected_at: DateTime
    
    fn new(drift_type: DriftType, score: Float, severity: DriftSeverity) -> Self {
        DriftResult {
            drift_type: drift_type,
            score: score,
            severity: severity,
            p_value: None,
            details: Map.empty(),
            detected_at: DateTime.now()
        }
    }
    
    fn with_p_value(p_value: Float) -> Self {
        self.p_value = Some(p_value)
        self
    }
    
    fn is_drifted() -> Bool {
        match self.severity {
            DriftSeverity.None | DriftSeverity.Low => false,
            _ => true
        }
    }
}

// -----------------------------------------------------------------------------
// Statistical Tests
// -----------------------------------------------------------------------------

/// Kolmogorov-Smirnov test for distribution comparison
fn ks_test(reference: [Float], current: [Float]) -> (Float, Float) {
    let n1 = reference.len() as Float
    let n2 = current.len() as Float
    
    var ref_sorted = reference.clone()
    var cur_sorted = current.clone()
    ref_sorted.sort()
    cur_sorted.sort()
    
    var max_diff = 0.0
    var i = 0
    var j = 0
    
    while i < ref_sorted.len() && j < cur_sorted.len() {
        let cdf1 = (i + 1) as Float / n1
        let cdf2 = (j + 1) as Float / n2
        let diff = (cdf1 - cdf2).abs()
        max_diff = max_diff.max(diff)
        
        if ref_sorted[i] <= cur_sorted[j] { i += 1 }
        else { j += 1 }
    }
    
    // Approximate p-value
    let en = (n1 * n2 / (n1 + n2)).sqrt()
    let p_value = (-2.0 * (max_diff * en).pow(2.0)).exp()
    
    (max_diff, p_value)
}

/// Population Stability Index
fn psi(reference: [Float], current: [Float], bins: Int = 10) -> Float {
    let min_val = reference.iter().chain(current.iter()).fold(Float.INFINITY, |a, b| a.min(*b))
    let max_val = reference.iter().chain(current.iter()).fold(Float.NEG_INFINITY, |a, b| a.max(*b))
    let bin_width = (max_val - min_val) / bins as Float
    
    var ref_hist = [0.0; bins]
    var cur_hist = [0.0; bins]
    
    for v in reference {
        let bin = ((v - min_val) / bin_width).floor() as Int
        let bin = bin.min(bins - 1)
        ref_hist[bin] += 1.0
    }
    
    for v in current {
        let bin = ((v - min_val) / bin_width).floor() as Int
        let bin = bin.min(bins - 1)
        cur_hist[bin] += 1.0
    }
    
    // Normalize
    let ref_total = reference.len() as Float
    let cur_total = current.len() as Float
    
    var psi_value = 0.0
    for i in 0..bins {
        let ref_pct = (ref_hist[i] / ref_total).max(0.0001)
        let cur_pct = (cur_hist[i] / cur_total).max(0.0001)
        psi_value += (cur_pct - ref_pct) * (cur_pct / ref_pct).ln()
    }
    
    psi_value
}

/// Jensen-Shannon Divergence
fn js_divergence(p: [Float], q: [Float]) -> Float {
    let m: [Float] = p.iter().zip(q.iter()).map(|(a, b)| (a + b) / 2.0).collect()
    (kl_divergence(p, m) + kl_divergence(q, m)) / 2.0
}

/// Kullback-Leibler Divergence
fn kl_divergence(p: [Float], q: [Float]) -> Float {
    var kl = 0.0
    for (pi, qi) in p.iter().zip(q.iter()) {
        if *pi > 0.0 && *qi > 0.0 {
            kl += pi * (pi / qi).ln()
        }
    }
    kl
}

// -----------------------------------------------------------------------------
// Drift Detector
// -----------------------------------------------------------------------------

/// Drift detector for a single feature
struct FeatureDriftDetector {
    name: String
    reference_data: [Float]
    thresholds: DriftThresholds
    method: DriftMethod
    
    fn new(name: String, reference_data: [Float]) -> Self {
        FeatureDriftDetector {
            name: name,
            reference_data: reference_data,
            thresholds: DriftThresholds.default(),
            method: DriftMethod.PSI
        }
    }
    
    fn with_thresholds(thresholds: DriftThresholds) -> Self {
        self.thresholds = thresholds
        self
    }
    
    fn with_method(method: DriftMethod) -> Self {
        self.method = method
        self
    }
    
    fn detect(current_data: [Float]) -> DriftResult {
        let (score, p_value) = match self.method {
            DriftMethod.KS => ks_test(self.reference_data, current_data),
            DriftMethod.PSI => (psi(self.reference_data, current_data), 0.0),
            DriftMethod.JS => (js_divergence(
                self.to_distribution(self.reference_data),
                self.to_distribution(current_data)
            ), 0.0)
        }
        
        let severity = DriftSeverity.from_score(score, self.thresholds)
        var result = DriftResult.new(DriftType.FeatureDrift(self.name.clone()), score, severity)
        if p_value > 0.0 { result = result.with_p_value(p_value) }
        result
    }
    
    fn to_distribution(data: [Float]) -> [Float] {
        let sum: Float = data.iter().sum()
        if sum == 0.0 { return data.clone() }
        data.iter().map(|x| x / sum).collect()
    }
}

/// Drift detection method
enum DriftMethod {
    KS      // Kolmogorov-Smirnov
    PSI     // Population Stability Index
    JS      // Jensen-Shannon Divergence
}

// -----------------------------------------------------------------------------
// Data Quality Monitor
// -----------------------------------------------------------------------------

/// Data quality metrics
struct DataQualityMetrics {
    null_rate: Float
    unique_rate: Float
    mean: Float
    std: Float
    min: Float
    max: Float
    percentiles: Map<Int, Float>
    
    fn from_data(data: [Float]) -> Self {
        let non_null: [Float] = data.iter().filter(|x| !x.is_nan()).cloned().collect()
        let n = data.len() as Float
        let n_valid = non_null.len() as Float
        
        var sorted = non_null.clone()
        sorted.sort()
        
        DataQualityMetrics {
            null_rate: (n - n_valid) / n,
            unique_rate: non_null.iter().collect::<Set<_>>().len() as Float / n_valid,
            mean: non_null.iter().sum::<Float>() / n_valid,
            std: {
                let m = non_null.iter().sum::<Float>() / n_valid
                (non_null.iter().map(|x| (x - m).pow(2.0)).sum::<Float>() / n_valid).sqrt()
            },
            min: sorted.first().cloned().unwrap_or(0.0),
            max: sorted.last().cloned().unwrap_or(0.0),
            percentiles: Map.from([
                (25, sorted[(n_valid * 0.25) as Int]),
                (50, sorted[(n_valid * 0.50) as Int]),
                (75, sorted[(n_valid * 0.75) as Int]),
                (95, sorted[(n_valid * 0.95) as Int]),
                (99, sorted[(n_valid * 0.99) as Int])
            ])
        }
    }
}

/// Data quality monitor
struct DataQualityMonitor {
    feature_name: String
    reference_metrics: DataQualityMetrics
    null_rate_threshold: Float
    
    fn new(feature_name: String, reference_data: [Float]) -> Self {
        DataQualityMonitor {
            feature_name: feature_name,
            reference_metrics: DataQualityMetrics.from_data(reference_data),
            null_rate_threshold: 0.1
        }
    }
    
    fn check(current_data: [Float]) -> DataQualityReport {
        let current_metrics = DataQualityMetrics.from_data(current_data)
        
        var issues = []
        
        if current_metrics.null_rate > self.null_rate_threshold {
            issues.push(DataQualityIssue.HighNullRate {
                feature: self.feature_name.clone(),
                rate: current_metrics.null_rate
            })
        }
        
        let mean_change = (current_metrics.mean - self.reference_metrics.mean).abs() / self.reference_metrics.std
        if mean_change > 2.0 {
            issues.push(DataQualityIssue.MeanShift {
                feature: self.feature_name.clone(),
                reference: self.reference_metrics.mean,
                current: current_metrics.mean
            })
        }
        
        let std_ratio = current_metrics.std / self.reference_metrics.std
        if std_ratio < 0.5 || std_ratio > 2.0 {
            issues.push(DataQualityIssue.VarianceChange {
                feature: self.feature_name.clone(),
                reference: self.reference_metrics.std,
                current: current_metrics.std
            })
        }
        
        DataQualityReport {
            feature: self.feature_name.clone(),
            metrics: current_metrics,
            issues: issues,
            checked_at: DateTime.now()
        }
    }
}

/// Data quality issue
enum DataQualityIssue {
    HighNullRate { feature: String, rate: Float }
    MeanShift { feature: String, reference: Float, current: Float }
    VarianceChange { feature: String, reference: Float, current: Float }
    OutOfRange { feature: String, value: Float, min: Float, max: Float }
    InvalidType { feature: String, expected: String, actual: String }
}

/// Data quality report
struct DataQualityReport {
    feature: String
    metrics: DataQualityMetrics
    issues: [DataQualityIssue]
    checked_at: DateTime
    
    fn has_issues() -> Bool {
        !self.issues.is_empty()
    }
}

// -----------------------------------------------------------------------------
// Model Performance Monitor
// -----------------------------------------------------------------------------

/// Performance metrics over time
struct PerformanceWindow {
    start_time: DateTime
    end_time: DateTime
    predictions: Int
    accuracy: Option<Float>
    precision: Option<Float>
    recall: Option<Float>
    f1: Option<Float>
    mse: Option<Float>
    mae: Option<Float>
    latency_p50: Float
    latency_p95: Float
    latency_p99: Float
    error_rate: Float
}

/// Model performance monitor
actor PerformanceMonitor {
    model_name: String
    model_version: Int
    windows: [PerformanceWindow]
    window_duration: Duration
    predictions: [PredictionRecord]
    latencies: [Float]
    errors: Int
    
    fn new(model_name: String, model_version: Int, window_duration: Duration) -> Self {
        PerformanceMonitor {
            model_name: model_name,
            model_version: model_version,
            windows: [],
            window_duration: window_duration,
            predictions: [],
            latencies: [],
            errors: 0
        }
    }
    
    fn record_prediction(prediction: Tensor, ground_truth: Option<Tensor>, latency_ms: Float) {
        self.predictions.push(PredictionRecord {
            prediction: prediction,
            ground_truth: ground_truth,
            timestamp: DateTime.now()
        })
        self.latencies.push(latency_ms)
    }
    
    fn record_error() {
        self.errors += 1
    }
    
    fn compute_window() -> PerformanceWindow {
        let now = DateTime.now()
        let start = now.sub(self.window_duration)
        
        var sorted_latencies = self.latencies.clone()
        sorted_latencies.sort()
        let n = sorted_latencies.len()
        
        PerformanceWindow {
            start_time: start,
            end_time: now,
            predictions: self.predictions.len(),
            accuracy: self.compute_accuracy(),
            precision: None,
            recall: None,
            f1: None,
            mse: self.compute_mse(),
            mae: self.compute_mae(),
            latency_p50: if n > 0 { sorted_latencies[n / 2] } else { 0.0 },
            latency_p95: if n > 0 { sorted_latencies[(n * 95) / 100] } else { 0.0 },
            latency_p99: if n > 0 { sorted_latencies[(n * 99) / 100] } else { 0.0 },
            error_rate: self.errors as Float / (self.predictions.len() + self.errors) as Float
        }
    }
    
    fn compute_accuracy() -> Option<Float> {
        let labeled: [PredictionRecord] = self.predictions.iter()
            .filter(|p| p.ground_truth.is_some())
            .cloned()
            .collect()
        
        if labeled.is_empty() { return None }
        
        var correct = 0
        for record in labeled {
            let pred = record.prediction.argmax()
            let truth = record.ground_truth.unwrap().argmax()
            if pred == truth { correct += 1 }
        }
        
        Some(correct as Float / labeled.len() as Float)
    }
    
    fn compute_mse() -> Option<Float> {
        let labeled: [PredictionRecord] = self.predictions.iter()
            .filter(|p| p.ground_truth.is_some())
            .cloned()
            .collect()
        
        if labeled.is_empty() { return None }
        
        var sum = 0.0
        for record in labeled {
            let diff = record.prediction.sub(record.ground_truth.unwrap())
            sum += diff.data.iter().map(|x| x * x).sum::<Float>()
        }
        
        Some(sum / labeled.len() as Float)
    }
    
    fn compute_mae() -> Option<Float> {
        let labeled: [PredictionRecord] = self.predictions.iter()
            .filter(|p| p.ground_truth.is_some())
            .cloned()
            .collect()
        
        if labeled.is_empty() { return None }
        
        var sum = 0.0
        for record in labeled {
            let diff = record.prediction.sub(record.ground_truth.unwrap())
            sum += diff.data.iter().map(|x| x.abs()).sum::<Float>()
        }
        
        Some(sum / labeled.len() as Float)
    }
    
    fn flush_window() {
        let window = self.compute_window()
        self.windows.push(window)
        self.predictions.clear()
        self.latencies.clear()
        self.errors = 0
    }
    
    fn get_windows() -> [PerformanceWindow] {
        self.windows.clone()
    }
}

/// Prediction record
struct PredictionRecord {
    prediction: Tensor
    ground_truth: Option<Tensor>
    timestamp: DateTime
}

// -----------------------------------------------------------------------------
// Alerting
// -----------------------------------------------------------------------------

/// Alert severity
enum AlertSeverity {
    Info
    Warning
    Error
    Critical
}

/// Alert definition
struct Alert {
    id: UUID
    name: String
    severity: AlertSeverity
    message: String
    model_name: String
    model_version: Int
    triggered_at: DateTime
    resolved_at: Option<DateTime>
    metadata: Map<String, String>
    
    fn new(name: String, severity: AlertSeverity, message: String, model_name: String, model_version: Int) -> Self {
        Alert {
            id: UUID.v4(),
            name: name,
            severity: severity,
            message: message,
            model_name: model_name,
            model_version: model_version,
            triggered_at: DateTime.now(),
            resolved_at: None,
            metadata: Map.empty()
        }
    }
    
    fn resolve() {
        self.resolved_at = Some(DateTime.now())
    }
    
    fn is_resolved() -> Bool {
        self.resolved_at.is_some()
    }
}

/// Alert rule
struct AlertRule {
    name: String
    condition: fn(PerformanceWindow) -> Bool
    severity: AlertSeverity
    message_template: String
    cooldown: Duration
    last_triggered: Option<DateTime>
    
    fn new(name: String, condition: fn(PerformanceWindow) -> Bool, severity: AlertSeverity) -> Self {
        AlertRule {
            name: name,
            condition: condition,
            severity: severity,
            message_template: "",
            cooldown: Duration.minutes(5),
            last_triggered: None
        }
    }
    
    fn with_message(template: String) -> Self {
        self.message_template = template
        self
    }
    
    fn with_cooldown(cooldown: Duration) -> Self {
        self.cooldown = cooldown
        self
    }
    
    fn check(window: PerformanceWindow, model_name: String, model_version: Int) -> Option<Alert> {
        // Check cooldown
        if let Some(last) = self.last_triggered {
            if DateTime.now().duration_since(last) < self.cooldown {
                return None
            }
        }
        
        if (self.condition)(window) {
            self.last_triggered = Some(DateTime.now())
            Some(Alert.new(self.name.clone(), self.severity, self.message_template.clone(), model_name, model_version))
        } else {
            None
        }
    }
}

/// Alert manager
actor AlertManager {
    rules: [AlertRule]
    alerts: [Alert]
    handlers: [Box<dyn AlertHandler>]
    
    fn new() -> Self {
        AlertManager { rules: [], alerts: [], handlers: [] }
    }
    
    fn add_rule(rule: AlertRule) {
        self.rules.push(rule)
    }
    
    fn add_handler(handler: Box<dyn AlertHandler>) {
        self.handlers.push(handler)
    }
    
    fn check(window: PerformanceWindow, model_name: String, model_version: Int) {
        for rule in self.rules.iter_mut() {
            if let Some(alert) = rule.check(window.clone(), model_name.clone(), model_version) {
                self.alerts.push(alert.clone())
                for handler in self.handlers {
                    handler.handle(alert.clone())
                }
            }
        }
    }
    
    fn get_active_alerts() -> [Alert] {
        self.alerts.iter().filter(|a| !a.is_resolved()).cloned().collect()
    }
    
    fn resolve_alert(id: UUID) {
        for alert in self.alerts.iter_mut() {
            if alert.id == id {
                alert.resolve()
                break
            }
        }
    }
}

/// Alert handler trait
trait AlertHandler {
    fn handle(alert: Alert)
}

/// Log alert handler
struct LogAlertHandler {}

impl AlertHandler for LogAlertHandler {
    fn handle(alert: Alert) {
        println("[ALERT] [\(alert.severity)] \(alert.name): \(alert.message)")
    }
}

/// Webhook alert handler
struct WebhookAlertHandler {
    url: String
    
    fn new(url: String) -> Self {
        WebhookAlertHandler { url: url }
    }
}

impl AlertHandler for WebhookAlertHandler {
    fn handle(alert: Alert) {
        @native("http_post", self.url, alert.to_json().to_string())
    }
}

// -----------------------------------------------------------------------------
// Monitoring Dashboard
// -----------------------------------------------------------------------------

/// Monitoring dashboard data
struct MonitoringDashboard {
    model_name: String
    model_version: Int
    current_window: PerformanceWindow
    drift_results: [DriftResult]
    quality_reports: [DataQualityReport]
    active_alerts: [Alert]
    
    fn new(model_name: String, model_version: Int) -> Self {
        MonitoringDashboard {
            model_name: model_name,
            model_version: model_version,
            current_window: PerformanceWindow {
                start_time: DateTime.now(),
                end_time: DateTime.now(),
                predictions: 0,
                accuracy: None,
                precision: None,
                recall: None,
                f1: None,
                mse: None,
                mae: None,
                latency_p50: 0.0,
                latency_p95: 0.0,
                latency_p99: 0.0,
                error_rate: 0.0
            },
            drift_results: [],
            quality_reports: [],
            active_alerts: []
        }
    }
    
    fn health_score() -> Float {
        var score = 100.0
        
        // Deduct for drift
        for drift in self.drift_results {
            match drift.severity {
                DriftSeverity.Low => score -= 5.0,
                DriftSeverity.Medium => score -= 15.0,
                DriftSeverity.High => score -= 30.0,
                DriftSeverity.Critical => score -= 50.0,
                _ => {}
            }
        }
        
        // Deduct for quality issues
        for report in self.quality_reports {
            score -= report.issues.len() as Float * 5.0
        }
        
        // Deduct for alerts
        for alert in self.active_alerts {
            match alert.severity {
                AlertSeverity.Warning => score -= 10.0,
                AlertSeverity.Error => score -= 20.0,
                AlertSeverity.Critical => score -= 40.0,
                _ => {}
            }
        }
        
        score.max(0.0)
    }
}

// -----------------------------------------------------------------------------
// Global Monitoring Instance
// -----------------------------------------------------------------------------

static MONITORS: Map<String, PerformanceMonitor> = Map.empty()
static ALERT_MANAGER: AlertManager = AlertManager.new()

fn create_monitor(model_name: String, model_version: Int) -> PerformanceMonitor {
    let key = "\(model_name):\(model_version)"
    let monitor = PerformanceMonitor.new(model_name, model_version, Duration.minutes(5))
    MONITORS.insert(key, monitor.clone())
    monitor
}

fn get_monitor(model_name: String, model_version: Int) -> Option<PerformanceMonitor> {
    let key = "\(model_name):\(model_version)"
    MONITORS.get(key).cloned()
}

fn add_alert_rule(rule: AlertRule) {
    ALERT_MANAGER.add_rule(rule)
}

// -----------------------------------------------------------------------------
// Predefined Alert Rules
// -----------------------------------------------------------------------------

fn high_latency_rule(threshold_ms: Float) -> AlertRule {
    AlertRule.new(
        "high_latency",
        |w: PerformanceWindow| w.latency_p95 > threshold_ms,
        AlertSeverity.Warning
    ).with_message("P95 latency exceeded \(threshold_ms)ms")
}

fn high_error_rate_rule(threshold: Float) -> AlertRule {
    AlertRule.new(
        "high_error_rate",
        |w: PerformanceWindow| w.error_rate > threshold,
        AlertSeverity.Error
    ).with_message("Error rate exceeded \(threshold * 100.0)%")
}

fn accuracy_drop_rule(threshold: Float) -> AlertRule {
    AlertRule.new(
        "accuracy_drop",
        |w: PerformanceWindow| w.accuracy.map(|a| a < threshold).unwrap_or(false),
        AlertSeverity.Critical
    ).with_message("Accuracy dropped below \(threshold * 100.0)%")
}

// -----------------------------------------------------------------------------
// Tests
// -----------------------------------------------------------------------------

test "ks_test" {
    let ref_data = [1.0, 2.0, 3.0, 4.0, 5.0]
    let cur_data = [1.0, 2.0, 3.0, 4.0, 5.0]
    
    let (stat, p_value) = ks_test(ref_data, cur_data)
    assert(stat < 0.1)?
}

test "psi_calculation" {
    let ref_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]
    let cur_data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]
    
    let psi_value = psi(ref_data, cur_data)
    assert(psi_value < 0.1)?
}

test "drift_detection" {
    let detector = FeatureDriftDetector.new("feature1", [1.0, 2.0, 3.0, 4.0, 5.0])
    let result = detector.detect([1.0, 2.0, 3.0, 4.0, 5.0])
    
    assert_eq(result.severity, DriftSeverity.None)?
}

test "data_quality_metrics" {
    let data = [1.0, 2.0, 3.0, 4.0, 5.0]
    let metrics = DataQualityMetrics.from_data(data)
    
    assert_eq(metrics.mean, 3.0)?
    assert_eq(metrics.min, 1.0)?
    assert_eq(metrics.max, 5.0)?
}

test "alert_rule" {
    let rule = high_latency_rule(100.0)
    
    let window = PerformanceWindow {
        start_time: DateTime.now(),
        end_time: DateTime.now(),
        predictions: 100,
        accuracy: Some(0.95),
        precision: None,
        recall: None,
        f1: None,
        mse: None,
        mae: None,
        latency_p50: 50.0,
        latency_p95: 150.0,
        latency_p99: 200.0,
        error_rate: 0.01
    }
    
    let alert = rule.check(window, "test-model", 1)
    assert(alert.is_some())?
}

test "monitoring_dashboard_health" {
    let dashboard = MonitoringDashboard.new("test-model", 1)
    assert_eq(dashboard.health_score(), 100.0)?
}
