// =============================================================================
// Vibee OS â€” Scrape Scheduler Module
// Scheduled and distributed web scraping
// =============================================================================

use scraper::{Scraper, ScrapedPage, ScrapeError}
use crawler::{Crawler, CrawlerConfig, CrawlResult}
use data_extractor::{DataExtractor, Value, ExtractionError}
use cron::{CronExpr, CronError}
use result::{Result, Ok, Err}
use option::{Option, Some, None}

// =============================================================================
// Scrape Scheduler
// =============================================================================

/// Scheduler for periodic scraping jobs
actor ScrapeScheduler {
    state jobs: Map<String, ScrapeJob>
    state running: Bool
    state results: Map<String, [ScrapeResult]>
    state max_results_per_job: Int
    state handlers: SchedulerHandlers
    
    /// Create new scheduler
    fn new() -> Self {
        ScrapeScheduler {
            jobs: Map.empty(),
            running: false,
            results: Map.empty(),
            max_results_per_job: 100,
            handlers: SchedulerHandlers.default()
        }
    }
    
    /// Set max results to keep per job
    fn max_results(n: Int) -> Self {
        ScrapeScheduler { max_results_per_job: n, ..self }
    }
    
    /// Add scrape job
    fn add_job(job: ScrapeJob) -> Result<(), SchedulerError> {
        // Validate cron expression
        CronExpr.parse(job.schedule.clone())
            .map_err(|e| SchedulerError.InvalidSchedule(e.to_string()))?
        
        self.jobs.set(job.id.clone(), job)
        self.results.set(job.id.clone(), [])
        Ok(())
    }
    
    /// Remove job
    fn remove_job(id: String) -> Bool {
        self.jobs.remove(id.clone()).is_some() && self.results.remove(id).is_some()
    }
    
    /// Enable job
    fn enable_job(id: String) {
        if let Some(job) = self.jobs.get_mut(id) {
            job.enabled = true
        }
    }
    
    /// Disable job
    fn disable_job(id: String) {
        if let Some(job) = self.jobs.get_mut(id) {
            job.enabled = false
        }
    }
    
    /// Set result handler
    fn on_result(handler: fn(String, ScrapeResult)) -> Self {
        self.handlers.on_result = Some(handler)
        self
    }
    
    /// Set error handler
    fn on_error(handler: fn(String, SchedulerError)) -> Self {
        self.handlers.on_error = Some(handler)
        self
    }
    
    /// Start scheduler
    fn start() {
        self.running = true
        self.run_loop()
    }
    
    /// Stop scheduler
    fn stop() {
        self.running = false
    }
    
    fn run_loop() {
        while self.running {
            let now = DateTime.now()
            
            for (id, job) in self.jobs.iter_mut() {
                if !job.enabled { continue }
                
                // Check if job should run
                if let Some(next_run) = job.next_run {
                    if now >= next_run {
                        spawn { self.execute_job(id.clone(), job.clone()) }
                        
                        // Update next run time
                        if let Ok(cron) = CronExpr.parse(job.schedule.clone()) {
                            job.next_run = cron.next(now)
                        }
                        job.last_run = Some(now)
                    }
                } else {
                    // Initialize next run
                    if let Ok(cron) = CronExpr.parse(job.schedule.clone()) {
                        job.next_run = cron.next(now)
                    }
                }
            }
            
            // Sleep until next check
            @native("sleep_ms", 1000)
        }
    }
    
    fn execute_job(id: String, job: ScrapeJob) {
        let start_time = DateTime.now()
        
        match job.execute() {
            Ok(data) => {
                let result = ScrapeResult {
                    job_id: id.clone(),
                    timestamp: start_time,
                    duration: DateTime.now() - start_time,
                    status: ScrapeStatus.Success,
                    data: Some(data),
                    error: None
                }
                
                self.store_result(id.clone(), result.clone())
                
                if let Some(handler) = self.handlers.on_result {
                    handler(id, result)
                }
            }
            Err(e) => {
                let result = ScrapeResult {
                    job_id: id.clone(),
                    timestamp: start_time,
                    duration: DateTime.now() - start_time,
                    status: ScrapeStatus.Failed,
                    data: None,
                    error: Some(e.to_string())
                }
                
                self.store_result(id.clone(), result)
                
                if let Some(handler) = self.handlers.on_error {
                    handler(id, SchedulerError.JobFailed(e.to_string()))
                }
            }
        }
    }
    
    fn store_result(job_id: String, result: ScrapeResult) {
        let results = self.results.entry(job_id).or_insert([])
        results.push(result)
        
        // Trim old results
        while results.len() > self.max_results_per_job {
            results.remove(0)
        }
    }
    
    /// Run job immediately
    fn run_now(id: String) -> Result<ScrapeResult, SchedulerError> {
        let job = self.jobs.get(id.clone())
            .ok_or(SchedulerError.JobNotFound(id.clone()))?
        
        let start_time = DateTime.now()
        
        match job.execute() {
            Ok(data) => {
                let result = ScrapeResult {
                    job_id: id.clone(),
                    timestamp: start_time,
                    duration: DateTime.now() - start_time,
                    status: ScrapeStatus.Success,
                    data: Some(data),
                    error: None
                }
                self.store_result(id, result.clone())
                Ok(result)
            }
            Err(e) => {
                let result = ScrapeResult {
                    job_id: id.clone(),
                    timestamp: start_time,
                    duration: DateTime.now() - start_time,
                    status: ScrapeStatus.Failed,
                    data: None,
                    error: Some(e.to_string())
                }
                self.store_result(id, result)
                Err(SchedulerError.JobFailed(e.to_string()))
            }
        }
    }
    
    /// Get job results
    fn get_results(job_id: String) -> [ScrapeResult] {
        self.results.get(job_id).cloned().unwrap_or([])
    }
    
    /// Get latest result
    fn get_latest_result(job_id: String) -> Option<ScrapeResult> {
        self.results.get(job_id)?.last().cloned()
    }
    
    /// Get all jobs
    fn list_jobs() -> [ScrapeJob] {
        self.jobs.values().collect()
    }
    
    /// Get job by ID
    fn get_job(id: String) -> Option<ScrapeJob> {
        self.jobs.get(id).cloned()
    }
    
    /// Get scheduler stats
    fn stats() -> SchedulerStats {
        var total_runs = 0
        var successful_runs = 0
        var failed_runs = 0
        
        for (_, results) in self.results.iter() {
            for result in results {
                total_runs += 1
                match result.status {
                    ScrapeStatus.Success => successful_runs += 1
                    ScrapeStatus.Failed => failed_runs += 1
                    _ => {}
                }
            }
        }
        
        SchedulerStats {
            total_jobs: self.jobs.len(),
            enabled_jobs: self.jobs.values().filter(|j| j.enabled).count(),
            total_runs: total_runs,
            successful_runs: successful_runs,
            failed_runs: failed_runs
        }
    }
}

// =============================================================================
// Scrape Job
// =============================================================================

/// Scrape job definition
struct ScrapeJob {
    id: String
    name: String
    schedule: String
    job_type: JobType
    config: JobConfig
    enabled: Bool
    last_run: Option<DateTime>
    next_run: Option<DateTime>
    
    /// Create URL scrape job
    fn url(id: String, url: String, schedule: String) -> Self {
        ScrapeJob {
            id: id,
            name: url.clone(),
            schedule: schedule,
            job_type: JobType.SingleUrl(url),
            config: JobConfig.default(),
            enabled: true,
            last_run: None,
            next_run: None
        }
    }
    
    /// Create multi-URL scrape job
    fn urls(id: String, urls: [String], schedule: String) -> Self {
        ScrapeJob {
            id: id,
            name: format!("{} URLs", urls.len()),
            schedule: schedule,
            job_type: JobType.MultipleUrls(urls),
            config: JobConfig.default(),
            enabled: true,
            last_run: None,
            next_run: None
        }
    }
    
    /// Create crawl job
    fn crawl(id: String, seed_url: String, schedule: String) -> Self {
        ScrapeJob {
            id: id,
            name: format!("Crawl: {}", seed_url),
            schedule: schedule,
            job_type: JobType.Crawl(seed_url),
            config: JobConfig.default(),
            enabled: true,
            last_run: None,
            next_run: None
        }
    }
    
    /// Set job name
    fn name(name: String) -> Self {
        ScrapeJob { name: name, ..self }
    }
    
    /// Set job config
    fn config(config: JobConfig) -> Self {
        ScrapeJob { config: config, ..self }
    }
    
    /// Set extractor
    fn extractor(extractor: DataExtractor) -> Self {
        ScrapeJob { config: JobConfig { extractor: Some(extractor), ..self.config }, ..self }
    }
    
    /// Execute job
    fn execute() -> Result<JobData, ScrapeError> {
        let scraper = Scraper.new()
            .user_agent(self.config.user_agent)
            .timeout(self.config.timeout_ms)
            .delay(self.config.delay_ms)
        
        match self.job_type {
            JobType.SingleUrl(url) => {
                let page = scraper.fetch(url)?
                let data = self.extract_data(page)?
                Ok(JobData.Single(data))
            }
            JobType.MultipleUrls(urls) => {
                var results = []
                for url in urls {
                    match scraper.fetch(url) {
                        Ok(page) => {
                            match self.extract_data(page) {
                                Ok(data) => results.push(data)
                                Err(_) => {}
                            }
                        }
                        Err(_) => {}
                    }
                }
                Ok(JobData.Multiple(results))
            }
            JobType.Crawl(seed) => {
                let crawler = Crawler.new()
                    .config(CrawlerConfig.default()
                        .max_pages(self.config.max_pages)
                        .max_depth(self.config.max_depth)
                        .delay(self.config.delay_ms))
                    .seed(seed)
                
                let result = crawler.start()
                Ok(JobData.Crawl(result))
            }
        }
    }
    
    fn extract_data(page: ScrapedPage) -> Result<Map<String, Value>, ScrapeError> {
        match self.config.extractor {
            Some(extractor) => {
                extractor.extract_page(page)
                    .map_err(|e| ScrapeError.ExtractionError(e.to_string()))
            }
            None => {
                // Default extraction
                var data = Map.empty()
                data.set("url", Value.String(page.url))
                data.set("title", Value.String(page.title().unwrap_or("")))
                data.set("text", Value.String(page.text_content()))
                Ok(data)
            }
        }
    }
}

/// Job type
enum JobType {
    SingleUrl(String)
    MultipleUrls([String])
    Crawl(String)
}

/// Job configuration
struct JobConfig {
    user_agent: String
    timeout_ms: Int64
    delay_ms: Int64
    max_pages: Int
    max_depth: Int
    extractor: Option<DataExtractor>
    
    fn default() -> Self {
        JobConfig {
            user_agent: "Vibee-Scheduler/1.0",
            timeout_ms: 30000,
            delay_ms: 1000,
            max_pages: 100,
            max_depth: 3,
            extractor: None
        }
    }
}

/// Job data result
enum JobData {
    Single(Map<String, Value>)
    Multiple([Map<String, Value>])
    Crawl(CrawlResult)
}

// =============================================================================
// Scrape Result
// =============================================================================

/// Scrape result
struct ScrapeResult {
    job_id: String
    timestamp: DateTime
    duration: Duration
    status: ScrapeStatus
    data: Option<JobData>
    error: Option<String>
}

/// Scrape status
enum ScrapeStatus {
    Success
    Failed
    Partial
    Skipped
}

// =============================================================================
// Scheduler Handlers
// =============================================================================

/// Scheduler event handlers
struct SchedulerHandlers {
    on_result: Option<fn(String, ScrapeResult)>
    on_error: Option<fn(String, SchedulerError)>
    
    fn default() -> Self {
        SchedulerHandlers {
            on_result: None,
            on_error: None
        }
    }
}

/// Scheduler statistics
struct SchedulerStats {
    total_jobs: Int
    enabled_jobs: Int
    total_runs: Int
    successful_runs: Int
    failed_runs: Int
    
    fn success_rate() -> Float {
        if self.total_runs > 0 {
            self.successful_runs as Float / self.total_runs as Float
        } else {
            0.0
        }
    }
}

// =============================================================================
// Job Builder
// =============================================================================

/// Fluent job builder
struct JobBuilder {
    id: String
    name: Option<String>
    schedule: String
    urls: [String]
    is_crawl: Bool
    config: JobConfig
    extractor: Option<DataExtractor>
    
    fn new(id: String) -> Self {
        JobBuilder {
            id: id,
            name: None,
            schedule: "0 * * * *",  // Every hour
            urls: [],
            is_crawl: false,
            config: JobConfig.default(),
            extractor: None
        }
    }
    
    fn name(name: String) -> Self {
        JobBuilder { name: Some(name), ..self }
    }
    
    fn schedule(schedule: String) -> Self {
        JobBuilder { schedule: schedule, ..self }
    }
    
    fn every_minute() -> Self {
        self.schedule("* * * * *")
    }
    
    fn every_hour() -> Self {
        self.schedule("0 * * * *")
    }
    
    fn every_day() -> Self {
        self.schedule("0 0 * * *")
    }
    
    fn every_week() -> Self {
        self.schedule("0 0 * * 0")
    }
    
    fn url(url: String) -> Self {
        self.urls.push(url)
        self
    }
    
    fn urls(urls: [String]) -> Self {
        self.urls.extend(urls)
        self
    }
    
    fn crawl(seed: String) -> Self {
        self.urls = [seed]
        JobBuilder { is_crawl: true, ..self }
    }
    
    fn user_agent(ua: String) -> Self {
        JobBuilder { config: JobConfig { user_agent: ua, ..self.config }, ..self }
    }
    
    fn timeout(ms: Int64) -> Self {
        JobBuilder { config: JobConfig { timeout_ms: ms, ..self.config }, ..self }
    }
    
    fn delay(ms: Int64) -> Self {
        JobBuilder { config: JobConfig { delay_ms: ms, ..self.config }, ..self }
    }
    
    fn max_pages(n: Int) -> Self {
        JobBuilder { config: JobConfig { max_pages: n, ..self.config }, ..self }
    }
    
    fn max_depth(n: Int) -> Self {
        JobBuilder { config: JobConfig { max_depth: n, ..self.config }, ..self }
    }
    
    fn extractor(extractor: DataExtractor) -> Self {
        JobBuilder { extractor: Some(extractor), ..self }
    }
    
    fn build() -> ScrapeJob {
        let job_type = if self.is_crawl {
            JobType.Crawl(self.urls.first().cloned().unwrap_or(""))
        } else if self.urls.len() == 1 {
            JobType.SingleUrl(self.urls[0].clone())
        } else {
            JobType.MultipleUrls(self.urls.clone())
        }
        
        let name = self.name.unwrap_or_else(|| {
            match job_type {
                JobType.SingleUrl(url) => url
                JobType.MultipleUrls(urls) => format!("{} URLs", urls.len())
                JobType.Crawl(seed) => format!("Crawl: {}", seed)
            }
        })
        
        ScrapeJob {
            id: self.id,
            name: name,
            schedule: self.schedule,
            job_type: job_type,
            config: JobConfig { extractor: self.extractor, ..self.config },
            enabled: true,
            last_run: None,
            next_run: None
        }
    }
}

// =============================================================================
// Batch Scheduler
// =============================================================================

/// Batch scraping scheduler
struct BatchScheduler {
    jobs: [BatchJob]
    concurrency: Int
    
    fn new() -> Self {
        BatchScheduler { jobs: [], concurrency: 5 }
    }
    
    fn concurrency(n: Int) -> Self {
        BatchScheduler { concurrency: n, ..self }
    }
    
    fn add(url: String, extractor: Option<DataExtractor>) -> Self {
        self.jobs.push(BatchJob { url: url, extractor: extractor })
        self
    }
    
    fn add_urls(urls: [String]) -> Self {
        for url in urls {
            self.jobs.push(BatchJob { url: url, extractor: None })
        }
        self
    }
    
    fn run() -> BatchResult {
        let scraper = Scraper.new()
        var results = []
        var errors = []
        
        // Process in batches
        for chunk in self.jobs.chunks(self.concurrency) {
            let batch_results = chunk.iter().map(|job| {
                match scraper.fetch(job.url.clone()) {
                    Ok(page) => {
                        let data = match job.extractor {
                            Some(ext) => ext.extract_page(page).ok()
                            None => None
                        }
                        BatchItemResult {
                            url: job.url.clone(),
                            success: true,
                            data: data,
                            error: None
                        }
                    }
                    Err(e) => {
                        BatchItemResult {
                            url: job.url.clone(),
                            success: false,
                            data: None,
                            error: Some(e.to_string())
                        }
                    }
                }
            }).collect()
            
            results.extend(batch_results)
        }
        
        BatchResult { results: results }
    }
}

/// Batch job
struct BatchJob {
    url: String
    extractor: Option<DataExtractor>
}

/// Batch item result
struct BatchItemResult {
    url: String
    success: Bool
    data: Option<Map<String, Value>>
    error: Option<String>
}

/// Batch result
struct BatchResult {
    results: [BatchItemResult]
    
    fn successful() -> [BatchItemResult] {
        self.results.iter().filter(|r| r.success).collect()
    }
    
    fn failed() -> [BatchItemResult] {
        self.results.iter().filter(|r| !r.success).collect()
    }
    
    fn success_rate() -> Float {
        if self.results.is_empty() {
            0.0
        } else {
            self.successful().len() as Float / self.results.len() as Float
        }
    }
}

// =============================================================================
// Errors
// =============================================================================

/// Scheduler error
enum SchedulerError {
    InvalidSchedule(String)
    JobNotFound(String)
    JobFailed(String)
    AlreadyRunning
}

impl Display for SchedulerError {
    fn fmt(f: Formatter) {
        match self {
            InvalidSchedule(s) => f.write(format!("Invalid schedule: {}", s))
            JobNotFound(s) => f.write(format!("Job not found: {}", s))
            JobFailed(s) => f.write(format!("Job failed: {}", s))
            AlreadyRunning => f.write("Scheduler already running")
        }
    }
}

// =============================================================================
// Convenience Functions
// =============================================================================

/// Create scheduler
fn scheduler() -> ScrapeScheduler {
    ScrapeScheduler.new()
}

/// Create job builder
fn job(id: String) -> JobBuilder {
    JobBuilder.new(id)
}

/// Create batch scheduler
fn batch() -> BatchScheduler {
    BatchScheduler.new()
}

// =============================================================================
// Tests
// =============================================================================

test "job builder" {
    let job = JobBuilder.new("test")
        .name("Test Job")
        .every_hour()
        .url("https://example.com")
        .timeout(5000)
        .build()
    
    assert_eq(job.id, "test")?
    assert_eq(job.name, "Test Job")?
    assert_eq(job.schedule, "0 * * * *")?
}

test "scheduler stats" {
    let stats = SchedulerStats {
        total_jobs: 10,
        enabled_jobs: 8,
        total_runs: 100,
        successful_runs: 95,
        failed_runs: 5
    }
    
    assert_eq(stats.success_rate(), 0.95)?
}

test "batch result" {
    let result = BatchResult {
        results: [
            BatchItemResult { url: "a", success: true, data: None, error: None },
            BatchItemResult { url: "b", success: false, data: None, error: Some("error") },
            BatchItemResult { url: "c", success: true, data: None, error: None }
        ]
    }
    
    assert_eq(result.successful().len(), 2)?
    assert_eq(result.failed().len(), 1)?
}

test "job config default" {
    let config = JobConfig.default()
    assert_eq(config.timeout_ms, 30000)?
    assert_eq(config.delay_ms, 1000)?
}
