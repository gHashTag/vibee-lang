// =============================================================================
// Vibee OS â€” Dataset Module
// Data loading and preprocessing for machine learning
// =============================================================================

use tensor::{Tensor, Shape}

// -----------------------------------------------------------------------------
// Dataset Trait
// -----------------------------------------------------------------------------

/// Base trait for all datasets
trait Dataset {
    type Item
    fn len() -> Int
    fn get(index: Int) -> Self.Item
    fn is_empty() -> Bool { self.len() == 0 }
}

/// Map-style dataset with tensor items
trait TensorDataset: Dataset<Item = (Tensor, Tensor)> {}

// -----------------------------------------------------------------------------
// Basic Datasets
// -----------------------------------------------------------------------------

/// In-memory tensor dataset
struct MemoryDataset {
    data: Tensor
    targets: Tensor
    
    fn new(data: Tensor, targets: Tensor) -> Self {
        MemoryDataset { data: data, targets: targets }
    }
    
    fn from_arrays(data: [[Float]], targets: [Float]) -> Self {
        let rows = data.len()
        let cols = data[0].len()
        var flat_data = []
        for row in data { flat_data.extend(row.iter()) }
        MemoryDataset {
            data: Tensor.new(flat_data, Shape.matrix(rows, cols)),
            targets: Tensor.from_vec(targets)
        }
    }
}

impl Dataset for MemoryDataset {
    type Item = (Tensor, Tensor)
    fn len() -> Int { self.data.shape.rows() }
    fn get(index: Int) -> (Tensor, Tensor) {
        let cols = self.data.shape.cols()
        let row_data = self.data.data[(index * cols)..((index + 1) * cols)].to_vec()
        (Tensor.from_vec(row_data), Tensor.scalar(self.targets.data[index]))
    }
}

/// Subset of a dataset
struct Subset<D: Dataset> {
    dataset: D
    indices: [Int]
    
    fn new(dataset: D, indices: [Int]) -> Self {
        Subset { dataset: dataset, indices: indices }
    }
}

impl<D: Dataset> Dataset for Subset<D> {
    type Item = D.Item
    fn len() -> Int { self.indices.len() }
    fn get(index: Int) -> D.Item { self.dataset.get(self.indices[index]) }
}

/// Concatenated datasets
struct ConcatDataset<D: Dataset> {
    datasets: [D]
    cumulative_sizes: [Int]
    
    fn new(datasets: [D]) -> Self {
        var cumulative = []
        var total = 0
        for d in datasets {
            total += d.len()
            cumulative.push(total)
        }
        ConcatDataset { datasets: datasets, cumulative_sizes: cumulative }
    }
}

impl<D: Dataset> Dataset for ConcatDataset<D> {
    type Item = D.Item
    fn len() -> Int { self.cumulative_sizes.last().unwrap_or(0) }
    fn get(index: Int) -> D.Item {
        var dataset_idx = 0
        for (i, size) in self.cumulative_sizes.iter().enumerate() {
            if index < *size {
                dataset_idx = i
                break
            }
        }
        let offset = if dataset_idx == 0 { 0 } else { self.cumulative_sizes[dataset_idx - 1] }
        self.datasets[dataset_idx].get(index - offset)
    }
}

// -----------------------------------------------------------------------------
// Sampler
// -----------------------------------------------------------------------------

/// Base trait for samplers
trait Sampler {
    fn iter() -> Iterator<Item = Int>
    fn len() -> Int
}

/// Sequential sampler
struct SequentialSampler {
    data_source_len: Int
    
    fn new(data_source_len: Int) -> Self {
        SequentialSampler { data_source_len: data_source_len }
    }
}

impl Sampler for SequentialSampler {
    fn iter() -> Iterator<Item = Int> { (0..self.data_source_len).iter() }
    fn len() -> Int { self.data_source_len }
}

/// Random sampler
struct RandomSampler {
    data_source_len: Int
    replacement: Bool
    num_samples: Option<Int>
    
    fn new(data_source_len: Int, replacement: Bool = false, num_samples: Option<Int> = None) -> Self {
        RandomSampler { data_source_len: data_source_len, replacement: replacement, num_samples: num_samples }
    }
}

impl Sampler for RandomSampler {
    fn iter() -> Iterator<Item = Int> {
        let n = self.num_samples.unwrap_or(self.data_source_len)
        if self.replacement {
            (0..n).map(|_| Random.int(0, self.data_source_len)).iter()
        } else {
            var indices = (0..self.data_source_len).collect::<Vec<_>>()
            shuffle(indices)
            indices[0..n].iter()
        }
    }
    fn len() -> Int { self.num_samples.unwrap_or(self.data_source_len) }
}

/// Weighted random sampler
struct WeightedRandomSampler {
    weights: [Float]
    num_samples: Int
    replacement: Bool
    
    fn new(weights: [Float], num_samples: Int, replacement: Bool = true) -> Self {
        WeightedRandomSampler { weights: weights, num_samples: num_samples, replacement: replacement }
    }
}

impl Sampler for WeightedRandomSampler {
    fn iter() -> Iterator<Item = Int> {
        // Weighted sampling
        let total = self.weights.iter().sum()
        let normalized = self.weights.iter().map(|w| w / total).collect::<Vec<_>>()
        
        var samples = []
        for _ in 0..self.num_samples {
            let r = Random.float()
            var cumsum = 0.0
            for (i, w) in normalized.iter().enumerate() {
                cumsum += *w
                if r < cumsum {
                    samples.push(i)
                    break
                }
            }
        }
        samples.iter()
    }
    fn len() -> Int { self.num_samples }
}

/// Batch sampler
struct BatchSampler {
    sampler: Box<dyn Sampler>
    batch_size: Int
    drop_last: Bool
    
    fn new(sampler: Box<dyn Sampler>, batch_size: Int, drop_last: Bool = false) -> Self {
        BatchSampler { sampler: sampler, batch_size: batch_size, drop_last: drop_last }
    }
}

impl Sampler for BatchSampler {
    fn iter() -> Iterator<Item = Int> {
        // Returns batch indices
        self.sampler.iter()
    }
    fn len() -> Int {
        if self.drop_last {
            self.sampler.len() / self.batch_size
        } else {
            (self.sampler.len() + self.batch_size - 1) / self.batch_size
        }
    }
}

// -----------------------------------------------------------------------------
// DataLoader
// -----------------------------------------------------------------------------

/// Batch of data
struct Batch {
    data: Tensor
    targets: Tensor
    
    fn new(data: Tensor, targets: Tensor) -> Self {
        Batch { data: data, targets: targets }
    }
}

/// Data loader for batching and shuffling
struct DataLoader<D: Dataset<Item = (Tensor, Tensor)>> {
    dataset: D
    batch_size: Int
    shuffle: Bool
    drop_last: Bool
    num_workers: Int
    pin_memory: Bool
    indices: [Int]
    current_idx: Int
    
    fn new(dataset: D, batch_size: Int = 1, shuffle: Bool = false, drop_last: Bool = false, num_workers: Int = 0, pin_memory: Bool = false) -> Self {
        let indices = (0..dataset.len()).collect()
        DataLoader {
            dataset: dataset, batch_size: batch_size, shuffle: shuffle,
            drop_last: drop_last, num_workers: num_workers, pin_memory: pin_memory,
            indices: indices, current_idx: 0
        }
    }
    
    fn len() -> Int {
        if self.drop_last {
            self.dataset.len() / self.batch_size
        } else {
            (self.dataset.len() + self.batch_size - 1) / self.batch_size
        }
    }
    
    fn reset() {
        self.current_idx = 0
        if self.shuffle {
            shuffle(self.indices)
        }
    }
}

impl<D: Dataset<Item = (Tensor, Tensor)>> Iterator for DataLoader<D> {
    type Item = Batch
    
    fn next() -> Option<Batch> {
        if self.current_idx >= self.dataset.len() {
            return None
        }
        
        let end_idx = (self.current_idx + self.batch_size).min(self.dataset.len())
        if self.drop_last && end_idx - self.current_idx < self.batch_size {
            return None
        }
        
        var batch_data = []
        var batch_targets = []
        
        for i in self.current_idx..end_idx {
            let (data, target) = self.dataset.get(self.indices[i])
            batch_data.extend(data.data.iter())
            batch_targets.push(target.item())
        }
        
        let batch_size = end_idx - self.current_idx
        let feature_size = self.dataset.get(0).0.numel()
        
        self.current_idx = end_idx
        
        Some(Batch {
            data: Tensor.new(batch_data, Shape.matrix(batch_size, feature_size)),
            targets: Tensor.from_vec(batch_targets)
        })
    }
}

// -----------------------------------------------------------------------------
// Data Transforms
// -----------------------------------------------------------------------------

/// Transform trait
trait Transform {
    fn apply(tensor: Tensor) -> Tensor
}

/// Normalize transform
struct Normalize {
    mean: [Float]
    std: [Float]
    
    fn new(mean: [Float], std: [Float]) -> Self {
        Normalize { mean: mean, std: std }
    }
}

impl Transform for Normalize {
    fn apply(tensor: Tensor) -> Tensor {
        var result = tensor.clone()
        for (i, (m, s)) in self.mean.iter().zip(self.std.iter()).enumerate() {
            // Normalize each channel
            result.data[i] = (result.data[i] - m) / s
        }
        result
    }
}

/// Standard scaler (z-score normalization)
struct StandardScaler {
    mean: Option<Float>
    std: Option<Float>
    
    fn new() -> Self { StandardScaler { mean: None, std: None } }
    
    fn fit(data: Tensor) {
        self.mean = Some(data.mean())
        self.std = Some(data.std())
    }
    
    fn transform(data: Tensor) -> Tensor {
        let m = self.mean.unwrap_or(0.0)
        let s = self.std.unwrap_or(1.0)
        data.add_scalar(-m).mul_scalar(1.0 / s)
    }
    
    fn fit_transform(data: Tensor) -> Tensor {
        self.fit(data)
        self.transform(data)
    }
    
    fn inverse_transform(data: Tensor) -> Tensor {
        let m = self.mean.unwrap_or(0.0)
        let s = self.std.unwrap_or(1.0)
        data.mul_scalar(s).add_scalar(m)
    }
}

/// Min-max scaler
struct MinMaxScaler {
    min: Option<Float>
    max: Option<Float>
    feature_range: (Float, Float)
    
    fn new(feature_range: (Float, Float) = (0.0, 1.0)) -> Self {
        MinMaxScaler { min: None, max: None, feature_range: feature_range }
    }
    
    fn fit(data: Tensor) {
        self.min = Some(data.min_val())
        self.max = Some(data.max_val())
    }
    
    fn transform(data: Tensor) -> Tensor {
        let min = self.min.unwrap_or(0.0)
        let max = self.max.unwrap_or(1.0)
        let (a, b) = self.feature_range
        let scale = (b - a) / (max - min)
        data.add_scalar(-min).mul_scalar(scale).add_scalar(a)
    }
    
    fn fit_transform(data: Tensor) -> Tensor {
        self.fit(data)
        self.transform(data)
    }
}

/// Compose multiple transforms
struct Compose {
    transforms: [Box<dyn Transform>]
    
    fn new(transforms: [Box<dyn Transform>]) -> Self {
        Compose { transforms: transforms }
    }
}

impl Transform for Compose {
    fn apply(tensor: Tensor) -> Tensor {
        var result = tensor
        for t in self.transforms { result = t.apply(result) }
        result
    }
}

/// Random horizontal flip
struct RandomHorizontalFlip {
    p: Float
    fn new(p: Float = 0.5) -> Self { RandomHorizontalFlip { p: p } }
}

impl Transform for RandomHorizontalFlip {
    fn apply(tensor: Tensor) -> Tensor {
        if Random.float() < self.p {
            // Flip horizontally
            @native("flip_horizontal", tensor)
        } else {
            tensor
        }
    }
}

/// Random crop
struct RandomCrop {
    size: (Int, Int)
    padding: Int
    
    fn new(size: Int, padding: Int = 0) -> Self {
        RandomCrop { size: (size, size), padding: padding }
    }
}

impl Transform for RandomCrop {
    fn apply(tensor: Tensor) -> Tensor {
        @native("random_crop", tensor, self.size, self.padding)
    }
}

/// To tensor transform
struct ToTensor {}
impl Transform for ToTensor {
    fn apply(tensor: Tensor) -> Tensor { tensor }
}

// -----------------------------------------------------------------------------
// Data Splitting
// -----------------------------------------------------------------------------

/// Split dataset into train and test
fn train_test_split<D: Dataset>(dataset: D, test_size: Float = 0.2, shuffle: Bool = true) -> (Subset<D>, Subset<D>) {
    let n = dataset.len()
    var indices = (0..n).collect::<Vec<_>>()
    
    if shuffle { shuffle(indices) }
    
    let split_idx = ((1.0 - test_size) * n as Float) as Int
    let train_indices = indices[0..split_idx].to_vec()
    let test_indices = indices[split_idx..n].to_vec()
    
    (Subset.new(dataset.clone(), train_indices), Subset.new(dataset, test_indices))
}

/// K-fold cross validation splits
fn kfold_split<D: Dataset>(dataset: D, k: Int, shuffle: Bool = true) -> [(Subset<D>, Subset<D>)] {
    let n = dataset.len()
    var indices = (0..n).collect::<Vec<_>>()
    
    if shuffle { shuffle(indices) }
    
    let fold_size = n / k
    var folds = []
    
    for i in 0..k {
        let start = i * fold_size
        let end = if i == k - 1 { n } else { (i + 1) * fold_size }
        
        let test_indices = indices[start..end].to_vec()
        var train_indices = []
        train_indices.extend(indices[0..start].iter())
        train_indices.extend(indices[end..n].iter())
        
        folds.push((
            Subset.new(dataset.clone(), train_indices),
            Subset.new(dataset.clone(), test_indices)
        ))
    }
    
    folds
}

// -----------------------------------------------------------------------------
// Synthetic Datasets
// -----------------------------------------------------------------------------

/// Generate classification dataset
fn make_classification(n_samples: Int = 100, n_features: Int = 20, n_classes: Int = 2, random_state: Option<Int> = None) -> MemoryDataset {
    if let Some(seed) = random_state { Random.seed(seed) }
    
    var data = []
    var targets = []
    
    let samples_per_class = n_samples / n_classes
    
    for class_idx in 0..n_classes {
        let center = (0..n_features).map(|_| Random.float() * 2.0 - 1.0).collect::<Vec<_>>()
        
        for _ in 0..samples_per_class {
            let sample = center.iter().map(|c| c + Random.float() * 0.5 - 0.25).collect::<Vec<_>>()
            data.push(sample)
            targets.push(class_idx as Float)
        }
    }
    
    MemoryDataset.from_arrays(data, targets)
}

/// Generate regression dataset
fn make_regression(n_samples: Int = 100, n_features: Int = 10, noise: Float = 0.1, random_state: Option<Int> = None) -> MemoryDataset {
    if let Some(seed) = random_state { Random.seed(seed) }
    
    let weights = (0..n_features).map(|_| Random.float() * 2.0 - 1.0).collect::<Vec<_>>()
    
    var data = []
    var targets = []
    
    for _ in 0..n_samples {
        let sample = (0..n_features).map(|_| Random.float() * 2.0 - 1.0).collect::<Vec<_>>()
        let target = sample.iter().zip(weights.iter()).map(|(x, w)| x * w).sum() + Random.float() * noise
        data.push(sample)
        targets.push(target)
    }
    
    MemoryDataset.from_arrays(data, targets)
}

/// Generate blobs dataset
fn make_blobs(n_samples: Int = 100, n_features: Int = 2, centers: Int = 3, cluster_std: Float = 1.0) -> MemoryDataset {
    var data = []
    var targets = []
    
    let samples_per_center = n_samples / centers
    
    for center_idx in 0..centers {
        let center = (0..n_features).map(|_| Random.float() * 10.0 - 5.0).collect::<Vec<_>>()
        
        for _ in 0..samples_per_center {
            let sample = center.iter().map(|c| c + Random.float() * cluster_std * 2.0 - cluster_std).collect::<Vec<_>>()
            data.push(sample)
            targets.push(center_idx as Float)
        }
    }
    
    MemoryDataset.from_arrays(data, targets)
}

/// Generate moons dataset
fn make_moons(n_samples: Int = 100, noise: Float = 0.1) -> MemoryDataset {
    let n_samples_out = n_samples / 2
    let n_samples_in = n_samples - n_samples_out
    
    var data = []
    var targets = []
    
    // Outer moon
    for i in 0..n_samples_out {
        let angle = PI * i as Float / n_samples_out as Float
        let x = angle.cos() + Random.float() * noise
        let y = angle.sin() + Random.float() * noise
        data.push([x, y])
        targets.push(0.0)
    }
    
    // Inner moon
    for i in 0..n_samples_in {
        let angle = PI * i as Float / n_samples_in as Float
        let x = 1.0 - angle.cos() + Random.float() * noise
        let y = 0.5 - angle.sin() + Random.float() * noise
        data.push([x, y])
        targets.push(1.0)
    }
    
    MemoryDataset.from_arrays(data, targets)
}

// -----------------------------------------------------------------------------
// Utility Functions
// -----------------------------------------------------------------------------

fn shuffle<T>(arr: [T]) {
    for i in (1..arr.len()).rev() {
        let j = Random.int(0, i + 1)
        arr.swap(i, j)
    }
}

const PI: Float = 3.141592653589793

// -----------------------------------------------------------------------------
// Tests
// -----------------------------------------------------------------------------

test "memory dataset" {
    let data = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]
    let targets = [0.0, 1.0, 0.0]
    let dataset = MemoryDataset.from_arrays(data, targets)
    
    assert_eq(dataset.len(), 3)?
    let (x, y) = dataset.get(0)
    assert_eq(x.data, [1.0, 2.0])?
}

test "dataloader" {
    let data = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0]]
    let targets = [0.0, 1.0, 0.0, 1.0]
    let dataset = MemoryDataset.from_arrays(data, targets)
    
    var loader = DataLoader.new(dataset, 2)
    assert_eq(loader.len(), 2)?
    
    let batch = loader.next().unwrap()
    assert_eq(batch.data.shape.rows(), 2)?
}

test "train_test_split" {
    let dataset = make_classification(100, 10, 2)
    let (train, test) = train_test_split(dataset, 0.2, false)
    
    assert_eq(train.len(), 80)?
    assert_eq(test.len(), 20)?
}

test "standard_scaler" {
    let data = Tensor.from_vec([1.0, 2.0, 3.0, 4.0, 5.0])
    var scaler = StandardScaler.new()
    let scaled = scaler.fit_transform(data)
    
    assert(scaled.mean().abs() < 0.0001)
}
