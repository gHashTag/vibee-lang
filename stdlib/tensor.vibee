// =============================================================================
// Vibee OS â€” Tensor Module
// N-dimensional tensors for machine learning
// =============================================================================

/// Tensor data type
enum DType { Float32, Float64, Int32, Int64, Bool }

/// Device for computation
enum Device { CPU, GPU(Int) }

/// Tensor shape
struct Shape {
    dims: [Int]
    
    fn new(dims: [Int]) -> Self { Shape { dims: dims } }
    fn scalar() -> Self { Shape { dims: [] } }
    fn vector(n: Int) -> Self { Shape { dims: [n] } }
    fn matrix(rows: Int, cols: Int) -> Self { Shape { dims: [rows, cols] } }
    
    fn ndim() -> Int { self.dims.len() }
    fn numel() -> Int { self.dims.iter().product().unwrap_or(1) }
    fn get(i: Int) -> Int { self.dims[i] }
    fn rows() -> Int { self.dims[0] }
    fn cols() -> Int { self.dims[1] }
    
    fn broadcast_with(other: Shape) -> Option<Shape> {
        let max_ndim = self.ndim().max(other.ndim())
        var result = []
        for i in 0..max_ndim {
            let a = if i < self.ndim() { self.dims[self.ndim() - 1 - i] } else { 1 }
            let b = if i < other.ndim() { other.dims[other.ndim() - 1 - i] } else { 1 }
            if a == b { result.insert(0, a) }
            else if a == 1 { result.insert(0, b) }
            else if b == 1 { result.insert(0, a) }
            else { return None }
        }
        Some(Shape { dims: result })
    }
    
    fn transpose() -> Shape {
        if self.ndim() < 2 { return self.clone() }
        var new_dims = self.dims.clone()
        new_dims.swap(self.ndim() - 2, self.ndim() - 1)
        Shape { dims: new_dims }
    }
}

/// N-dimensional tensor
struct Tensor {
    data: [Float]
    shape: Shape
    dtype: DType
    device: Device
    requires_grad: Bool
    grad: Option<Box<Tensor>>
    
    fn new(data: [Float], shape: Shape) -> Self {
        Tensor { data: data, shape: shape, dtype: DType.Float32, device: Device.CPU, requires_grad: false, grad: None }
    }
    
    fn zeros(shape: Shape) -> Self { Tensor.new([0.0; shape.numel()], shape) }
    fn ones(shape: Shape) -> Self { Tensor.new([1.0; shape.numel()], shape) }
    fn full(shape: Shape, value: Float) -> Self { Tensor.new([value; shape.numel()], shape) }
    fn eye(n: Int) -> Self {
        var data = [0.0; n * n]
        for i in 0..n { data[i * n + i] = 1.0 }
        Tensor.new(data, Shape.matrix(n, n))
    }
    fn rand(shape: Shape) -> Self {
        Tensor.new((0..shape.numel()).map(|_| Random.float()).collect(), shape)
    }
    fn randn(shape: Shape) -> Self {
        var data = []
        for _ in 0..shape.numel() {
            let u1 = Random.float(); let u2 = Random.float()
            data.push((-2.0 * u1.ln()).sqrt() * (6.283185 * u2).cos())
        }
        Tensor.new(data, shape)
    }
    fn arange(start: Float, end: Float, step: Float = 1.0) -> Self {
        var data = []; var v = start
        while v < end { data.push(v); v += step }
        Tensor.new(data, Shape.vector(data.len()))
    }
    fn linspace(start: Float, end: Float, steps: Int) -> Self {
        let step = (end - start) / (steps - 1) as Float
        Tensor.new((0..steps).map(|i| start + i as Float * step).collect(), Shape.vector(steps))
    }
    fn from_vec(data: [Float]) -> Self { Tensor.new(data.clone(), Shape.vector(data.len())) }
    fn scalar(value: Float) -> Self { Tensor.new([value], Shape.scalar()) }
    
    fn ndim() -> Int { self.shape.ndim() }
    fn numel() -> Int { self.shape.numel() }
    fn item() -> Float { self.data[0] }
    
    fn reshape(new_shape: [Int]) -> Self { Tensor.new(self.data.clone(), Shape.new(new_shape)) }
    fn flatten() -> Self { Tensor.new(self.data.clone(), Shape.vector(self.numel())) }
    fn squeeze() -> Self {
        Tensor.new(self.data.clone(), Shape.new(self.shape.dims.iter().filter(|d| **d != 1).cloned().collect()))
    }
    fn unsqueeze(dim: Int) -> Self {
        var dims = self.shape.dims.clone(); dims.insert(dim, 1)
        Tensor.new(self.data.clone(), Shape.new(dims))
    }
    fn transpose() -> Self {
        let rows = self.shape.rows(); let cols = self.shape.cols()
        var new_data = [0.0; self.numel()]
        for i in 0..rows { for j in 0..cols { new_data[j * rows + i] = self.data[i * cols + j] } }
        Tensor.new(new_data, self.shape.transpose())
    }
    fn t() -> Self { self.transpose() }
    
    fn binary_op(other: Tensor, op: fn(Float, Float) -> Float) -> Self {
        let shape = self.shape.broadcast_with(other.shape).expect("Not broadcastable")
        var result = [0.0; shape.numel()]
        for i in 0..result.len() { result[i] = op(self.data[i % self.numel()], other.data[i % other.numel()]) }
        Tensor.new(result, shape)
    }
    fn unary_op(op: fn(Float) -> Float) -> Self {
        Tensor.new(self.data.iter().map(|x| op(*x)).collect(), self.shape.clone())
    }
    
    fn add(other: Tensor) -> Self { self.binary_op(other, |a, b| a + b) }
    fn sub(other: Tensor) -> Self { self.binary_op(other, |a, b| a - b) }
    fn mul(other: Tensor) -> Self { self.binary_op(other, |a, b| a * b) }
    fn div(other: Tensor) -> Self { self.binary_op(other, |a, b| a / b) }
    fn add_scalar(s: Float) -> Self { self.unary_op(|x| x + s) }
    fn mul_scalar(s: Float) -> Self { self.unary_op(|x| x * s) }
    
    fn neg() -> Self { self.unary_op(|x| -x) }
    fn abs() -> Self { self.unary_op(|x| x.abs()) }
    fn sqrt() -> Self { self.unary_op(|x| x.sqrt()) }
    fn exp() -> Self { self.unary_op(|x| x.exp()) }
    fn log() -> Self { self.unary_op(|x| x.ln()) }
    fn pow(exp: Float) -> Self { self.unary_op(|x| x.pow(exp)) }
    fn sin() -> Self { self.unary_op(|x| x.sin()) }
    fn cos() -> Self { self.unary_op(|x| x.cos()) }
    fn tanh() -> Self { self.unary_op(|x| x.tanh()) }
    fn sigmoid() -> Self { self.unary_op(|x| 1.0 / (1.0 + (-x).exp())) }
    fn relu() -> Self { self.unary_op(|x| x.max(0.0)) }
    fn leaky_relu(alpha: Float = 0.01) -> Self { self.unary_op(|x| if x > 0.0 { x } else { alpha * x }) }
    fn gelu() -> Self { self.unary_op(|x| 0.5 * x * (1.0 + (0.7978845 * (x + 0.044715 * x.pow(3.0))).tanh())) }
    fn silu() -> Self { self.unary_op(|x| x / (1.0 + (-x).exp())) }
    fn clamp(min: Float, max: Float) -> Self { self.unary_op(|x| x.max(min).min(max)) }
    
    fn sum() -> Float { self.data.iter().sum() }
    fn mean() -> Float { self.sum() / self.numel() as Float }
    fn max_val() -> Float { self.data.iter().fold(Float.NEG_INFINITY, |a, b| a.max(*b)) }
    fn min_val() -> Float { self.data.iter().fold(Float.INFINITY, |a, b| a.min(*b)) }
    fn var() -> Float { let m = self.mean(); self.data.iter().map(|x| (*x - m).pow(2.0)).sum() / self.numel() as Float }
    fn std() -> Float { self.var().sqrt() }
    fn norm(p: Float = 2.0) -> Float { self.data.iter().map(|x| x.abs().pow(p)).sum().pow(1.0 / p) }
    
    fn argmax() -> Int {
        var idx = 0; var max = self.data[0]
        for (i, v) in self.data.iter().enumerate() { if *v > max { max = *v; idx = i } }
        idx
    }
    fn argmin() -> Int {
        var idx = 0; var min = self.data[0]
        for (i, v) in self.data.iter().enumerate() { if *v < min { min = *v; idx = i } }
        idx
    }
    
    fn matmul(other: Tensor) -> Self {
        let m = self.shape.rows(); let k = self.shape.cols(); let n = other.shape.cols()
        var result = [0.0; m * n]
        for i in 0..m { for j in 0..n { var s = 0.0
            for l in 0..k { s += self.data[i * k + l] * other.data[l * n + j] }
            result[i * n + j] = s
        }}
        Tensor.new(result, Shape.matrix(m, n))
    }
    fn mm(other: Tensor) -> Self { self.matmul(other) }
    fn dot(other: Tensor) -> Float { self.data.iter().zip(other.data.iter()).map(|(a, b)| a * b).sum() }
    
    fn softmax() -> Self {
        let max = self.max_val()
        let exp_data = self.data.iter().map(|x| (*x - max).exp()).collect::<Vec<_>>()
        let sum: Float = exp_data.iter().sum()
        Tensor.new(exp_data.iter().map(|x| x / sum).collect(), self.shape.clone())
    }
    fn log_softmax() -> Self { let s = self.softmax(); s.log() }
    
    fn requires_grad_(requires: Bool = true) -> Self { self.requires_grad = requires; self }
    fn backward() { if self.requires_grad { self.grad = Some(Box.new(Tensor.ones(self.shape.clone()))) } }
    fn zero_grad() { self.grad = None }
    fn detach() -> Self { var t = self.clone(); t.requires_grad = false; t }
    fn to(device: Device) -> Self { var t = self.clone(); t.device = device; t }
    fn cpu() -> Self { self.to(Device.CPU) }
    fn cuda(id: Int = 0) -> Self { self.to(Device.GPU(id)) }
    fn clone() -> Self { Tensor.new(self.data.clone(), self.shape.clone()) }
}

impl Add for Tensor { fn add(o: Tensor) -> Tensor { self.add(o) } }
impl Sub for Tensor { fn sub(o: Tensor) -> Tensor { self.sub(o) } }
impl Mul for Tensor { fn mul(o: Tensor) -> Tensor { self.mul(o) } }
impl Neg for Tensor { fn neg() -> Tensor { self.neg() } }

fn zeros(shape: [Int]) -> Tensor { Tensor.zeros(Shape.new(shape)) }
fn ones(shape: [Int]) -> Tensor { Tensor.ones(Shape.new(shape)) }
fn rand(shape: [Int]) -> Tensor { Tensor.rand(Shape.new(shape)) }
fn randn(shape: [Int]) -> Tensor { Tensor.randn(Shape.new(shape)) }
fn eye(n: Int) -> Tensor { Tensor.eye(n) }

fn cat(tensors: [Tensor], dim: Int = 0) -> Tensor {
    var data = []; for t in tensors { data.extend(t.data.iter()) }
    var dims = tensors[0].shape.dims.clone()
    dims[dim] = tensors.iter().map(|t| t.shape.get(dim)).sum()
    Tensor.new(data, Shape.new(dims))
}

fn stack(tensors: [Tensor], dim: Int = 0) -> Tensor {
    cat(tensors.iter().map(|t| t.unsqueeze(dim)).collect(), dim)
}

test "tensor_ops" {
    let a = Tensor.from_vec([1.0, 2.0, 3.0])
    let b = Tensor.from_vec([4.0, 5.0, 6.0])
    assert_eq(a.add(b).data, [5.0, 7.0, 9.0])?
}

test "matmul" {
    let a = Tensor.new([1.0, 2.0, 3.0, 4.0], Shape.matrix(2, 2))
    let b = Tensor.new([5.0, 6.0, 7.0, 8.0], Shape.matrix(2, 2))
    let c = a.matmul(b)
    assert_eq(c.data[0], 19.0)?
}
