// =============================================================================
// Vibee OS â€” Ranking Module
// Search result ranking and relevance scoring algorithms
// =============================================================================

// =============================================================================
// Core Types
// =============================================================================

/// Ranked item with score breakdown
struct RankedItem<T> {
    item: T
    total_score: Float
    score_breakdown: Map<String, Float>
    rank: Int
    
    fn new(item: T, score: Float) -> Self {
        RankedItem { item: item, total_score: score, score_breakdown: Map.empty(), rank: 0 }
    }
    
    fn with_component(name: String, score: Float) -> Self {
        self.score_breakdown.set(name, score)
        self
    }
}

/// Scoring signal
struct Signal {
    name: String
    value: Float
    weight: Float
    
    fn new(name: String, value: Float, weight: Float) -> Self {
        Signal { name: name, value: value, weight: weight }
    }
    
    fn weighted_value() -> Float {
        self.value * self.weight
    }
}

// =============================================================================
// Scoring Functions
// =============================================================================

/// TF-IDF scoring
struct TfIdf {
    doc_count: Int
    doc_frequencies: Map<String, Int>
    
    fn new() -> Self {
        TfIdf { doc_count: 0, doc_frequencies: Map.empty() }
    }
    
    /// Update statistics from document
    fn add_document(terms: [String]) {
        self.doc_count += 1
        let unique_terms: Set<String> = terms.iter().collect()
        for term in unique_terms {
            *self.doc_frequencies.entry(term).or_insert(0) += 1
        }
    }
    
    /// Calculate TF (term frequency)
    fn tf(term: String, doc_terms: [String]) -> Float {
        let count = doc_terms.iter().filter(|t| t == term).count()
        if count == 0 { return 0.0 }
        1.0 + (count as Float).ln()
    }
    
    /// Calculate IDF (inverse document frequency)
    fn idf(term: String) -> Float {
        let df = self.doc_frequencies.get(term).unwrap_or(0)
        if df == 0 { return 0.0 }
        (self.doc_count as Float / df as Float).ln() + 1.0
    }
    
    /// Calculate TF-IDF score
    fn score(term: String, doc_terms: [String]) -> Float {
        self.tf(term.clone(), doc_terms) * self.idf(term)
    }
    
    /// Score document against query
    fn score_document(query_terms: [String], doc_terms: [String]) -> Float {
        query_terms.iter()
            .map(|t| self.score(t, doc_terms.clone()))
            .sum()
    }
}

/// BM25 scoring (Okapi BM25)
struct BM25 {
    k1: Float
    b: Float
    doc_count: Int
    doc_frequencies: Map<String, Int>
    avg_doc_length: Float
    total_doc_length: Int
    
    fn new() -> Self {
        BM25 { k1: 1.2, b: 0.75, doc_count: 0, doc_frequencies: Map.empty(), avg_doc_length: 0.0, total_doc_length: 0 }
    }
    
    fn with_params(k1: Float, b: Float) -> Self {
        BM25 { k1: k1, b: b, ..Self.new() }
    }
    
    /// Add document to statistics
    fn add_document(terms: [String]) {
        self.doc_count += 1
        self.total_doc_length += terms.len()
        self.avg_doc_length = self.total_doc_length as Float / self.doc_count as Float
        
        let unique_terms: Set<String> = terms.iter().collect()
        for term in unique_terms {
            *self.doc_frequencies.entry(term).or_insert(0) += 1
        }
    }
    
    /// Calculate IDF component
    fn idf(term: String) -> Float {
        let n = self.doc_count as Float
        let df = self.doc_frequencies.get(term).unwrap_or(0) as Float
        ((n - df + 0.5) / (df + 0.5) + 1.0).ln()
    }
    
    /// Score a single term
    fn score_term(term: String, doc_terms: [String]) -> Float {
        let tf = doc_terms.iter().filter(|t| t == term).count() as Float
        let doc_len = doc_terms.len() as Float
        
        let idf = self.idf(term)
        let numerator = tf * (self.k1 + 1.0)
        let denominator = tf + self.k1 * (1.0 - self.b + self.b * doc_len / self.avg_doc_length)
        
        idf * numerator / denominator
    }
    
    /// Score document against query
    fn score(query_terms: [String], doc_terms: [String]) -> Float {
        query_terms.iter()
            .map(|t| self.score_term(t, doc_terms.clone()))
            .sum()
    }
}

/// BM25F scoring (field-weighted BM25)
struct BM25F {
    bm25: BM25
    field_weights: Map<String, Float>
    field_b: Map<String, Float>
    
    fn new() -> Self {
        BM25F { bm25: BM25.new(), field_weights: Map.empty(), field_b: Map.empty() }
    }
    
    fn set_field_weight(field: String, weight: Float) -> Self {
        self.field_weights.set(field, weight)
        self
    }
    
    fn set_field_b(field: String, b: Float) -> Self {
        self.field_b.set(field, b)
        self
    }
    
    /// Score document with multiple fields
    fn score(query_terms: [String], doc_fields: Map<String, [String]>) -> Float {
        var total_score = 0.0
        
        for term in query_terms {
            var weighted_tf = 0.0
            
            for (field, terms) in doc_fields.iter() {
                let tf = terms.iter().filter(|t| t == term).count() as Float
                let weight = self.field_weights.get(field).unwrap_or(1.0)
                let b = self.field_b.get(field).unwrap_or(self.bm25.b)
                let field_len = terms.len() as Float
                
                // Weighted TF with field-specific length normalization
                weighted_tf += weight * tf / (1.0 - b + b * field_len / self.bm25.avg_doc_length)
            }
            
            let idf = self.bm25.idf(term)
            total_score += idf * weighted_tf * (self.bm25.k1 + 1.0) / (weighted_tf + self.bm25.k1)
        }
        
        total_score
    }
}

// =============================================================================
// Ranking Algorithms
// =============================================================================

/// Ranking algorithm trait
trait Ranker<T> {
    fn rank(items: [T]) -> [RankedItem<T>]
}

/// Score-based ranker
struct ScoreRanker<T> {
    scorer: fn(T) -> Float
    
    fn new(scorer: fn(T) -> Float) -> Self {
        ScoreRanker { scorer: scorer }
    }
}

impl<T> Ranker<T> for ScoreRanker<T> {
    fn rank(items: [T]) -> [RankedItem<T>] {
        var ranked: [RankedItem<T>] = items.iter()
            .map(|item| RankedItem.new(item, (self.scorer)(item.clone())))
            .collect()
        
        ranked.sort_by(|a, b| b.total_score.partial_cmp(a.total_score).unwrap_or(Ordering.Equal))
        
        for (i, item) in ranked.iter_mut().enumerate() {
            item.rank = i + 1
        }
        
        ranked
    }
}

/// Multi-signal ranker
struct MultiSignalRanker<T> {
    signals: [fn(T) -> Signal]
    combiner: SignalCombiner
    
    fn new() -> Self {
        MultiSignalRanker { signals: [], combiner: SignalCombiner.WeightedSum }
    }
    
    fn add_signal(signal_fn: fn(T) -> Signal) -> Self {
        self.signals.push(signal_fn)
        self
    }
    
    fn with_combiner(combiner: SignalCombiner) -> Self {
        self.combiner = combiner
        self
    }
}

impl<T> Ranker<T> for MultiSignalRanker<T> {
    fn rank(items: [T]) -> [RankedItem<T>] {
        var ranked = []
        
        for item in items {
            var signals: [Signal] = self.signals.iter()
                .map(|f| f(item.clone()))
                .collect()
            
            let total_score = self.combiner.combine(signals.clone())
            
            var ranked_item = RankedItem.new(item, total_score)
            for signal in signals {
                ranked_item.score_breakdown.set(signal.name, signal.weighted_value())
            }
            
            ranked.push(ranked_item)
        }
        
        ranked.sort_by(|a, b| b.total_score.partial_cmp(a.total_score).unwrap_or(Ordering.Equal))
        
        for (i, item) in ranked.iter_mut().enumerate() {
            item.rank = i + 1
        }
        
        ranked
    }
}

/// Signal combination strategy
enum SignalCombiner {
    WeightedSum
    WeightedProduct
    Max
    Min
    Harmonic
}

impl SignalCombiner {
    fn combine(signals: [Signal]) -> Float {
        if signals.is_empty() { return 0.0 }
        
        match self {
            SignalCombiner.WeightedSum => {
                signals.iter().map(|s| s.weighted_value()).sum()
            }
            SignalCombiner.WeightedProduct => {
                signals.iter().map(|s| s.weighted_value()).product()
            }
            SignalCombiner.Max => {
                signals.iter().map(|s| s.weighted_value()).max().unwrap_or(0.0)
            }
            SignalCombiner.Min => {
                signals.iter().map(|s| s.weighted_value()).min().unwrap_or(0.0)
            }
            SignalCombiner.Harmonic => {
                let n = signals.len() as Float
                let sum_reciprocals: Float = signals.iter()
                    .map(|s| 1.0 / s.weighted_value().max(0.0001))
                    .sum()
                n / sum_reciprocals
            }
        }
    }
}

// =============================================================================
// Learning to Rank
// =============================================================================

/// Feature vector for learning to rank
struct FeatureVector {
    features: [Float]
    label: Option<Float>
    query_id: Option<String>
    doc_id: Option<String>
    
    fn new(features: [Float]) -> Self {
        FeatureVector { features: features, label: None, query_id: None, doc_id: None }
    }
    
    fn with_label(label: Float) -> Self { self.label = Some(label); self }
    fn with_query_id(id: String) -> Self { self.query_id = Some(id); self }
    fn with_doc_id(id: String) -> Self { self.doc_id = Some(id); self }
}

/// Linear ranking model
struct LinearRankingModel {
    weights: [Float]
    bias: Float
    
    fn new(num_features: Int) -> Self {
        LinearRankingModel { weights: [0.0; num_features], bias: 0.0 }
    }
    
    fn with_weights(weights: [Float], bias: Float) -> Self {
        LinearRankingModel { weights: weights, bias: bias }
    }
    
    /// Predict score for feature vector
    fn predict(features: [Float]) -> Float {
        var score = self.bias
        for (i, w) in self.weights.iter().enumerate() {
            if i < features.len() {
                score += w * features[i]
            }
        }
        score
    }
    
    /// Train model using gradient descent
    fn train(data: [FeatureVector], learning_rate: Float, epochs: Int) {
        for _ in 0..epochs {
            for sample in data.iter() {
                if let Some(label) = sample.label {
                    let pred = self.predict(sample.features.clone())
                    let error = label - pred
                    
                    // Update weights
                    for (i, feature) in sample.features.iter().enumerate() {
                        if i < self.weights.len() {
                            self.weights[i] += learning_rate * error * feature
                        }
                    }
                    self.bias += learning_rate * error
                }
            }
        }
    }
    
    /// Rank items using the model
    fn rank<T>(items: [T], feature_extractor: fn(T) -> [Float]) -> [RankedItem<T>] {
        var ranked: [RankedItem<T>] = items.iter()
            .map(|item| {
                let features = feature_extractor(item.clone())
                let score = self.predict(features)
                RankedItem.new(item, score)
            })
            .collect()
        
        ranked.sort_by(|a, b| b.total_score.partial_cmp(a.total_score).unwrap_or(Ordering.Equal))
        
        for (i, item) in ranked.iter_mut().enumerate() {
            item.rank = i + 1
        }
        
        ranked
    }
}

// =============================================================================
// Ranking Metrics
// =============================================================================

/// Precision at K
fn precision_at_k(relevant: Set<String>, ranked: [String], k: Int) -> Float {
    let top_k: [String] = ranked.iter().take(k).collect()
    let relevant_in_top_k = top_k.iter().filter(|id| relevant.contains(id)).count()
    relevant_in_top_k as Float / k as Float
}

/// Recall at K
fn recall_at_k(relevant: Set<String>, ranked: [String], k: Int) -> Float {
    if relevant.is_empty() { return 0.0 }
    let top_k: [String] = ranked.iter().take(k).collect()
    let relevant_in_top_k = top_k.iter().filter(|id| relevant.contains(id)).count()
    relevant_in_top_k as Float / relevant.len() as Float
}

/// Mean Reciprocal Rank (MRR)
fn mrr(relevant: Set<String>, ranked: [String]) -> Float {
    for (i, id) in ranked.iter().enumerate() {
        if relevant.contains(id) {
            return 1.0 / (i + 1) as Float
        }
    }
    0.0
}

/// Discounted Cumulative Gain (DCG)
fn dcg(relevance_scores: [Float], k: Int) -> Float {
    relevance_scores.iter()
        .take(k)
        .enumerate()
        .map(|(i, rel)| rel / (2.0 + i as Float).log2())
        .sum()
}

/// Normalized DCG (NDCG)
fn ndcg(relevance_scores: [Float], k: Int) -> Float {
    let actual_dcg = dcg(relevance_scores.clone(), k)
    
    // Ideal DCG (sorted by relevance)
    var ideal_scores = relevance_scores.clone()
    ideal_scores.sort_by(|a, b| b.partial_cmp(a).unwrap_or(Ordering.Equal))
    let ideal_dcg = dcg(ideal_scores, k)
    
    if ideal_dcg == 0.0 { 0.0 } else { actual_dcg / ideal_dcg }
}

/// Mean Average Precision (MAP)
fn mean_average_precision(queries: [(Set<String>, [String])]) -> Float {
    if queries.is_empty() { return 0.0 }
    
    let sum: Float = queries.iter()
        .map(|(relevant, ranked)| average_precision(relevant, ranked))
        .sum()
    
    sum / queries.len() as Float
}

/// Average Precision for single query
fn average_precision(relevant: Set<String>, ranked: [String]) -> Float {
    if relevant.is_empty() { return 0.0 }
    
    var sum = 0.0
    var relevant_count = 0
    
    for (i, id) in ranked.iter().enumerate() {
        if relevant.contains(id) {
            relevant_count += 1
            sum += relevant_count as Float / (i + 1) as Float
        }
    }
    
    sum / relevant.len() as Float
}

// =============================================================================
// Diversity Ranking
// =============================================================================

/// Maximal Marginal Relevance (MMR) for diversity
struct MMR {
    lambda: Float  // Trade-off between relevance and diversity
    
    fn new(lambda: Float) -> Self {
        MMR { lambda: lambda }
    }
    
    /// Rerank for diversity
    fn rerank<T: Clone>(
        items: [T],
        relevance: fn(T) -> Float,
        similarity: fn(T, T) -> Float,
        k: Int
    ) -> [T] {
        if items.is_empty() || k == 0 { return [] }
        
        var selected: [T] = []
        var remaining: [T] = items.clone()
        
        while selected.len() < k && !remaining.is_empty() {
            var best_idx = 0
            var best_score = Float.NEG_INFINITY
            
            for (i, item) in remaining.iter().enumerate() {
                let rel = relevance(item.clone())
                
                // Max similarity to already selected items
                let max_sim = if selected.is_empty() {
                    0.0
                } else {
                    selected.iter()
                        .map(|s| similarity(item.clone(), s.clone()))
                        .max()
                        .unwrap_or(0.0)
                }
                
                let mmr_score = self.lambda * rel - (1.0 - self.lambda) * max_sim
                
                if mmr_score > best_score {
                    best_score = mmr_score
                    best_idx = i
                }
            }
            
            selected.push(remaining.remove(best_idx))
        }
        
        selected
    }
}

// =============================================================================
// Personalized Ranking
// =============================================================================

/// User profile for personalization
struct UserProfile {
    id: String
    preferences: Map<String, Float>
    history: [String]
    
    fn new(id: String) -> Self {
        UserProfile { id: id, preferences: Map.empty(), history: [] }
    }
    
    fn set_preference(key: String, value: Float) -> Self {
        self.preferences.set(key, value)
        self
    }
    
    fn add_to_history(item_id: String) {
        self.history.push(item_id)
        if self.history.len() > 100 {
            self.history.remove(0)
        }
    }
}

/// Personalized ranker
struct PersonalizedRanker<T> {
    base_scorer: fn(T) -> Float
    personalization_weight: Float
    
    fn new(base_scorer: fn(T) -> Float) -> Self {
        PersonalizedRanker { base_scorer: base_scorer, personalization_weight: 0.3 }
    }
    
    fn with_weight(weight: Float) -> Self {
        self.personalization_weight = weight
        self
    }
    
    /// Rank with personalization
    fn rank(
        items: [T],
        user: UserProfile,
        item_features: fn(T) -> Map<String, Float>
    ) -> [RankedItem<T>] {
        var ranked: [RankedItem<T>] = items.iter()
            .map(|item| {
                let base_score = (self.base_scorer)(item.clone())
                let features = item_features(item.clone())
                
                // Calculate personalization boost
                var personal_score = 0.0
                for (key, pref) in user.preferences.iter() {
                    if let Some(feature_val) = features.get(key) {
                        personal_score += pref * feature_val
                    }
                }
                
                let total = base_score * (1.0 - self.personalization_weight) +
                           personal_score * self.personalization_weight
                
                var ranked_item = RankedItem.new(item, total)
                ranked_item.score_breakdown.set("base", base_score)
                ranked_item.score_breakdown.set("personal", personal_score)
                ranked_item
            })
            .collect()
        
        ranked.sort_by(|a, b| b.total_score.partial_cmp(a.total_score).unwrap_or(Ordering.Equal))
        
        for (i, item) in ranked.iter_mut().enumerate() {
            item.rank = i + 1
        }
        
        ranked
    }
}

// =============================================================================
// Ranking Pipeline
// =============================================================================

/// Ranking pipeline stage
trait RankingStage<T> {
    fn process(items: [RankedItem<T>]) -> [RankedItem<T>]
}

/// Boost stage
struct BoostStage<T> {
    condition: fn(T) -> Bool
    boost: Float
    
    fn new(condition: fn(T) -> Bool, boost: Float) -> Self {
        BoostStage { condition: condition, boost: boost }
    }
}

impl<T> RankingStage<T> for BoostStage<T> {
    fn process(items: [RankedItem<T>]) -> [RankedItem<T>] {
        for item in items.iter_mut() {
            if (self.condition)(item.item.clone()) {
                item.total_score *= self.boost
                item.score_breakdown.set("boost", self.boost)
            }
        }
        items
    }
}

/// Filter stage
struct FilterStage<T> {
    predicate: fn(T) -> Bool
    
    fn new(predicate: fn(T) -> Bool) -> Self {
        FilterStage { predicate: predicate }
    }
}

impl<T> RankingStage<T> for FilterStage<T> {
    fn process(items: [RankedItem<T>]) -> [RankedItem<T>] {
        items.into_iter()
            .filter(|item| (self.predicate)(item.item.clone()))
            .collect()
    }
}

/// Ranking pipeline
struct RankingPipeline<T> {
    stages: [Box<dyn RankingStage<T>>]
    
    fn new() -> Self {
        RankingPipeline { stages: [] }
    }
    
    fn add_stage(stage: Box<dyn RankingStage<T>>) -> Self {
        self.stages.push(stage)
        self
    }
    
    fn execute(items: [RankedItem<T>]) -> [RankedItem<T>] {
        var result = items
        for stage in self.stages.iter() {
            result = stage.process(result)
        }
        
        // Re-sort and re-rank
        result.sort_by(|a, b| b.total_score.partial_cmp(a.total_score).unwrap_or(Ordering.Equal))
        for (i, item) in result.iter_mut().enumerate() {
            item.rank = i + 1
        }
        
        result
    }
}

// =============================================================================
// Tests
// =============================================================================

test "tf-idf scoring" {
    var tfidf = TfIdf.new()
    tfidf.add_document(["the", "quick", "brown", "fox"])
    tfidf.add_document(["the", "lazy", "dog"])
    tfidf.add_document(["quick", "fox", "jumps"])
    
    let score = tfidf.score("quick", ["the", "quick", "brown", "fox"])
    assert(score > 0.0)?
}

test "bm25 scoring" {
    var bm25 = BM25.new()
    bm25.add_document(["hello", "world"])
    bm25.add_document(["hello", "there"])
    bm25.add_document(["goodbye", "world"])
    
    let score = bm25.score(["hello"], ["hello", "world"])
    assert(score > 0.0)?
}

test "precision at k" {
    let relevant = Set.from_iter(["a", "b", "c"].iter())
    let ranked = ["a", "d", "b", "e", "c"]
    
    assert_eq(precision_at_k(relevant, ranked, 3), 2.0 / 3.0)?
}

test "ndcg" {
    let scores = [3.0, 2.0, 3.0, 0.0, 1.0]
    let ndcg_score = ndcg(scores, 5)
    assert(ndcg_score > 0.0 && ndcg_score <= 1.0)?
}

test "mmr diversity" {
    let items = ["a", "b", "c", "d"]
    let mmr = MMR.new(0.5)
    
    let reranked = mmr.rerank(
        items,
        |_| 1.0,  // All equally relevant
        |a, b| if a == b { 1.0 } else { 0.0 },  // Only identical items are similar
        3
    )
    
    assert_eq(reranked.len(), 3)?
}

test "linear ranking model" {
    var model = LinearRankingModel.with_weights([1.0, 2.0, 0.5], 0.1)
    
    let score = model.predict([1.0, 1.0, 1.0])
    assert_eq(score, 3.6)?  // 1*1 + 2*1 + 0.5*1 + 0.1
}

test "multi-signal ranker" {
    let items = [10, 20, 15, 5]
    
    let ranker = MultiSignalRanker::<Int>.new()
        .add_signal(|x| Signal.new("value", x as Float, 1.0))
        .add_signal(|x| Signal.new("inverse", 100.0 / x as Float, 0.5))
    
    let ranked = ranker.rank(items)
    assert_eq(ranked[0].item, 20)?  // Highest value
}
