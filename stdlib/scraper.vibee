// =============================================================================
// Vibee OS â€” Web Scraper Module
// Web scraping with CSS selectors and data extraction
// =============================================================================

use http::{Client, Request, Response, HttpError}
use html_parser::{HtmlDocument, HtmlElement, HtmlParseError}
use url::{URL, URLError}
use result::{Result, Ok, Err}
use option::{Option, Some, None}

// =============================================================================
// Scraper
// =============================================================================

/// Web scraper for extracting data from web pages
struct Scraper {
    client: Client
    base_url: Option<URL>
    user_agent: String
    delay_ms: Int64
    retry_count: Int
    retry_delay_ms: Int64
    
    /// Create new scraper
    fn new() -> Self {
        Scraper {
            client: Client.new(),
            base_url: None,
            user_agent: "Vibee-Scraper/1.0",
            delay_ms: 0,
            retry_count: 3,
            retry_delay_ms: 1000
        }
    }
    
    /// Set base URL for relative links
    fn base_url(url: String) -> Result<Self, URLError> {
        let parsed = URL.parse(url)?
        Ok(Scraper { base_url: Some(parsed), ..self })
    }
    
    /// Set user agent
    fn user_agent(ua: String) -> Self {
        Scraper { user_agent: ua, ..self }
    }
    
    /// Set delay between requests
    fn delay(ms: Int64) -> Self {
        Scraper { delay_ms: ms, ..self }
    }
    
    /// Set retry configuration
    fn retry(count: Int, delay_ms: Int64) -> Self {
        Scraper { retry_count: count, retry_delay_ms: delay_ms, ..self }
    }
    
    /// Set timeout
    fn timeout(ms: Int64) -> Self {
        Scraper { client: self.client.timeout(ms), ..self }
    }
    
    /// Fetch and parse HTML from URL
    fn fetch(url: String) -> Result<ScrapedPage, ScrapeError> {
        let full_url = self.resolve_url(url)?
        
        var last_error = None
        for attempt in 0..self.retry_count {
            if attempt > 0 {
                @native("sleep_ms", self.retry_delay_ms)
            }
            
            match self.do_fetch(full_url.to_string()) {
                Ok(page) => return Ok(page)
                Err(e) => last_error = Some(e)
            }
        }
        
        Err(last_error.unwrap_or(ScrapeError.NetworkError("Unknown error")))
    }
    
    fn do_fetch(url: String) -> Result<ScrapedPage, ScrapeError> {
        if self.delay_ms > 0 {
            @native("sleep_ms", self.delay_ms)
        }
        
        let req = Request.get(url)
            .header("User-Agent", self.user_agent)
            .header("Accept", "text/html,application/xhtml+xml")
            .header("Accept-Language", "en-US,en;q=0.9")
        
        let response = self.client.send(req)
            .map_err(|e| ScrapeError.NetworkError(e.to_string()))?
        
        if !response.is_success() {
            return Err(ScrapeError.HttpError(response.status.code))
        }
        
        let html = response.text()
        let document = HtmlDocument.parse(html)
            .map_err(|e| ScrapeError.ParseError(e.to_string()))?
        
        Ok(ScrapedPage {
            url: url,
            document: document,
            response: response,
            scraper: self
        })
    }
    
    fn resolve_url(url: String) -> Result<URL, ScrapeError> {
        if url.contains("://") {
            URL.parse(url).map_err(|e| ScrapeError.InvalidUrl(e.to_string()))
        } else if let Some(base) = self.base_url {
            base.resolve(url).map_err(|e| ScrapeError.InvalidUrl(e.to_string()))
        } else {
            Err(ScrapeError.InvalidUrl("Relative URL without base URL"))
        }
    }
    
    /// Fetch multiple URLs
    fn fetch_all(urls: [String]) -> [Result<ScrapedPage, ScrapeError>] {
        urls.iter().map(|url| self.fetch(url)).collect()
    }
    
    /// Create extraction builder
    fn extract() -> ExtractionBuilder {
        ExtractionBuilder.new(self)
    }
}

// =============================================================================
// Scraped Page
// =============================================================================

/// Scraped page with parsed HTML
struct ScrapedPage {
    url: String
    document: HtmlDocument
    response: Response
    scraper: Scraper
    
    /// Get page title
    fn title() -> Option<String> {
        self.document.title()
    }
    
    /// Query single element
    fn select(selector: String) -> Option<HtmlElement> {
        self.document.query_selector(selector)
    }
    
    /// Query all matching elements
    fn select_all(selector: String) -> [HtmlElement] {
        self.document.query_selector_all(selector)
    }
    
    /// Get text from selector
    fn text(selector: String) -> Option<String> {
        self.select(selector)?.text_content().trim().to_string().into()
    }
    
    /// Get all texts from selector
    fn texts(selector: String) -> [String] {
        self.select_all(selector)
            .iter()
            .map(|e| e.text_content().trim())
            .filter(|t| !t.is_empty())
            .collect()
    }
    
    /// Get attribute from selector
    fn attr(selector: String, attr: String) -> Option<String> {
        self.select(selector)?.get_attribute(attr)
    }
    
    /// Get all attributes from selector
    fn attrs(selector: String, attr: String) -> [String] {
        self.select_all(selector)
            .iter()
            .filter_map(|e| e.get_attribute(attr))
            .collect()
    }
    
    /// Get all links
    fn links() -> [Link] {
        self.document.links()
            .iter()
            .filter_map(|e| {
                let href = e.href()?
                let text = e.text_content().trim()
                Some(Link { href: href, text: text, element: e })
            })
            .collect()
    }
    
    /// Get all images
    fn images() -> [Image] {
        self.document.images()
            .iter()
            .filter_map(|e| {
                let src = e.src()?
                let alt = e.get_attribute("alt").unwrap_or("")
                Some(Image { src: src, alt: alt, element: e })
            })
            .collect()
    }
    
    /// Get meta content
    fn meta(name: String) -> Option<String> {
        self.select(format!("meta[name=\"{}\"]", name))?.get_attribute("content")
    }
    
    /// Get Open Graph meta
    fn og(property: String) -> Option<String> {
        self.select(format!("meta[property=\"og:{}\"]", property))?.get_attribute("content")
    }
    
    /// Extract structured data
    fn extract<T: FromScrapedPage>() -> Result<T, ScrapeError> {
        T.from_scraped_page(self)
    }
    
    /// Extract table data
    fn table(selector: String) -> Option<TableData> {
        let table = self.select(selector)?
        Some(TableData.from_element(table))
    }
    
    /// Extract all tables
    fn tables() -> [TableData] {
        self.select_all("table")
            .iter()
            .map(|t| TableData.from_element(t))
            .collect()
    }
    
    /// Follow link
    fn follow(selector: String) -> Result<ScrapedPage, ScrapeError> {
        let href = self.attr(selector, "href")
            .ok_or(ScrapeError.ElementNotFound(selector))?
        self.scraper.fetch(href)
    }
    
    /// Get absolute URL
    fn absolute_url(relative: String) -> Result<String, ScrapeError> {
        let base = URL.parse(self.url)
            .map_err(|e| ScrapeError.InvalidUrl(e.to_string()))?
        let resolved = base.resolve(relative)
            .map_err(|e| ScrapeError.InvalidUrl(e.to_string()))?
        Ok(resolved.to_string())
    }
    
    /// Get HTML content
    fn html() -> String {
        self.document.to_html()
    }
    
    /// Get text content
    fn text_content() -> String {
        self.document.text_content()
    }
}

// =============================================================================
// Extraction Builder
// =============================================================================

/// Fluent extraction builder
struct ExtractionBuilder {
    scraper: Scraper
    fields: [FieldExtractor]
    
    fn new(scraper: Scraper) -> Self {
        ExtractionBuilder { scraper: scraper, fields: [] }
    }
    
    /// Add text field
    fn text(name: String, selector: String) -> Self {
        self.fields.push(FieldExtractor {
            name: name,
            selector: selector,
            extractor: ExtractorType.Text,
            required: false,
            default: None,
            transform: None
        })
        self
    }
    
    /// Add attribute field
    fn attr(name: String, selector: String, attr: String) -> Self {
        self.fields.push(FieldExtractor {
            name: name,
            selector: selector,
            extractor: ExtractorType.Attribute(attr),
            required: false,
            default: None,
            transform: None
        })
        self
    }
    
    /// Add HTML field
    fn html(name: String, selector: String) -> Self {
        self.fields.push(FieldExtractor {
            name: name,
            selector: selector,
            extractor: ExtractorType.Html,
            required: false,
            default: None,
            transform: None
        })
        self
    }
    
    /// Add list field
    fn list(name: String, selector: String) -> Self {
        self.fields.push(FieldExtractor {
            name: name,
            selector: selector,
            extractor: ExtractorType.List,
            required: false,
            default: None,
            transform: None
        })
        self
    }
    
    /// Mark last field as required
    fn required() -> Self {
        if let Some(last) = self.fields.last_mut() {
            last.required = true
        }
        self
    }
    
    /// Set default value for last field
    fn default(value: String) -> Self {
        if let Some(last) = self.fields.last_mut() {
            last.default = Some(value)
        }
        self
    }
    
    /// Add transform for last field
    fn transform(f: fn(String) -> String) -> Self {
        if let Some(last) = self.fields.last_mut() {
            last.transform = Some(f)
        }
        self
    }
    
    /// Extract from URL
    fn from_url(url: String) -> Result<Map<String, ExtractedValue>, ScrapeError> {
        let page = self.scraper.fetch(url)?
        self.from_page(page)
    }
    
    /// Extract from page
    fn from_page(page: ScrapedPage) -> Result<Map<String, ExtractedValue>, ScrapeError> {
        var result = Map.empty()
        
        for field in self.fields {
            let value = field.extract(page.document)?
            result.set(field.name, value)
        }
        
        Ok(result)
    }
}

/// Field extractor
struct FieldExtractor {
    name: String
    selector: String
    extractor: ExtractorType
    required: Bool
    default: Option<String>
    transform: Option<fn(String) -> String>
    
    fn extract(doc: HtmlDocument) -> Result<ExtractedValue, ScrapeError> {
        let value = match self.extractor {
            Text => {
                match doc.query_selector(self.selector) {
                    Some(e) => ExtractedValue.Single(e.text_content().trim())
                    None => {
                        if self.required {
                            return Err(ScrapeError.RequiredFieldMissing(self.name))
                        }
                        match self.default {
                            Some(d) => ExtractedValue.Single(d)
                            None => ExtractedValue.None
                        }
                    }
                }
            }
            Attribute(attr) => {
                match doc.query_selector(self.selector) {
                    Some(e) => match e.get_attribute(attr) {
                        Some(v) => ExtractedValue.Single(v)
                        None => {
                            if self.required {
                                return Err(ScrapeError.RequiredFieldMissing(self.name))
                            }
                            match self.default {
                                Some(d) => ExtractedValue.Single(d)
                                None => ExtractedValue.None
                            }
                        }
                    }
                    None => {
                        if self.required {
                            return Err(ScrapeError.RequiredFieldMissing(self.name))
                        }
                        match self.default {
                            Some(d) => ExtractedValue.Single(d)
                            None => ExtractedValue.None
                        }
                    }
                }
            }
            Html => {
                match doc.query_selector(self.selector) {
                    Some(e) => ExtractedValue.Single(e.inner_html())
                    None => {
                        if self.required {
                            return Err(ScrapeError.RequiredFieldMissing(self.name))
                        }
                        ExtractedValue.None
                    }
                }
            }
            List => {
                let elements = doc.query_selector_all(self.selector)
                let values = elements.iter()
                    .map(|e| e.text_content().trim())
                    .filter(|t| !t.is_empty())
                    .collect()
                ExtractedValue.Multiple(values)
            }
        }
        
        // Apply transform
        let value = match (value, self.transform) {
            (ExtractedValue.Single(s), Some(f)) => ExtractedValue.Single(f(s))
            (ExtractedValue.Multiple(vs), Some(f)) => ExtractedValue.Multiple(vs.iter().map(f).collect())
            (v, _) => v
        }
        
        Ok(value)
    }
}

/// Extractor type
enum ExtractorType {
    Text
    Attribute(String)
    Html
    List
}

/// Extracted value
enum ExtractedValue {
    Single(String)
    Multiple([String])
    None
    
    fn as_string() -> Option<String> {
        match self {
            Single(s) => Some(s)
            _ => None
        }
    }
    
    fn as_list() -> [String] {
        match self {
            Single(s) => [s]
            Multiple(vs) => vs
            None => []
        }
    }
    
    fn is_none() -> Bool {
        match self {
            None => true
            _ => false
        }
    }
}

// =============================================================================
// Data Types
// =============================================================================

/// Link data
struct Link {
    href: String
    text: String
    element: HtmlElement
    
    fn is_external(base_url: String) -> Bool {
        !self.href.starts_with(base_url) && self.href.contains("://")
    }
    
    fn is_internal(base_url: String) -> Bool {
        !self.is_external(base_url)
    }
}

/// Image data
struct Image {
    src: String
    alt: String
    element: HtmlElement
    
    fn width() -> Option<Int> {
        self.element.get_attribute("width").and_then(|w| Int.parse(w).ok())
    }
    
    fn height() -> Option<Int> {
        self.element.get_attribute("height").and_then(|h| Int.parse(h).ok())
    }
}

/// Table data
struct TableData {
    headers: [String]
    rows: [[String]]
    
    fn from_element(table: HtmlElement) -> Self {
        var headers = []
        var rows = []
        
        // Extract headers
        for th in table.query_selector_all("thead th, tr:first-child th") {
            headers.push(th.text_content().trim())
        }
        
        // If no thead, try first row
        if headers.is_empty() {
            for td in table.query_selector_all("tr:first-child td") {
                headers.push(td.text_content().trim())
            }
        }
        
        // Extract rows
        let row_selector = if headers.is_empty() { "tr" } else { "tbody tr, tr:not(:first-child)" }
        for tr in table.query_selector_all(row_selector) {
            let cells = tr.query_selector_all("td")
                .iter()
                .map(|td| td.text_content().trim())
                .collect()
            if !cells.is_empty() {
                rows.push(cells)
            }
        }
        
        TableData { headers: headers, rows: rows }
    }
    
    /// Get column by index
    fn column(index: Int) -> [String] {
        self.rows.iter()
            .filter_map(|row| row.get(index).cloned())
            .collect()
    }
    
    /// Get column by header name
    fn column_by_name(name: String) -> [String] {
        match self.headers.iter().position(|h| h == name) {
            Some(idx) => self.column(idx)
            None => []
        }
    }
    
    /// Get row by index
    fn row(index: Int) -> Option<[String]> {
        self.rows.get(index).cloned()
    }
    
    /// Convert to maps
    fn to_maps() -> [Map<String, String>] {
        self.rows.iter().map(|row| {
            var map = Map.empty()
            for (i, cell) in row.iter().enumerate() {
                let key = self.headers.get(i).cloned().unwrap_or(format!("col_{}", i))
                map.set(key, cell)
            }
            map
        }).collect()
    }
    
    /// Number of rows
    fn len() -> Int {
        self.rows.len()
    }
    
    /// Check if empty
    fn is_empty() -> Bool {
        self.rows.is_empty()
    }
}

// =============================================================================
// Trait for Custom Extraction
// =============================================================================

/// Trait for types that can be extracted from scraped pages
trait FromScrapedPage {
    fn from_scraped_page(page: ScrapedPage) -> Result<Self, ScrapeError>
}

// =============================================================================
// Errors
// =============================================================================

/// Scrape error
enum ScrapeError {
    NetworkError(String)
    HttpError(Int)
    ParseError(String)
    InvalidUrl(String)
    ElementNotFound(String)
    RequiredFieldMissing(String)
    ExtractionError(String)
    RateLimited
    Blocked
}

impl Display for ScrapeError {
    fn fmt(f: Formatter) {
        match self {
            NetworkError(s) => f.write(format!("Network error: {}", s))
            HttpError(code) => f.write(format!("HTTP error: {}", code))
            ParseError(s) => f.write(format!("Parse error: {}", s))
            InvalidUrl(s) => f.write(format!("Invalid URL: {}", s))
            ElementNotFound(s) => f.write(format!("Element not found: {}", s))
            RequiredFieldMissing(s) => f.write(format!("Required field missing: {}", s))
            ExtractionError(s) => f.write(format!("Extraction error: {}", s))
            RateLimited => f.write("Rate limited")
            Blocked => f.write("Blocked by server")
        }
    }
}

// =============================================================================
// Convenience Functions
// =============================================================================

/// Quick scrape
fn scrape(url: String) -> Result<ScrapedPage, ScrapeError> {
    Scraper.new().fetch(url)
}

/// Quick text extraction
fn scrape_text(url: String, selector: String) -> Result<String, ScrapeError> {
    let page = scrape(url)?
    page.text(selector).ok_or(ScrapeError.ElementNotFound(selector))
}

/// Quick link extraction
fn scrape_links(url: String) -> Result<[Link], ScrapeError> {
    let page = scrape(url)?
    Ok(page.links())
}

// =============================================================================
// Tests
// =============================================================================

test "scraper creation" {
    let scraper = Scraper.new()
        .user_agent("TestBot/1.0")
        .delay(100)
        .timeout(5000)
    
    assert_eq(scraper.user_agent, "TestBot/1.0")?
    assert_eq(scraper.delay_ms, 100)?
}

test "extraction builder" {
    let builder = ExtractionBuilder.new(Scraper.new())
        .text("title", "h1")
        .required()
        .attr("link", "a.main", "href")
        .default("https://example.com")
        .list("items", "ul li")
    
    assert_eq(builder.fields.len(), 3)?
}

test "table data" {
    let table = TableData {
        headers: ["Name", "Age"],
        rows: [["Alice", "30"], ["Bob", "25"]]
    }
    
    assert_eq(table.column(0), ["Alice", "Bob"])?
    assert_eq(table.column_by_name("Age"), ["30", "25"])?
}

test "extracted value" {
    let single = ExtractedValue.Single("hello")
    assert_eq(single.as_string(), Some("hello"))?
    
    let multiple = ExtractedValue.Multiple(["a", "b"])
    assert_eq(multiple.as_list(), ["a", "b"])?
}

test "link external check" {
    let link = Link { href: "https://other.com/page", text: "External", element: HtmlElement.new("a") }
    assert(link.is_external("https://example.com"))?
    
    let internal = Link { href: "/page", text: "Internal", element: HtmlElement.new("a") }
    assert(internal.is_internal("https://example.com"))?
}
