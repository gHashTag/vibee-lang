// =============================================================================
// Vibee OS â€” Neural Network Module
// Neural network layers and building blocks
// =============================================================================

use tensor::{Tensor, Shape}

// -----------------------------------------------------------------------------
// Module Trait
// -----------------------------------------------------------------------------

/// Base trait for all neural network modules
trait Module {
    fn forward(input: Tensor) -> Tensor
    fn parameters() -> [Tensor]
    fn train() { }
    fn eval() { }
}

// -----------------------------------------------------------------------------
// Activation Functions
// -----------------------------------------------------------------------------

/// ReLU activation
struct ReLU {}
impl Module for ReLU {
    fn forward(input: Tensor) -> Tensor { input.relu() }
    fn parameters() -> [Tensor] { [] }
}

/// Leaky ReLU activation
struct LeakyReLU { negative_slope: Float }
impl LeakyReLU { fn new(negative_slope: Float = 0.01) -> Self { LeakyReLU { negative_slope: negative_slope } } }
impl Module for LeakyReLU {
    fn forward(input: Tensor) -> Tensor { input.leaky_relu(self.negative_slope) }
    fn parameters() -> [Tensor] { [] }
}

/// Sigmoid activation
struct Sigmoid {}
impl Module for Sigmoid {
    fn forward(input: Tensor) -> Tensor { input.sigmoid() }
    fn parameters() -> [Tensor] { [] }
}

/// Tanh activation
struct Tanh {}
impl Module for Tanh {
    fn forward(input: Tensor) -> Tensor { input.tanh() }
    fn parameters() -> [Tensor] { [] }
}

/// GELU activation
struct GELU {}
impl Module for GELU {
    fn forward(input: Tensor) -> Tensor { input.gelu() }
    fn parameters() -> [Tensor] { [] }
}

/// SiLU (Swish) activation
struct SiLU {}
impl Module for SiLU {
    fn forward(input: Tensor) -> Tensor { input.silu() }
    fn parameters() -> [Tensor] { [] }
}

/// Softmax activation
struct Softmax { dim: Int }
impl Softmax { fn new(dim: Int = -1) -> Self { Softmax { dim: dim } } }
impl Module for Softmax {
    fn forward(input: Tensor) -> Tensor { input.softmax() }
    fn parameters() -> [Tensor] { [] }
}

/// LogSoftmax activation
struct LogSoftmax { dim: Int }
impl LogSoftmax { fn new(dim: Int = -1) -> Self { LogSoftmax { dim: dim } } }
impl Module for LogSoftmax {
    fn forward(input: Tensor) -> Tensor { input.log_softmax() }
    fn parameters() -> [Tensor] { [] }
}

// -----------------------------------------------------------------------------
// Linear Layers
// -----------------------------------------------------------------------------

/// Fully connected linear layer
struct Linear {
    in_features: Int
    out_features: Int
    weight: Tensor
    bias: Option<Tensor>
    
    fn new(in_features: Int, out_features: Int, bias: Bool = true) -> Self {
        let k = 1.0 / (in_features as Float).sqrt()
        let weight = Tensor.randn(Shape.matrix(out_features, in_features)).mul_scalar(k)
        let b = if bias { Some(Tensor.randn(Shape.vector(out_features)).mul_scalar(k)) } else { None }
        Linear { in_features: in_features, out_features: out_features, weight: weight, bias: b }
    }
}

impl Module for Linear {
    fn forward(input: Tensor) -> Tensor {
        var output = input.matmul(self.weight.t())
        if let Some(b) = self.bias { output = output.add(b) }
        output
    }
    fn parameters() -> [Tensor] {
        match self.bias { Some(b) => [self.weight.clone(), b.clone()], None => [self.weight.clone()] }
    }
}

/// Identity layer (pass-through)
struct Identity {}
impl Module for Identity {
    fn forward(input: Tensor) -> Tensor { input }
    fn parameters() -> [Tensor] { [] }
}

// -----------------------------------------------------------------------------
// Convolutional Layers
// -----------------------------------------------------------------------------

/// 2D Convolution layer
struct Conv2d {
    in_channels: Int
    out_channels: Int
    kernel_size: (Int, Int)
    stride: (Int, Int)
    padding: (Int, Int)
    weight: Tensor
    bias: Option<Tensor>
    
    fn new(in_channels: Int, out_channels: Int, kernel_size: Int, stride: Int = 1, padding: Int = 0, bias: Bool = true) -> Self {
        let k = 1.0 / ((in_channels * kernel_size * kernel_size) as Float).sqrt()
        let weight = Tensor.randn(Shape.new([out_channels, in_channels, kernel_size, kernel_size])).mul_scalar(k)
        let b = if bias { Some(Tensor.randn(Shape.vector(out_channels)).mul_scalar(k)) } else { None }
        Conv2d {
            in_channels: in_channels, out_channels: out_channels,
            kernel_size: (kernel_size, kernel_size), stride: (stride, stride), padding: (padding, padding),
            weight: weight, bias: b
        }
    }
}

impl Module for Conv2d {
    fn forward(input: Tensor) -> Tensor {
        // Simplified convolution (would use native implementation)
        @native("conv2d_forward", input, self.weight, self.bias, self.stride, self.padding)
    }
    fn parameters() -> [Tensor] {
        match self.bias { Some(b) => [self.weight.clone(), b.clone()], None => [self.weight.clone()] }
    }
}

/// 2D Transposed Convolution
struct ConvTranspose2d {
    in_channels: Int
    out_channels: Int
    kernel_size: (Int, Int)
    stride: (Int, Int)
    padding: (Int, Int)
    weight: Tensor
    bias: Option<Tensor>
    
    fn new(in_channels: Int, out_channels: Int, kernel_size: Int, stride: Int = 1, padding: Int = 0, bias: Bool = true) -> Self {
        let k = 1.0 / ((in_channels * kernel_size * kernel_size) as Float).sqrt()
        let weight = Tensor.randn(Shape.new([in_channels, out_channels, kernel_size, kernel_size])).mul_scalar(k)
        let b = if bias { Some(Tensor.randn(Shape.vector(out_channels)).mul_scalar(k)) } else { None }
        ConvTranspose2d {
            in_channels: in_channels, out_channels: out_channels,
            kernel_size: (kernel_size, kernel_size), stride: (stride, stride), padding: (padding, padding),
            weight: weight, bias: b
        }
    }
}

impl Module for ConvTranspose2d {
    fn forward(input: Tensor) -> Tensor {
        @native("conv_transpose2d_forward", input, self.weight, self.bias, self.stride, self.padding)
    }
    fn parameters() -> [Tensor] {
        match self.bias { Some(b) => [self.weight.clone(), b.clone()], None => [self.weight.clone()] }
    }
}

// -----------------------------------------------------------------------------
// Pooling Layers
// -----------------------------------------------------------------------------

/// 2D Max Pooling
struct MaxPool2d {
    kernel_size: (Int, Int)
    stride: (Int, Int)
    padding: (Int, Int)
    
    fn new(kernel_size: Int, stride: Option<Int> = None, padding: Int = 0) -> Self {
        let s = stride.unwrap_or(kernel_size)
        MaxPool2d { kernel_size: (kernel_size, kernel_size), stride: (s, s), padding: (padding, padding) }
    }
}

impl Module for MaxPool2d {
    fn forward(input: Tensor) -> Tensor {
        @native("max_pool2d_forward", input, self.kernel_size, self.stride, self.padding)
    }
    fn parameters() -> [Tensor] { [] }
}

/// 2D Average Pooling
struct AvgPool2d {
    kernel_size: (Int, Int)
    stride: (Int, Int)
    padding: (Int, Int)
    
    fn new(kernel_size: Int, stride: Option<Int> = None, padding: Int = 0) -> Self {
        let s = stride.unwrap_or(kernel_size)
        AvgPool2d { kernel_size: (kernel_size, kernel_size), stride: (s, s), padding: (padding, padding) }
    }
}

impl Module for AvgPool2d {
    fn forward(input: Tensor) -> Tensor {
        @native("avg_pool2d_forward", input, self.kernel_size, self.stride, self.padding)
    }
    fn parameters() -> [Tensor] { [] }
}

/// Adaptive Average Pooling
struct AdaptiveAvgPool2d { output_size: (Int, Int) }
impl AdaptiveAvgPool2d { fn new(output_size: Int) -> Self { AdaptiveAvgPool2d { output_size: (output_size, output_size) } } }
impl Module for AdaptiveAvgPool2d {
    fn forward(input: Tensor) -> Tensor { @native("adaptive_avg_pool2d_forward", input, self.output_size) }
    fn parameters() -> [Tensor] { [] }
}

// -----------------------------------------------------------------------------
// Normalization Layers
// -----------------------------------------------------------------------------

/// Batch Normalization
struct BatchNorm1d {
    num_features: Int
    eps: Float
    momentum: Float
    weight: Tensor
    bias: Tensor
    running_mean: Tensor
    running_var: Tensor
    training: Bool
    
    fn new(num_features: Int, eps: Float = 1e-5, momentum: Float = 0.1) -> Self {
        BatchNorm1d {
            num_features: num_features, eps: eps, momentum: momentum,
            weight: Tensor.ones(Shape.vector(num_features)),
            bias: Tensor.zeros(Shape.vector(num_features)),
            running_mean: Tensor.zeros(Shape.vector(num_features)),
            running_var: Tensor.ones(Shape.vector(num_features)),
            training: true
        }
    }
}

impl Module for BatchNorm1d {
    fn forward(input: Tensor) -> Tensor {
        let mean = if self.training { Tensor.scalar(input.mean()) } else { self.running_mean.clone() }
        let var = if self.training { Tensor.scalar(input.var()) } else { self.running_var.clone() }
        let normalized = input.sub(mean).div(var.add_scalar(self.eps).sqrt())
        normalized.mul(self.weight).add(self.bias)
    }
    fn parameters() -> [Tensor] { [self.weight.clone(), self.bias.clone()] }
    fn train() { self.training = true }
    fn eval() { self.training = false }
}

/// Batch Normalization 2D
struct BatchNorm2d {
    num_features: Int
    eps: Float
    momentum: Float
    weight: Tensor
    bias: Tensor
    running_mean: Tensor
    running_var: Tensor
    training: Bool
    
    fn new(num_features: Int, eps: Float = 1e-5, momentum: Float = 0.1) -> Self {
        BatchNorm2d {
            num_features: num_features, eps: eps, momentum: momentum,
            weight: Tensor.ones(Shape.vector(num_features)),
            bias: Tensor.zeros(Shape.vector(num_features)),
            running_mean: Tensor.zeros(Shape.vector(num_features)),
            running_var: Tensor.ones(Shape.vector(num_features)),
            training: true
        }
    }
}

impl Module for BatchNorm2d {
    fn forward(input: Tensor) -> Tensor {
        @native("batch_norm2d_forward", input, self.weight, self.bias, self.running_mean, self.running_var, self.training, self.eps, self.momentum)
    }
    fn parameters() -> [Tensor] { [self.weight.clone(), self.bias.clone()] }
    fn train() { self.training = true }
    fn eval() { self.training = false }
}

/// Layer Normalization
struct LayerNorm {
    normalized_shape: [Int]
    eps: Float
    weight: Tensor
    bias: Tensor
    
    fn new(normalized_shape: [Int], eps: Float = 1e-5) -> Self {
        let size = normalized_shape.iter().product().unwrap_or(1)
        LayerNorm {
            normalized_shape: normalized_shape, eps: eps,
            weight: Tensor.ones(Shape.vector(size)),
            bias: Tensor.zeros(Shape.vector(size))
        }
    }
}

impl Module for LayerNorm {
    fn forward(input: Tensor) -> Tensor {
        let mean = input.mean()
        let var = input.var()
        let normalized = input.add_scalar(-mean).mul_scalar(1.0 / (var + self.eps).sqrt())
        normalized.mul(self.weight).add(self.bias)
    }
    fn parameters() -> [Tensor] { [self.weight.clone(), self.bias.clone()] }
}

// -----------------------------------------------------------------------------
// Dropout Layers
// -----------------------------------------------------------------------------

/// Dropout layer
struct Dropout {
    p: Float
    training: Bool
    
    fn new(p: Float = 0.5) -> Self { Dropout { p: p, training: true } }
}

impl Module for Dropout {
    fn forward(input: Tensor) -> Tensor {
        if !self.training || self.p == 0.0 { return input }
        let mask = Tensor.rand(input.shape.clone()).unary_op(|x| if x > self.p { 1.0 / (1.0 - self.p) } else { 0.0 })
        input.mul(mask)
    }
    fn parameters() -> [Tensor] { [] }
    fn train() { self.training = true }
    fn eval() { self.training = false }
}

/// Dropout 2D (spatial dropout)
struct Dropout2d {
    p: Float
    training: Bool
    
    fn new(p: Float = 0.5) -> Self { Dropout2d { p: p, training: true } }
}

impl Module for Dropout2d {
    fn forward(input: Tensor) -> Tensor {
        if !self.training || self.p == 0.0 { return input }
        @native("dropout2d_forward", input, self.p)
    }
    fn parameters() -> [Tensor] { [] }
    fn train() { self.training = true }
    fn eval() { self.training = false }
}

// -----------------------------------------------------------------------------
// Recurrent Layers
// -----------------------------------------------------------------------------

/// LSTM layer
struct LSTM {
    input_size: Int
    hidden_size: Int
    num_layers: Int
    bidirectional: Bool
    weight_ih: [Tensor]
    weight_hh: [Tensor]
    bias_ih: [Tensor]
    bias_hh: [Tensor]
    
    fn new(input_size: Int, hidden_size: Int, num_layers: Int = 1, bidirectional: Bool = false) -> Self {
        var weight_ih = []
        var weight_hh = []
        var bias_ih = []
        var bias_hh = []
        
        let num_directions = if bidirectional { 2 } else { 1 }
        for layer in 0..num_layers {
            for dir in 0..num_directions {
                let in_size = if layer == 0 { input_size } else { hidden_size * num_directions }
                weight_ih.push(Tensor.randn(Shape.matrix(4 * hidden_size, in_size)))
                weight_hh.push(Tensor.randn(Shape.matrix(4 * hidden_size, hidden_size)))
                bias_ih.push(Tensor.zeros(Shape.vector(4 * hidden_size)))
                bias_hh.push(Tensor.zeros(Shape.vector(4 * hidden_size)))
            }
        }
        
        LSTM {
            input_size: input_size, hidden_size: hidden_size, num_layers: num_layers,
            bidirectional: bidirectional, weight_ih: weight_ih, weight_hh: weight_hh,
            bias_ih: bias_ih, bias_hh: bias_hh
        }
    }
}

impl Module for LSTM {
    fn forward(input: Tensor) -> Tensor {
        @native("lstm_forward", input, self.weight_ih, self.weight_hh, self.bias_ih, self.bias_hh)
    }
    fn parameters() -> [Tensor] {
        var params = []
        params.extend(self.weight_ih.iter())
        params.extend(self.weight_hh.iter())
        params.extend(self.bias_ih.iter())
        params.extend(self.bias_hh.iter())
        params
    }
}

/// GRU layer
struct GRU {
    input_size: Int
    hidden_size: Int
    num_layers: Int
    weight_ih: [Tensor]
    weight_hh: [Tensor]
    bias_ih: [Tensor]
    bias_hh: [Tensor]
    
    fn new(input_size: Int, hidden_size: Int, num_layers: Int = 1) -> Self {
        var weight_ih = []
        var weight_hh = []
        var bias_ih = []
        var bias_hh = []
        
        for layer in 0..num_layers {
            let in_size = if layer == 0 { input_size } else { hidden_size }
            weight_ih.push(Tensor.randn(Shape.matrix(3 * hidden_size, in_size)))
            weight_hh.push(Tensor.randn(Shape.matrix(3 * hidden_size, hidden_size)))
            bias_ih.push(Tensor.zeros(Shape.vector(3 * hidden_size)))
            bias_hh.push(Tensor.zeros(Shape.vector(3 * hidden_size)))
        }
        
        GRU {
            input_size: input_size, hidden_size: hidden_size, num_layers: num_layers,
            weight_ih: weight_ih, weight_hh: weight_hh, bias_ih: bias_ih, bias_hh: bias_hh
        }
    }
}

impl Module for GRU {
    fn forward(input: Tensor) -> Tensor {
        @native("gru_forward", input, self.weight_ih, self.weight_hh, self.bias_ih, self.bias_hh)
    }
    fn parameters() -> [Tensor] {
        var params = []
        params.extend(self.weight_ih.iter())
        params.extend(self.weight_hh.iter())
        params.extend(self.bias_ih.iter())
        params.extend(self.bias_hh.iter())
        params
    }
}

// -----------------------------------------------------------------------------
// Embedding Layers
// -----------------------------------------------------------------------------

/// Embedding layer
struct Embedding {
    num_embeddings: Int
    embedding_dim: Int
    weight: Tensor
    
    fn new(num_embeddings: Int, embedding_dim: Int) -> Self {
        Embedding {
            num_embeddings: num_embeddings,
            embedding_dim: embedding_dim,
            weight: Tensor.randn(Shape.matrix(num_embeddings, embedding_dim))
        }
    }
}

impl Module for Embedding {
    fn forward(input: Tensor) -> Tensor {
        // Lookup embeddings by indices
        @native("embedding_forward", input, self.weight)
    }
    fn parameters() -> [Tensor] { [self.weight.clone()] }
}

// -----------------------------------------------------------------------------
// Attention Layers
// -----------------------------------------------------------------------------

/// Multi-Head Attention
struct MultiHeadAttention {
    embed_dim: Int
    num_heads: Int
    head_dim: Int
    q_proj: Linear
    k_proj: Linear
    v_proj: Linear
    out_proj: Linear
    
    fn new(embed_dim: Int, num_heads: Int) -> Self {
        let head_dim = embed_dim / num_heads
        MultiHeadAttention {
            embed_dim: embed_dim, num_heads: num_heads, head_dim: head_dim,
            q_proj: Linear.new(embed_dim, embed_dim),
            k_proj: Linear.new(embed_dim, embed_dim),
            v_proj: Linear.new(embed_dim, embed_dim),
            out_proj: Linear.new(embed_dim, embed_dim)
        }
    }
}

impl Module for MultiHeadAttention {
    fn forward(input: Tensor) -> Tensor {
        let q = self.q_proj.forward(input)
        let k = self.k_proj.forward(input)
        let v = self.v_proj.forward(input)
        
        // Scaled dot-product attention
        let scale = 1.0 / (self.head_dim as Float).sqrt()
        let scores = q.matmul(k.t()).mul_scalar(scale)
        let attn = scores.softmax()
        let context = attn.matmul(v)
        
        self.out_proj.forward(context)
    }
    fn parameters() -> [Tensor] {
        var params = []
        params.extend(self.q_proj.parameters().iter())
        params.extend(self.k_proj.parameters().iter())
        params.extend(self.v_proj.parameters().iter())
        params.extend(self.out_proj.parameters().iter())
        params
    }
}

// -----------------------------------------------------------------------------
// Container Modules
// -----------------------------------------------------------------------------

/// Sequential container
struct Sequential {
    modules: [Box<dyn Module>]
    
    fn new(modules: [Box<dyn Module>]) -> Self { Sequential { modules: modules } }
    fn add(module: Box<dyn Module>) { self.modules.push(module) }
}

impl Module for Sequential {
    fn forward(input: Tensor) -> Tensor {
        var x = input
        for module in self.modules { x = module.forward(x) }
        x
    }
    fn parameters() -> [Tensor] {
        var params = []
        for module in self.modules { params.extend(module.parameters().iter()) }
        params
    }
    fn train() { for module in self.modules { module.train() } }
    fn eval() { for module in self.modules { module.eval() } }
}

/// ModuleList container
struct ModuleList {
    modules: [Box<dyn Module>]
    
    fn new(modules: [Box<dyn Module>] = []) -> Self { ModuleList { modules: modules } }
    fn append(module: Box<dyn Module>) { self.modules.push(module) }
    fn get(idx: Int) -> Box<dyn Module> { self.modules[idx].clone() }
    fn len() -> Int { self.modules.len() }
}

impl Module for ModuleList {
    fn forward(input: Tensor) -> Tensor { input }  // No default forward
    fn parameters() -> [Tensor] {
        var params = []
        for module in self.modules { params.extend(module.parameters().iter()) }
        params
    }
}

// -----------------------------------------------------------------------------
// Loss Functions
// -----------------------------------------------------------------------------

/// Mean Squared Error Loss
struct MSELoss {}
impl MSELoss {
    fn new() -> Self { MSELoss {} }
    fn forward(input: Tensor, target: Tensor) -> Tensor {
        let diff = input.sub(target)
        Tensor.scalar(diff.mul(diff).mean())
    }
}

/// Cross Entropy Loss
struct CrossEntropyLoss {}
impl CrossEntropyLoss {
    fn new() -> Self { CrossEntropyLoss {} }
    fn forward(input: Tensor, target: Tensor) -> Tensor {
        let log_probs = input.log_softmax()
        // Simplified: would need proper indexing
        Tensor.scalar(-log_probs.mul(target).sum() / input.shape.rows() as Float)
    }
}

/// Binary Cross Entropy Loss
struct BCELoss {}
impl BCELoss {
    fn new() -> Self { BCELoss {} }
    fn forward(input: Tensor, target: Tensor) -> Tensor {
        let eps = 1e-7
        let clamped = input.clamp(eps, 1.0 - eps)
        let loss = target.mul(clamped.log()).add(
            Tensor.ones(target.shape.clone()).sub(target).mul(
                Tensor.ones(clamped.shape.clone()).sub(clamped).log()
            )
        )
        Tensor.scalar(-loss.mean())
    }
}

/// Binary Cross Entropy with Logits Loss
struct BCEWithLogitsLoss {}
impl BCEWithLogitsLoss {
    fn new() -> Self { BCEWithLogitsLoss {} }
    fn forward(input: Tensor, target: Tensor) -> Tensor {
        let max_val = input.clamp(0.0, Float.INFINITY)
        let loss = max_val.sub(input.mul(target)).add(
            Tensor.ones(input.shape.clone()).add(input.abs().neg().exp()).log()
        )
        Tensor.scalar(loss.mean())
    }
}

/// L1 Loss
struct L1Loss {}
impl L1Loss {
    fn new() -> Self { L1Loss {} }
    fn forward(input: Tensor, target: Tensor) -> Tensor {
        Tensor.scalar(input.sub(target).abs().mean())
    }
}

/// Smooth L1 Loss (Huber Loss)
struct SmoothL1Loss { beta: Float }
impl SmoothL1Loss {
    fn new(beta: Float = 1.0) -> Self { SmoothL1Loss { beta: beta } }
    fn forward(input: Tensor, target: Tensor) -> Tensor {
        let diff = input.sub(target).abs()
        let loss = diff.unary_op(|x| if x < self.beta { 0.5 * x * x / self.beta } else { x - 0.5 * self.beta })
        Tensor.scalar(loss.mean())
    }
}

// -----------------------------------------------------------------------------
// Utility Functions
// -----------------------------------------------------------------------------

/// Initialize weights with Xavier uniform
fn xavier_uniform(tensor: Tensor) -> Tensor {
    let fan_in = tensor.shape.get(1)
    let fan_out = tensor.shape.get(0)
    let bound = (6.0 / (fan_in + fan_out) as Float).sqrt()
    Tensor.rand(tensor.shape.clone()).mul_scalar(2.0 * bound).add_scalar(-bound)
}

/// Initialize weights with Xavier normal
fn xavier_normal(tensor: Tensor) -> Tensor {
    let fan_in = tensor.shape.get(1)
    let fan_out = tensor.shape.get(0)
    let std = (2.0 / (fan_in + fan_out) as Float).sqrt()
    Tensor.randn(tensor.shape.clone()).mul_scalar(std)
}

/// Initialize weights with Kaiming uniform
fn kaiming_uniform(tensor: Tensor, a: Float = 0.0) -> Tensor {
    let fan_in = tensor.shape.get(1)
    let gain = (2.0 / (1.0 + a * a)).sqrt()
    let bound = gain * (3.0 / fan_in as Float).sqrt()
    Tensor.rand(tensor.shape.clone()).mul_scalar(2.0 * bound).add_scalar(-bound)
}

/// Initialize weights with Kaiming normal
fn kaiming_normal(tensor: Tensor, a: Float = 0.0) -> Tensor {
    let fan_in = tensor.shape.get(1)
    let gain = (2.0 / (1.0 + a * a)).sqrt()
    let std = gain / (fan_in as Float).sqrt()
    Tensor.randn(tensor.shape.clone()).mul_scalar(std)
}

// -----------------------------------------------------------------------------
// Tests
// -----------------------------------------------------------------------------

test "linear layer" {
    let linear = Linear.new(10, 5)
    let input = Tensor.randn(Shape.matrix(2, 10))
    let output = linear.forward(input)
    assert_eq(output.shape.dims, [2, 5])?
}

test "sequential" {
    let model = Sequential.new([
        Box.new(Linear.new(10, 20)),
        Box.new(ReLU {}),
        Box.new(Linear.new(20, 5))
    ])
    let input = Tensor.randn(Shape.matrix(2, 10))
    let output = model.forward(input)
    assert_eq(output.shape.dims, [2, 5])?
}

test "mse loss" {
    let loss_fn = MSELoss.new()
    let pred = Tensor.from_vec([1.0, 2.0, 3.0])
    let target = Tensor.from_vec([1.0, 2.0, 3.0])
    let loss = loss_fn.forward(pred, target)
    assert(loss.item() < 0.0001)
}
