// =============================================================================
// Vibee OS â€” NLP Tokenizer Module
// Text tokenization for natural language processing
// =============================================================================

// =============================================================================
// Token Types
// =============================================================================

/// Token type enumeration
enum TokenType {
    Word
    Punctuation
    Number
    Whitespace
    Symbol
    Emoji
    Hashtag
    Mention
    Url
    Email
    Unknown
}

/// A single token from text
struct Token {
    text: String
    token_type: TokenType
    start: Int
    end: Int
    normalized: String
    
    fn new(text: String, token_type: TokenType, start: Int, end: Int) -> Self {
        Token {
            text: text.clone(),
            token_type: token_type,
            start: start,
            end: end,
            normalized: text.to_lowercase()
        }
    }
    
    fn len() -> Int { self.end - self.start }
    fn is_word() -> Bool { matches!(self.token_type, TokenType.Word) }
    fn is_punctuation() -> Bool { matches!(self.token_type, TokenType.Punctuation) }
    fn is_number() -> Bool { matches!(self.token_type, TokenType.Number) }
}

impl Display for Token {
    fn fmt(f: Formatter) {
        f.write(format!("Token('{}', {:?}, {}:{})", self.text, self.token_type, self.start, self.end))
    }
}

// =============================================================================
// Tokenizer Trait
// =============================================================================

/// Base trait for all tokenizers
trait Tokenizer {
    fn tokenize(text: String) -> [Token]
    fn tokenize_batch(texts: [String]) -> [[Token]] {
        texts.map(|t| self.tokenize(t))
    }
}

// =============================================================================
// Word Tokenizer
// =============================================================================

/// Simple word tokenizer based on whitespace and punctuation
struct WordTokenizer {
    lowercase: Bool
    remove_punctuation: Bool
    min_length: Int
    
    fn new() -> Self {
        WordTokenizer {
            lowercase: true,
            remove_punctuation: false,
            min_length: 1
        }
    }
    
    fn with_options(lowercase: Bool, remove_punctuation: Bool, min_length: Int) -> Self {
        WordTokenizer {
            lowercase: lowercase,
            remove_punctuation: remove_punctuation,
            min_length: min_length
        }
    }
}

impl Tokenizer for WordTokenizer {
    fn tokenize(text: String) -> [Token] {
        var tokens: [Token] = []
        var current_word = ""
        var word_start = 0
        var pos = 0
        
        for c in text.chars() {
            if c.is_whitespace() {
                if !current_word.is_empty() {
                    let token_type = classify_token(current_word)
                    if current_word.len() >= self.min_length {
                        if !self.remove_punctuation || token_type != TokenType.Punctuation {
                            let normalized = if self.lowercase { current_word.to_lowercase() } else { current_word }
                            tokens.push(Token {
                                text: current_word,
                                token_type: token_type,
                                start: word_start,
                                end: pos,
                                normalized: normalized
                            })
                        }
                    }
                    current_word = ""
                }
                word_start = pos + 1
            } else if is_punctuation(c) {
                if !current_word.is_empty() {
                    let token_type = classify_token(current_word)
                    if current_word.len() >= self.min_length {
                        let normalized = if self.lowercase { current_word.to_lowercase() } else { current_word }
                        tokens.push(Token {
                            text: current_word,
                            token_type: token_type,
                            start: word_start,
                            end: pos,
                            normalized: normalized
                        })
                    }
                    current_word = ""
                }
                if !self.remove_punctuation {
                    tokens.push(Token {
                        text: c.to_string(),
                        token_type: TokenType.Punctuation,
                        start: pos,
                        end: pos + 1,
                        normalized: c.to_string()
                    })
                }
                word_start = pos + 1
            } else {
                current_word.push(c)
            }
            pos += 1
        }
        
        // Handle last word
        if !current_word.is_empty() && current_word.len() >= self.min_length {
            let token_type = classify_token(current_word)
            if !self.remove_punctuation || token_type != TokenType.Punctuation {
                let normalized = if self.lowercase { current_word.to_lowercase() } else { current_word }
                tokens.push(Token {
                    text: current_word,
                    token_type: token_type,
                    start: word_start,
                    end: pos,
                    normalized: normalized
                })
            }
        }
        
        tokens
    }
}

// =============================================================================
// Sentence Tokenizer
// =============================================================================

/// Sentence boundary detection tokenizer
struct SentenceTokenizer {
    abbreviations: Set<String>
    
    fn new() -> Self {
        SentenceTokenizer {
            abbreviations: default_abbreviations()
        }
    }
    
    fn with_abbreviations(abbrevs: [String]) -> Self {
        SentenceTokenizer {
            abbreviations: Set.from_iter(abbrevs.iter())
        }
    }
    
    fn tokenize_sentences(text: String) -> [String] {
        var sentences: [String] = []
        var current = ""
        var i = 0
        let chars: [Char] = text.chars().collect()
        
        while i < chars.len() {
            let c = chars[i]
            current.push(c)
            
            if is_sentence_end(c) {
                // Check if it's a real sentence boundary
                let word_before = get_word_before(current)
                let is_abbrev = self.abbreviations.contains(word_before.to_lowercase())
                
                // Look ahead for next non-whitespace
                var next_char: Option<Char> = None
                var j = i + 1
                while j < chars.len() {
                    if !chars[j].is_whitespace() {
                        next_char = Some(chars[j])
                        break
                    }
                    j += 1
                }
                
                let is_boundary = !is_abbrev && 
                    next_char.map(|nc| nc.is_uppercase()).unwrap_or(true)
                
                if is_boundary {
                    sentences.push(current.trim())
                    current = ""
                }
            }
            i += 1
        }
        
        if !current.trim().is_empty() {
            sentences.push(current.trim())
        }
        
        sentences
    }
}

/// Sentence structure
struct Sentence {
    text: String
    start: Int
    end: Int
    tokens: [Token]
    
    fn word_count() -> Int {
        self.tokens.iter().filter(|t| t.is_word()).count()
    }
}

// =============================================================================
// Regex Tokenizer
// =============================================================================

/// Tokenizer based on regular expressions
struct RegexTokenizer {
    patterns: [(String, TokenType)]
    
    fn new() -> Self {
        RegexTokenizer {
            patterns: default_patterns()
        }
    }
    
    fn with_patterns(patterns: [(String, TokenType)]) -> Self {
        RegexTokenizer { patterns: patterns }
    }
    
    fn add_pattern(pattern: String, token_type: TokenType) -> Self {
        self.patterns.push((pattern, token_type))
        self
    }
}

impl Tokenizer for RegexTokenizer {
    fn tokenize(text: String) -> [Token] {
        var tokens: [Token] = []
        var remaining = text
        var offset = 0
        
        while !remaining.is_empty() {
            var best_match: Option<(Int, Int, TokenType)> = None
            
            for (pattern, token_type) in self.patterns {
                if let Ok(re) = Regex.new(format!("^{}", pattern)) {
                    if let Some(m) = re.find(remaining) {
                        if best_match.is_none() || m.end > best_match.unwrap().1 {
                            best_match = Some((m.start, m.end, token_type))
                        }
                    }
                }
            }
            
            match best_match {
                Some((start, end, token_type)) => {
                    let matched_text = remaining[start..end].to_string()
                    tokens.push(Token.new(matched_text, token_type, offset + start, offset + end))
                    remaining = remaining[end..].to_string()
                    offset += end
                }
                None => {
                    // Skip one character if no pattern matches
                    remaining = remaining[1..].to_string()
                    offset += 1
                }
            }
        }
        
        tokens
    }
}

// =============================================================================
// Subword Tokenizer (BPE-like)
// =============================================================================

/// Byte-Pair Encoding tokenizer
struct BPETokenizer {
    vocab: Map<String, Int>
    merges: [(String, String)]
    unk_token: String
    
    fn new() -> Self {
        BPETokenizer {
            vocab: Map.empty(),
            merges: [],
            unk_token: "<UNK>"
        }
    }
    
    fn from_vocab(vocab: Map<String, Int>, merges: [(String, String)]) -> Self {
        BPETokenizer {
            vocab: vocab,
            merges: merges,
            unk_token: "<UNK>"
        }
    }
    
    fn train(texts: [String], vocab_size: Int) -> Self {
        var word_freqs: Map<String, Int> = Map.empty()
        
        // Count word frequencies
        let tokenizer = WordTokenizer.new()
        for text in texts {
            for token in tokenizer.tokenize(text) {
                if token.is_word() {
                    *word_freqs.entry(token.text).or_insert(0) += 1
                }
            }
        }
        
        // Initialize with characters
        var vocab: Map<String, Int> = Map.empty()
        var idx = 0
        for (word, _) in word_freqs {
            for c in word.chars() {
                if !vocab.contains_key(c.to_string()) {
                    vocab.insert(c.to_string(), idx)
                    idx += 1
                }
            }
        }
        
        // Learn merges
        var merges: [(String, String)] = []
        while vocab.len() < vocab_size {
            let pair = find_best_pair(word_freqs)
            match pair {
                Some((a, b)) => {
                    let merged = format!("{}{}", a, b)
                    vocab.insert(merged.clone(), idx)
                    idx += 1
                    merges.push((a, b))
                    word_freqs = apply_merge(word_freqs, a, b)
                }
                None => break
            }
        }
        
        BPETokenizer { vocab: vocab, merges: merges, unk_token: "<UNK>" }
    }
    
    fn encode(text: String) -> [Int] {
        let word_tokenizer = WordTokenizer.new()
        var ids: [Int] = []
        
        for token in word_tokenizer.tokenize(text) {
            let subwords = self.tokenize_word(token.text)
            for sw in subwords {
                ids.push(self.vocab.get(sw).unwrap_or(self.vocab.get(self.unk_token).unwrap_or(0)))
            }
        }
        
        ids
    }
    
    fn decode(ids: [Int]) -> String {
        let reverse_vocab: Map<Int, String> = self.vocab.iter()
            .map(|(k, v)| (v, k))
            .collect()
        
        ids.iter()
            .map(|id| reverse_vocab.get(id).unwrap_or(self.unk_token))
            .join("")
    }
    
    fn tokenize_word(word: String) -> [String] {
        var chars: [String] = word.chars().map(|c| c.to_string()).collect()
        
        for (a, b) in self.merges {
            var i = 0
            while i < chars.len() - 1 {
                if chars[i] == a && chars[i + 1] == b {
                    chars[i] = format!("{}{}", a, b)
                    chars.remove(i + 1)
                } else {
                    i += 1
                }
            }
        }
        
        chars
    }
}

// =============================================================================
// N-gram Tokenizer
// =============================================================================

/// N-gram tokenizer for character or word n-grams
struct NGramTokenizer {
    n: Int
    level: NGramLevel
    
    fn new(n: Int) -> Self {
        NGramTokenizer { n: n, level: NGramLevel.Word }
    }
    
    fn char_ngrams(n: Int) -> Self {
        NGramTokenizer { n: n, level: NGramLevel.Character }
    }
    
    fn word_ngrams(n: Int) -> Self {
        NGramTokenizer { n: n, level: NGramLevel.Word }
    }
}

enum NGramLevel {
    Character
    Word
}

impl Tokenizer for NGramTokenizer {
    fn tokenize(text: String) -> [Token] {
        match self.level {
            NGramLevel.Character => self.char_ngram_tokens(text),
            NGramLevel.Word => self.word_ngram_tokens(text)
        }
    }
}

impl NGramTokenizer {
    fn char_ngram_tokens(text: String) -> [Token] {
        var tokens: [Token] = []
        let chars: [Char] = text.chars().collect()
        
        for i in 0..(chars.len() - self.n + 1) {
            let ngram: String = chars[i..(i + self.n)].iter().collect()
            tokens.push(Token.new(ngram, TokenType.Word, i, i + self.n))
        }
        
        tokens
    }
    
    fn word_ngram_tokens(text: String) -> [Token] {
        let word_tokenizer = WordTokenizer.new()
        let words = word_tokenizer.tokenize(text)
            .iter()
            .filter(|t| t.is_word())
            .collect::<Vec<_>>()
        
        var tokens: [Token] = []
        
        for i in 0..(words.len() - self.n + 1) {
            let ngram_words = words[i..(i + self.n)].to_vec()
            let ngram_text = ngram_words.iter().map(|t| t.text).join(" ")
            let start = ngram_words[0].start
            let end = ngram_words[ngram_words.len() - 1].end
            tokens.push(Token.new(ngram_text, TokenType.Word, start, end))
        }
        
        tokens
    }
    
    fn extract_ngrams(text: String) -> [String] {
        self.tokenize(text).iter().map(|t| t.text).collect()
    }
}

// =============================================================================
// Social Media Tokenizer
// =============================================================================

/// Tokenizer optimized for social media text
struct SocialMediaTokenizer {
    preserve_case: Bool
    reduce_lengthening: Bool
    
    fn new() -> Self {
        SocialMediaTokenizer {
            preserve_case: false,
            reduce_lengthening: true
        }
    }
}

impl Tokenizer for SocialMediaTokenizer {
    fn tokenize(text: String) -> [Token] {
        var tokens: [Token] = []
        var processed = text
        
        // Reduce lengthening (e.g., "sooooo" -> "soo")
        if self.reduce_lengthening {
            processed = reduce_repeated_chars(processed)
        }
        
        // Extract special tokens first
        let special_patterns = [
            (r"@\w+", TokenType.Mention),
            (r"#\w+", TokenType.Hashtag),
            (r"https?://\S+", TokenType.Url),
            (r"[\U0001F600-\U0001F64F]", TokenType.Emoji)
        ]
        
        var pos = 0
        var remaining = processed
        
        while !remaining.is_empty() {
            var found = false
            
            // Try special patterns first
            for (pattern, token_type) in special_patterns {
                if let Ok(re) = Regex.new(format!("^{}", pattern)) {
                    if let Some(m) = re.find(remaining) {
                        tokens.push(Token.new(m.text, token_type, pos, pos + m.len()))
                        remaining = remaining[m.len()..].to_string()
                        pos += m.len()
                        found = true
                        break
                    }
                }
            }
            
            if !found {
                // Fall back to word tokenization
                if remaining.chars().next().map(|c| c.is_whitespace()).unwrap_or(false) {
                    remaining = remaining[1..].to_string()
                    pos += 1
                } else {
                    // Extract word
                    var word = ""
                    var word_len = 0
                    for c in remaining.chars() {
                        if c.is_whitespace() || is_punctuation(c) { break }
                        word.push(c)
                        word_len += 1
                    }
                    if !word.is_empty() {
                        let normalized = if self.preserve_case { word.clone() } else { word.to_lowercase() }
                        tokens.push(Token {
                            text: word,
                            token_type: TokenType.Word,
                            start: pos,
                            end: pos + word_len,
                            normalized: normalized
                        })
                        remaining = remaining[word_len..].to_string()
                        pos += word_len
                    }
                }
            }
        }
        
        tokens
    }
}

// =============================================================================
// Whitespace Tokenizer
// =============================================================================

/// Simple whitespace-based tokenizer
struct WhitespaceTokenizer {}

impl WhitespaceTokenizer {
    fn new() -> Self { WhitespaceTokenizer {} }
}

impl Tokenizer for WhitespaceTokenizer {
    fn tokenize(text: String) -> [Token] {
        var tokens: [Token] = []
        var pos = 0
        
        for part in text.split_whitespace() {
            let start = text[pos..].find(part).map(|i| pos + i).unwrap_or(pos)
            let end = start + part.len()
            tokens.push(Token.new(part.to_string(), classify_token(part), start, end))
            pos = end
        }
        
        tokens
    }
}

// =============================================================================
// Helper Functions
// =============================================================================

fn is_punctuation(c: Char) -> Bool {
    match c {
        '.' | ',' | '!' | '?' | ';' | ':' | '"' | '\'' | 
        '(' | ')' | '[' | ']' | '{' | '}' | '-' | '/' | '\\' => true,
        _ => false
    }
}

fn is_sentence_end(c: Char) -> Bool {
    match c {
        '.' | '!' | '?' => true,
        _ => false
    }
}

fn classify_token(text: String) -> TokenType {
    if text.is_empty() { return TokenType.Unknown }
    
    let first = text.chars().next().unwrap()
    
    if text.chars().all(|c| c.is_numeric() || c == '.' || c == ',') {
        TokenType.Number
    } else if text.chars().all(|c| is_punctuation(c)) {
        TokenType.Punctuation
    } else if first == '@' {
        TokenType.Mention
    } else if first == '#' {
        TokenType.Hashtag
    } else if text.starts_with("http://") || text.starts_with("https://") {
        TokenType.Url
    } else if text.contains('@') && text.contains('.') {
        TokenType.Email
    } else if text.chars().all(|c| c.is_alphabetic() || c == '\'' || c == '-') {
        TokenType.Word
    } else {
        TokenType.Unknown
    }
}

fn get_word_before(text: String) -> String {
    let trimmed = text.trim_end_matches(|c: Char| is_sentence_end(c) || c.is_whitespace())
    let words: [String] = trimmed.split_whitespace().collect()
    words.last().unwrap_or("").to_string()
}

fn default_abbreviations() -> Set<String> {
    Set.from_iter([
        "mr", "mrs", "ms", "dr", "prof", "sr", "jr",
        "vs", "etc", "inc", "ltd", "co", "corp",
        "st", "ave", "blvd", "rd",
        "jan", "feb", "mar", "apr", "jun", "jul", "aug", "sep", "oct", "nov", "dec",
        "i.e", "e.g", "cf", "al"
    ].iter().map(|s| s.to_string()))
}

fn default_patterns() -> [(String, TokenType)] {
    [
        (r"https?://\S+", TokenType.Url),
        (r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", TokenType.Email),
        (r"@\w+", TokenType.Mention),
        (r"#\w+", TokenType.Hashtag),
        (r"\d+(?:\.\d+)?", TokenType.Number),
        (r"[a-zA-Z]+(?:'[a-zA-Z]+)?", TokenType.Word),
        (r"[.,!?;:\"'()\[\]{}]", TokenType.Punctuation),
        (r"\s+", TokenType.Whitespace)
    ]
}

fn reduce_repeated_chars(text: String) -> String {
    var result = ""
    var prev: Option<Char> = None
    var count = 0
    
    for c in text.chars() {
        if Some(c) == prev {
            count += 1
            if count <= 2 {
                result.push(c)
            }
        } else {
            result.push(c)
            prev = Some(c)
            count = 1
        }
    }
    
    result
}

fn find_best_pair(word_freqs: Map<String, Int>) -> Option<(String, String)> {
    var pair_freqs: Map<(String, String), Int> = Map.empty()
    
    for (word, freq) in word_freqs {
        let chars: [String] = word.chars().map(|c| c.to_string()).collect()
        for i in 0..(chars.len() - 1) {
            let pair = (chars[i].clone(), chars[i + 1].clone())
            *pair_freqs.entry(pair).or_insert(0) += freq
        }
    }
    
    pair_freqs.iter().max_by_key(|(_, freq)| freq).map(|(pair, _)| pair)
}

fn apply_merge(word_freqs: Map<String, Int>, a: String, b: String) -> Map<String, Int> {
    var new_freqs: Map<String, Int> = Map.empty()
    let merged = format!("{}{}", a, b)
    
    for (word, freq) in word_freqs {
        let new_word = word.replace(format!("{}{}", a, b).as_str(), merged.as_str())
        new_freqs.insert(new_word, freq)
    }
    
    new_freqs
}

// =============================================================================
// Convenience Functions
// =============================================================================

/// Quick tokenize text into words
fn tokenize(text: String) -> [String] {
    WordTokenizer.new().tokenize(text)
        .iter()
        .filter(|t| t.is_word())
        .map(|t| t.normalized)
        .collect()
}

/// Tokenize into sentences
fn sentences(text: String) -> [String] {
    SentenceTokenizer.new().tokenize_sentences(text)
}

/// Extract n-grams
fn ngrams(text: String, n: Int) -> [String] {
    NGramTokenizer.word_ngrams(n).extract_ngrams(text)
}

/// Extract character n-grams
fn char_ngrams(text: String, n: Int) -> [String] {
    NGramTokenizer.char_ngrams(n).extract_ngrams(text)
}

// =============================================================================
// Tests
// =============================================================================

test "word tokenizer basic" {
    let tokenizer = WordTokenizer.new()
    let tokens = tokenizer.tokenize("Hello, world!")
    assert_eq(tokens.len(), 4)?  // Hello, ,, world, !
    assert_eq(tokens[0].text, "Hello")?
    assert_eq(tokens[0].normalized, "hello")?
}

test "sentence tokenizer" {
    let tokenizer = SentenceTokenizer.new()
    let sentences = tokenizer.tokenize_sentences("Hello world. How are you? I'm fine!")
    assert_eq(sentences.len(), 3)?
    assert_eq(sentences[0], "Hello world.")?
}

test "sentence tokenizer with abbreviations" {
    let tokenizer = SentenceTokenizer.new()
    let sentences = tokenizer.tokenize_sentences("Dr. Smith went to the store. He bought milk.")
    assert_eq(sentences.len(), 2)?
}

test "ngram tokenizer" {
    let bigrams = ngrams("the quick brown fox", 2)
    assert_eq(bigrams.len(), 3)?
    assert_eq(bigrams[0], "the quick")?
}

test "char ngrams" {
    let trigrams = char_ngrams("hello", 3)
    assert_eq(trigrams.len(), 3)?
    assert_eq(trigrams[0], "hel")?
    assert_eq(trigrams[1], "ell")?
    assert_eq(trigrams[2], "llo")?
}

test "social media tokenizer" {
    let tokenizer = SocialMediaTokenizer.new()
    let tokens = tokenizer.tokenize("Check out @user's post! #awesome https://example.com")
    
    let mentions = tokens.iter().filter(|t| matches!(t.token_type, TokenType.Mention)).count()
    let hashtags = tokens.iter().filter(|t| matches!(t.token_type, TokenType.Hashtag)).count()
    let urls = tokens.iter().filter(|t| matches!(t.token_type, TokenType.Url)).count()
    
    assert_eq(mentions, 1)?
    assert_eq(hashtags, 1)?
    assert_eq(urls, 1)?
}

test "reduce lengthening" {
    let result = reduce_repeated_chars("sooooo goooood")
    assert_eq(result, "soo good")?
}

test "whitespace tokenizer" {
    let tokenizer = WhitespaceTokenizer.new()
    let tokens = tokenizer.tokenize("hello   world")
    assert_eq(tokens.len(), 2)?
}

test "classify token" {
    assert_eq(classify_token("hello"), TokenType.Word)?
    assert_eq(classify_token("123"), TokenType.Number)?
    assert_eq(classify_token("@user"), TokenType.Mention)?
    assert_eq(classify_token("#tag"), TokenType.Hashtag)?
}
