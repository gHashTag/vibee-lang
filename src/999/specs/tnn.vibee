name: ternary_neural_networks
version: "1.0.0"
language: 999
module: tnn
description: |
  TERNARY NEURAL NETWORKS (TNN)
  
  TTQ/TRQ квантизация для 16x сжатия моделей.
  Научные работы: TTQ (ICLR 2017), TRQ (AAAI 2020).
  
  Creation Pattern:
    FP32 Weights → Ternary Quantization → {-α, 0, +α} Weights
       Source    →      Transformer      →       Result

creation_pattern:
  source: FP32NeuralNetwork
  transformer: TernaryQuantization  
  result: TernaryNeuralNetwork
  iteration: until_accuracy_threshold

types:
  TritTensor:
    shape: [u64]
    data: [Trit64]
    stride: [u64]

  F32Tensor:
    shape: [u64]
    data: [f32]
    stride: [u64]

  TTQParams:
    w_p: f32          # positive scale
    w_n: f32          # negative scale
    threshold: f32

  TRQParams:
    stem: TTQParams
    residual: TTQParams

  TritLinear:
    weights: TritTensor
    bias: F32Tensor?
    params: TTQParams
    in_features: u64
    out_features: u64

  TritNet:
    layers: [TritLinear]

  QuantizationStats:
    sparsity: f32         # % of zeros
    compression_ratio: f32
    accuracy_loss: f32

transformers:
  - name: ttq_quantize
    type: pure
    input: {weights: F32Tensor, params: TTQParams}
    output: TritTensor
    rule: |
      for each weight w in weights:
        if w > params.threshold:
          trit = PLUS
        elif w < -params.threshold:
          trit = MINUS
        else:
          trit = ZERO
      return packed_trits
    patterns: [precomputation]
    expected_speedup: 16.0

  - name: ttq_dequantize
    type: pure
    input: {trits: TritTensor, params: TTQParams}
    output: F32Tensor
    rule: |
      for each trit in trits:
        match trit:
          PLUS -> params.w_p
          MINUS -> -params.w_n
          ZERO -> 0.0
    patterns: [precomputation]

  - name: trq_quantize
    type: pure
    input: {weights: F32Tensor, params: TRQParams}
    output: {stem: TritTensor, residual: TritTensor}
    rule: |
      stem = ttq_quantize(weights, params.stem)
      stem_f32 = ttq_dequantize(stem, params.stem)
      residual_f32 = weights - stem_f32
      residual = ttq_quantize(residual_f32, params.residual)
      return {stem, residual}
    patterns: [algebraic_reorg]
    expected_speedup: 1.5

  - name: trit_linear_forward
    type: pure
    input: {layer: TritLinear, input: F32Tensor}
    output: F32Tensor
    rule: |
      # SIMD-optimized dot product with ternary weights
      for batch in 0..input.shape[0]:
        for out_idx in 0..layer.out_features:
          sum = 0.0
          for in_idx in 0..layer.in_features:
            trit = layer.weights[out_idx, in_idx]
            x = input[batch, in_idx]
            match trit:
              PLUS -> sum += x * layer.params.w_p
              MINUS -> sum -= x * layer.params.w_n
          if layer.bias:
            sum += layer.bias[out_idx]
          output[batch, out_idx] = sum
    patterns: [precomputation, divide_and_conquer]
    expected_speedup: 8.0

  - name: ttq_backward_ste
    type: pure
    input: {grad: F32Tensor, trits: TritTensor, params: TTQParams}
    output: F32Tensor
    rule: |
      # Straight-Through Estimator
      for each (g, t) in zip(grad, trits):
        if t != ZERO:
          grad_out = g
        else:
          grad_out = 0.0  # sparsity gradient
    patterns: [probabilistic]

behaviors:
  - name: ternary_quantization_compression
    given: FP32 weights tensor
    when: Applying TTQ quantization
    then: Achieves 16x compression ratio
    test_cases:
      - name: linear_layer_compression
        input: {weights_size: "1MB", precision: "fp32"}
        expected: {compressed_size: "62.5KB", ratio: 16.0}
      - name: model_compression
        input: {model_size: "100MB"}
        expected: {compressed_size: "<7MB"}

  - name: ternary_forward_accuracy
    given: Quantized ternary layer
    when: Running forward pass
    then: Produces valid output with bounded error
    test_cases:
      - name: mnist_accuracy
        input: {dataset: "MNIST", epochs: 10}
        expected: {accuracy_min: 0.95}
      - name: imagenet_top5
        input: {dataset: "ImageNet", epochs: 90}
        expected: {top5_accuracy_min: 0.80}

  - name: trq_residual_improvement
    given: TTQ quantized weights
    when: Adding TRQ residual correction
    then: Reduces quantization error
    test_cases:
      - name: residual_error_reduction
        input: {ttq_error: 0.1}
        expected: {trq_error_max: 0.05}

  - name: simd_inference_speedup
    given: Ternary layer with SIMD support
    when: Running inference
    then: Achieves 8x+ speedup vs FP32
    test_cases:
      - name: avx512_speedup
        input: {batch_size: 32, layer_size: 1024}
        expected: {speedup_min: 8.0}

functions:
  - name: ttq_quantize
    params: {weights: F32Tensor, params: TTQParams}
    returns: TritTensor
    
  - name: ttq_dequantize
    params: {trits: TritTensor, params: TTQParams}
    returns: F32Tensor
    
  - name: trq_quantize
    params: {weights: F32Tensor, params: TRQParams}
    returns: {stem: TritTensor, residual: TritTensor}
    
  - name: trq_dequantize
    params: {stem: TritTensor, residual: TritTensor, params: TRQParams}
    returns: F32Tensor
    
  - name: trit_linear_new
    params: {in_features: u64, out_features: u64}
    returns: TritLinear
    
  - name: trit_linear_forward
    params: {layer: TritLinear, input: F32Tensor}
    returns: F32Tensor
    
  - name: trit_net_new
    params: {architecture: [u64]}
    returns: TritNet
    
  - name: trit_net_forward
    params: {net: TritNet, x: F32Tensor}
    returns: F32Tensor

data:
  default_ttq_params:
    w_p: 1.0
    w_n: 1.0
    threshold: 0.05

  supported_architectures:
    - name: "TritMLP"
      layers: [784, 256, 128, 10]
      accuracy: 0.98
    - name: "TritCNN"
      layers: ["conv32", "conv64", "fc128", "fc10"]
      accuracy: 0.95

test_generation:
  boundary: true
  property: true
  stress: [100, 1000, 10000]
  coverage: 90
