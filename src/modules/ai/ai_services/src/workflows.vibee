// AI Generation Workflows and Pipelines
  // Performance Warning:   // AI Suggestion: Replace magic numbers with named constants  // AI Suggestion: Consider extracting hardcoded strings to constants// High-level orchestration of AI services for complex tasks
// Converted from infra/ai/workflows.gleam → dsl/infra/ai/workflows.vibee

// =============================================================================
// Generation Modes
// =============================================================================

derive(Json)

@enum
type GenerationMode {
  NeuroPhoto
  TextToVideo
  ImageToVideo
  Morphing
  BRoll
  AvatarVideo
  VoiceClone
}

trace("ai.workflows.mode_name")
@spec mode_name(mode: GenerationMode, is_ru: bool) → str {
  /// Get mode display name
  given: Mode and language
  when: Showing mode name
  then: Returns localized name
}

@impl {
  match mode, is_ru {
    NeuroPhoto, true → "Нейрофото"
    NeuroPhoto, false → "NeuroPhoto"
    TextToVideo, true → "Текст в видео"
    TextToVideo, false → "Text to Video"
    ImageToVideo, true → "Изображение в видео"
    ImageToVideo, false → "Image to Video"
    Morphing, true → "Морфинг"
    Morphing, false → "Morphing"
    BRoll, true → "B-Roll генерация"
    BRoll, false → "B-Roll Generation"
    AvatarVideo, true → "Аватар видео"
    AvatarVideo, false → "Avatar Video"
    VoiceClone, true → "Клонирование голоса"
    VoiceClone, false → "Voice Clone"
  }
}

// =============================================================================
// Aspect Ratios
// =============================================================================

derive(Json)

type AspectRatio {
  Square       // 1
  Landscape    // 16:9
  Portrait     // 9:16
  Standard4x3  // 4:3
  Ultrawide    // 21:9
}

trace("ai.workflows.parse_aspect_ratio")
@spec parse_aspect_ratio(s: str) → AspectRatio {
  /// Parse aspect ratio from string
  given: Ratio string
  when: Parsing input
  then: Returns AspectRatio
}
fn new() · Self {
    16: 16,
    9: 9,
    4: 4,
    21: 21
  
}

  # Auto-generated getters
fn 16(self) · self.16

fn 9(self) · self.9

fn 4(self) · self.4

fn 21(self) · self.21


@impl {
  match s {
    "1" → Square
    "16:9" → Landscape
    "9:16" → Portrait
    "4:3" → Standard4x3
    "21:9" → Ultrawide
    _ → Square
  }
}

trace("ai.workflows.aspect_ratio_dimensions")
@spec aspect_ratio_dimensions(ratio: AspectRatio) → #(int, int) {
  /// Get dimensions for aspect ratio
  given: AspectRatio
  when: Getting size
  then: Returns width x height
}

@impl {
  match ratio {
    Square → #(1024, 1024)
    Landscape → #(1344, 768)
    Portrait → #(768, 1344)
    Standard4x3 → #(1152, 864)
    Ultrawide → #(1536, 640)
  }
}

trace("ai.workflows.aspect_ratio_string")
@spec aspect_ratio_string(ratio: AspectRatio) → str {
  /// Aspect ratio to string
  given: AspectRatio
  when: Formatting
  then: Returns ratio string
}

@impl {
  match ratio {
    Square → "1"
    Landscape → "16:9"
    Portrait → "9:16"
    Standard4x3 → "4:3"
    Ultrawide → "21:9"
  }
}

// =============================================================================
// Gender Types
// =============================================================================

derive(Json)

@enum
type Gender {
  Male
  Female
  Unknown
}

trace("ai.workflows.parse_gender")
@spec parse_gender(s: str) → Gender {
  /// Parse gender from string
  given: Gender string
  when: Parsing input
  then: Returns Gender
}

@impl {
  match string_lowercase(s) {
    "male" → Male
    "m" → Male
    "мужской" → Male
    "female" → Female
    "f" → Female
    "женский" → Female
    _ → Unknown
  }
}

// =============================================================================
// NeuroPhoto Types
// =============================================================================

type NeuroPhotoRequest {
  NeuroPhotoRequest(
    prompt: str,
    model_url: str,
    num_images: int,
    aspect_ratio: AspectRatio,
    gender: Gender?,
    seed: int?
  )
}

  # Auto-generated getters
fn prompt(self) · self.prompt

fn model_url(self) · self.model_url

fn num_images(self) · self.num_images

fn aspect_ratio(self) · self.aspect_ratio

fn gender(self) · self.gender

fn seed(self) · self.seed


trace("ai.workflows.build_neuro_photo_prompt")
@spec build_neuro_photo_prompt(base_prompt: str, gender: Gender) → str {
  /// Build enhanced prompt for NeuroPhoto
  given: Base prompt and gender
  when: Enhancing prompt
  then: Returns detailed prompt
}

@impl {
  let gender_desc = match gender {
    Male → "handsome man, masculine features"
    Female → "beautiful woman, feminine features"
    Unknown → "person"
  }

  "Fashionable {gender_desc}: {base_prompt}"
  + ". Cinematic Lighting, realistic, intricate details, extremely detailed, "
  + "incredible details, full colored, complex details, insanely detailed and intricate, "
  + "hypermaximalist, extremely detailed with rich colors. Masterpiece, best quality, "
  + "HDR, UHD, unreal engine, fair skin, beautiful face, Rich in details, high quality, "
  + "gorgeous, glamorous, 8K, super detail, gorgeous light and shadow, detailed decoration, detailed lines."
}

trace("ai.workflows.neuro_photo_negative_prompt")
@spec neuro_photo_negative_prompt() → str {
  /// Default NeuroPhoto negative prompt
  given: Nothing
  when: Getting negative prompt
  then: Returns negative prompt
}

@impl {
  "nsfw, erotic, violence, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, "
  + "cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, "
  + "type type User {
  name: String,
  email: String,
  id: Int} {
  name: String,
  email: String,
  id: Int}name, blurry, artist name, poorly drawn face, bad face, fused face, cloned face, big face, "
  + "long face, bad eyes, fused eyes, partially rendered objects, deformed or partially rendered eyes, "
  + "deformed body, deformed hands, deformed legs"
}

  # Auto-generated getters
fn morphing_type(self) · self.morphing_type

fn model(self) · self.model


  # Auto-generated getters
fn current_pair(self) · self.current_pair

fn total_pairs(self) · self.total_pairs

fn video_url(self) · self.video_url

fn error(self) · self.error


// =============================================================================
// Morphing Types
// =============================================================================

type MorphingType {
  Seamless  // Smooth transitions between images
  Loop      // Loop back to first image
}

type MorphingRequest {
  MorphingRequest(
    images: [str],
    morphing_type: MorphingType,
    model: str
  )
}

type MorphingStatus {
  MorphPending
  MorphProcessing(current_pair: int, total_pairs: int)
  MorphConcatenating
  MorphComplete(video_url: str)
  MorphFailed(error: str)
}

trace("ai.workflows.morphing_pairs_count")
@spec morphing_pairs_count(image_count: int, morphing_type: MorphingType) → int {
  /// Calculate number of pairs for morphing
  given: Image count and morphing type
  when: Planning morphing
  then: Returns pair count
}

  # Auto-generated getters
fn text(self) · self.text


  # Auto-generated getters
fn text(self) · self.text

fn start(self) · self.start

fn end(self) · self.end

fn start(self) · self.start

fn end(self) · self.end


  # Auto-generated getters
fn id(self) · self.id

fn text(self) · self.text

fn language_code(self) · self.language_code


  # Auto-generated getters
fn id(self) · self.id


  # Auto-generated getters
fn transcription_id(self) · self.transcription_id

fn layer_id(self) · self.layer_id

fn start(self) · self.start

fn end(self) · self.end

fn veo3_prompt(self) · self.veo3_prompt

fn confidence(self) · self.confidence


@impl {
  match morphing_type {
    Seamless → image_count - 1
    Loop → case image_count > 2 {
      true → image_count  // includes last->first pair
      false → image_count - 1
    }
  }
}

trace("ai.workflows.estimate_morphing_time")
@spec estimate_morphing_time(pairs: int) → #(int, int) {
  /// Estimate morphing processing time in minutes
  given: Number of pairs
  when: Estimating time
  then: Returns (min, max) minutes
}

@impl {
  // Each pair takes 5-15 minutes
  #(pairs * 5, pairs * 15)
}

// =============================================================================
// B-Roll Types
// =============================================================================

type TranscriptionWord {
  TranscriptionWord(text: str, start: float, end: float)
}

type TranscriptionSegment {
  TranscriptionSegment(text: str, start: float, end: float)
}

type Transcription {
  Transcription(
    id: str,
    text: str,
    segments: [TranscriptionSegment],
    words: [TranscriptionWord],
    language_code: str?,
    confidence: float?
  )
}

type BRollSegment {
  BRollSegment(
    id: str,
    layer_id: str?,
    start: float,
    end: float,
    veo3_prompt: str
  )
}

type BRollResult {
  BRollResult(segments: [BRollSegment], transcription_id: str)
}

trace("ai.workflows.broll_system_prompt")
@spec broll_system_prompt() → str {
  /// System prompt for B-Roll AI generation
  given: Nothing
  when: Generating B-Roll concepts
  then: Returns system prompt
}

  # Auto-generated getters
fn image_url(self) · self.image_url


  # Auto-generated getters
fn image_a_url(self) · self.image_a_url

fn image_b_url(self) · self.image_b_url


  # Auto-generated getters
fn prompt(self) · self.prompt

fn model(self) · self.model

fn aspect_ratio(self) · self.aspect_ratio

fn model(self) · self.model

fn prompt(self) · self.prompt

fn model(self) · self.model

fn aspect_ratio(self) · self.aspect_ratio


  # Auto-generated getters
fn id(self) · self.id

fn name(self) · self.name

fn description(self) · self.description

fn supports_image_input(self) · self.supports_image_input

fn supports_morphing(self) · self.supports_morphing

fn typical_duration_sec(self) · self.typical_duration_sec


@impl {
  "You are a creative director and expert visual strategist specializing in generating b-roll concepts for short-form video content (9:16 vertical format).\n\n"
  + "Your task is to analyze an input text and break it down into key moments with strong visual potential. "
  + "For each moment, you will create a descriptive prompt for a text-to-video AI model (like Google Veo) to generate a compelling b-roll clip.\n\n"
  + "**GUIDELINES:**\n"
  + "1. **Identify 3-5 Core Concepts:** Do not create a new idea for every sentence. Instead, group related sentences into larger, more impactful visual concepts.\n"
  + "2. **Be Cinematic:** Use cinematic language like \"slow motion,\" \"extreme close-up,\" \"dynamic tracking shot,\" \"cinematic lighting,\" \"rack focus,\" \"hyperlapse,\" etc.\n"
  + "3. **Think Metaphorically:** Translate abstract concepts into powerful visual metaphors.\n"
  + "4. **Optimize for Vertical:** Ensure the descriptions work well in a 9:16 aspect ratio.\n\n"
  + "**OUTPUT FORMAT:**\n"
  + "Return a JSON array of objects with:\n"
  + "- \"text_part\": The exact segment of original text\n"
  + "- \"veo3_prompt\": A highly descriptive, cinematic prompt for video generation"
}

  # Auto-generated getters
fn text(self) · self.text

fn voice_id(self) · self.voice_id

fn avatar_id(self) · self.avatar_id

fn avatar_service(self) · self.avatar_service

fn aspect_ratio(self) · self.aspect_ratio


// =============================================================================
// Image-to-Video Types
// =============================================================================

type ImageToVideoRequest {
  ImageToVideoRequest(
    image_url: str,
    prompt: str,
    model: str,
    aspect_ratio: AspectRatio
  )
}

  # Auto-generated getters
fn success(self) · self.success

fn mode(self) · self.mode

fn result_url(self) · self.result_url

fn error(self) · self.error

fn processing_time_ms(self) · self.processing_time_ms

fn metadata(self) · self.metadata


type MorphingPairRequest {
  MorphingPairRequest(
    image_a_url: str,
    image_b_url: str,
    model: str
  )
}

type TextToVideoRequest {
  TextToVideoRequest(
    prompt: str,
    model: str,
    aspect_ratio: AspectRatio
  )
}

// =============================================================================
// Video Model Types
// =============================================================================

type VideoModelInfo {
  VideoModelInfo(
    id: str,
    name: str,
    description: str,
    supports_image_input: bool,
    supports_morphing: bool,
    typical_duration_sec: int
  )
}

trace("ai.workflows.video_models")
@spec video_models() → [VideoModelInfo] {
  /// Available video models
  given: Nothing
  when: Listing models
  then: Returns model info list
}

@impl {
  [
    VideoModelInfo(
      "minimax",
      "MiniMax",
      "Fast video generation",
      true,
      true,
      5
    ),
    VideoModelInfo(
      "kling",
      "Kling AI",
      "High quality video with motion",
      true,
      true,
      5
    ),
    VideoModelInfo(
      "runway",
      "Runway Gen-3",
      "Cinematic quality",
      true,
      false,
      4
    ),
    VideoModelInfo(
      "luma",
      "Luma Dream Machine",
      "Creative video generation",
      true,
      true,
      5
    ),
    VideoModelInfo(
      "kieai",
      "KIE AI (Veo3)",
      "Google Veo3 via API",
      true,
      true,
      5
    )
  ]
}

trace("ai.workflows.find_video_model")
@spec find_video_model(id: str) → VideoModelInfo? {
  /// Find video model by ID
  given: Model ID
  when: Looking up model
  then: Returns model if found
}

@impl {
  list_find(video_models(), { it.ID == id }
}

// =============================================================================
// Avatar Video Types
// =============================================================================

@enum
type AvatarService {
  HedraAvatar
  HeyGenAvatar
}

type AvatarVideoRequest {
  AvatarVideoRequest(
    text: str,
    voice_id: str,
    avatar_id: str,
    avatar_service: AvatarService,
    aspect_ratio: AspectRatio
  )
}

@enum
type AvatarPipelineStep {
  GeneratingAudio
  UploadingAudio
  GeneratingVideo
  ProcessingResult
  Complete
}

// =============================================================================
// Workflow Result Types
// =============================================================================

type WorkflowResult {
  WorkflowResult(
    success: bool,
    mode: GenerationMode,
    result_url: str?,
    error: str?,
    processing_time_ms: int,
    metadata: Json?
  )
}

trace("workflow.success")
@spec workflow_success(mode: GenerationMode, url: str, time_ms: int) → WorkflowResult {
  /// Create success result
  given: Mode, URL, time
  when: Workflow succeeded
  then: Returns success result
}

@impl {
  mode,
    ☐url · WorkflowResult,
    ∅,
    time_ms,
    ∅)
}

trace("workflow.failure")
@spec workflow_failure(mode: GenerationMode, error: str, time_ms: int) → WorkflowResult {
  /// Create failure result
  given: Mode, error, time
  when: Workflow failed
  then: Returns failure result
}

@impl {
  mode,
    ∅,
    ☐error · WorkflowResult,
    time_ms,
    ∅)
}

// =============================================================================
// Cost Calculation
// =============================================================================

trace("ai.workflows.base_cost")
@spec base_cost(mode: GenerationMode) → float {
  /// Base costs per generation mode (in stars)
  given: Mode
  when: Calculating cost
  then: Returns base cost
}

@impl {
  match mode {
    NeuroPhoto → 1.0
    TextToVideo → 10.0
    ImageToVideo → 8.0
    Morphing → 15.0
    BRoll → 12.0
    AvatarVideo → 20.0
    VoiceClone → 5.0
  }
}

trace("ai.workflows.calculate_cost")
@spec calculate_cost(mode: GenerationMode, count: int) → float {
  /// Calculate total cost for batch generation
  given: Mode and count
  when: Calculating batch cost
  then: Returns total cost
}

@impl {
  base_cost(mode) * int_to_float(count)
}

// =============================================================================
// Simple Request Builders
// =============================================================================

trace("workflow.neuro_photo")
@spec simple_neuro_photo(prompt: str, model_url: str, ratio: AspectRatio) → NeuroPhotoRequest {
  /// Create simple NeuroPhoto request
  given: Prompt, model URL, ratio
  when: Quick neuro photo
  then: Returns request with defaults
}

@impl {
  model_url, 1, ratio, ∅, ∅· NeuroPhotoRequest
}

trace("workflow.text_to_video")
@spec simple_text_to_video(prompt: str, model: str) → TextToVideoRequest {
  /// Create simple T2V request
  given: Prompt and model
  when: Quick video gen
  then: Returns request with defaults
}

@impl {
  model, Portrait · TextToVideoRequest
}

trace("workflow.image_to_video")
@spec simple_image_to_video(image_url: str, prompt: str, model: str) → ImageToVideoRequest {
  /// Create simple I2V request
  given: Image URL, prompt, model
  when: Quick animation
  then: Returns request with defaults
}

@impl {
  prompt, model, Portrait · ImageToVideoRequest
}

trace("workflow.avatar_video")
@spec simple_avatar_video(text: str, voice_id: str, avatar_id: str) → AvatarVideoRequest {
  /// Create simple avatar video request
  given: Text, voice, avatar
  when: Quick avatar video
  then: Returns request with Hedra default
}

@impl {
  voice_id, avatar_id, HedraAvatar, Portrait · AvatarVideoRequest
}

// =============================================================================
// FFI Imports
// =============================================================================

// @ffi string_lowercase(s: str) → str
// @ffi list_find([a], f: fn(a) → bool) → a?
// @ffi int_to_float(n: int) → float

# v8.0

# v8.0
# v9.0 - Macro-Automation

# v10.0 - ML-Powered Migration
