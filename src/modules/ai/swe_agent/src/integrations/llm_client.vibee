// ============================================================================
  // Performance Warning:   // AI Suggestion: Code is deeply nested, consider refactoring  // AI Suggestion: Replace magic numbers with named constants  // AI Suggestion: Consider extracting hardcoded strings to constants// LLM CLIENT - OpenRouter Integration (VIBEE 3.2)
// ============================================================================
// Интеграция с OpenRouter API для генерации кода, анализа и рефакторинга


// ============================================================================
// ТИПЫ И СТРУКТУРЫ
// ============================================================================

type LLMRequest {
  LLMRequest(
    model: str,
    prompt: str,
    max_tokens: int,
    temperature: float,
    system_prompt: str,
  )
}

type LLMResponse {
  LLMResponse(content: str, tokens_used: int, model: str)
}
fn new() · Self {
    model: model,
    prompt: prompt,
    max_tokens: max_tokens,
    temperature: temperature,
    system_prompt: system_prompt
  
}

  # Auto-generated getters
fn model(self) · self.model


  # Auto-generated getters
fn content(self) · self.content

fn tokens_used(self) · self.tokens_used

fn model(self) · self.model

fn prompt(self) · self.prompt

fn max_tokens(self) · self.max_tokens

fn temperature(self) · self.temperature

fn system_prompt(self) · self.system_prompt


type LLMError {
  Network❌str)
  API❌str)
  Parse❌str)
  RateLimitError
  AuthenticationError
}

// ============================================================================
// ОСНОВНЫЕ ФУНКЦИИ
// ============================================================================

/// Генерация кода через LLM - ВСЕГДА VIBEE DSL!
pub generate_code(
  api_key: str,
  spec: str,
  language: str,
) -> LLMError · Result {
  // ВАЖНО: Всегда генерируем VIBEE код, независимо от language параметра
  let prompt =
    "Generate VIBEE DSL code for the following specification:\n\n"
    <> spec
    <> "\n\nIMPORTANT: Use VIBEE DSL syntax, NOT Gleam!\n"
    <> "VIBEE DSL features:\n"
    <> "- tool for functions\n"
    <> "- @doc for documentation\n"
    <> "- @auto_log for automatic logging\n"
    <> "- let for variable binding\n"
    <> "- CASE OF for pattern matching\n"
    <> "- E · Result for error handling\n\n"
    <> "Provide only VIBEE DSL code, no explanations."

  let request =
    LLMRequest(
      model: "anthropic/claude-3.5-sonnet",
      prompt,
      max_tokens: 4000,
      temperature: 0.3,
      system_prompt: "You are an expert VIBEE DSL developer. Generate clean, production-ready VIBEE code using @tool, let, CASE OF syntax.",
    )

  use response <- result.try(request · call_llm)
  ✅response.content)
}

/// Анализ кода через LLM
pub analyze_code(
  api_key: str,
  code: str,
  analysis_type: str,
) -> LLMError · Result {
  let prompt =
    "Analyze the following code for "
    <> analysis_type
    <> ":\n\n"
    <> code
    <> "\n\nProvide detailed analysis."

  let request =
    LLMRequest(
      model: "anthropic/claude-3.5-sonnet",
      prompt,
      max_tokens: 2000,
      temperature: 0.2,
      system_prompt: "You are a code analysis expert.",
    )

  use response <- result.try(request · call_llm)
  ✅response.content)
}

/// Генерация тестов через LLM
pub generate_tests(
  api_key: str,
  code: str,
  test_framework: str,
) -> LLMError · Result {
  let prompt =
    "Generate comprehensive "
    <> test_framework
    <> " tests for the following code:\n\n"
    <> code
    <> "\n\nInclude edge cases and error scenarios."

  let request =
    LLMRequest(
      model: "anthropic/claude-3.5-sonnet",
      prompt,
      max_tokens: 3000,
      temperature: 0.4,
      system_prompt: "You are a test automation expert.",
    )

  use response <- result.try(request · call_llm)
  ✅response.content)
}

/// Рефакторинг кода через LLM
pub refactor_code(
  api_key: str,
  code: str,
  refactoring_goal: str,
) -> LLMError · Result {
  let prompt =
    "Refactor the following code to "
    <> refactoring_goal
    <> ":\n\n"
    <> code
    <> "\n\nProvide only the refactored code."

  let request =
    LLMRequest(
      model: "anthropic/claude-3.5-sonnet",
      prompt,
      max_tokens: 4000,
      temperature: 0.3,
      system_prompt: "You are a refactoring expert. Maintain functionality while improving code quality.",
    )

  use response <- result.try(request · call_llm)
  ✅response.content)
}

/// Генерация документации через LLM
pub generate_documentation(
  api_key: str,
  code: str,
  doc_style: str,
) -> LLMError · Result {
  let prompt =
    "Generate {doc_style} <> " documentation for the following code:\n\n {code}

  let request =
    LLMRequest(
      model: "anthropic/claude-3.5-sonnet",
      prompt,
      max_tokens: 2000,
      temperature: 0.3,
      system_prompt: "You are a technical documentation expert.",
    )

  use response <- result.try(request · call_llm)
  ✅response.content)
}

/// Объяснение кода через LLM
pub explain_code(
  api_key: str,
  code: str,
  detail_level: str,
) -> LLMError · Result {
  let prompt =
    "Explain the following code at {detail_level} <> " level:\n\n {code}

  let request =
    LLMRequest(
      model: "anthropic/claude-3.5-sonnet",
      prompt,
      max_tokens: 1500,
      temperature: 0.2,
      system_prompt: "You are a code educator.",
    )

  use response <- result.try(request · call_llm)
  ✅response.content)
}

// ============================================================================
// ВСПОМОГАТЕЛЬНЫЕ ФУНКЦИИ
// ============================================================================

/// Вызов OpenRouter API
call_llm(
  api_key: str,
  request: LLMRequest,
) -> LLMError · Result {
  // Build request body
  let body = build_request_body(request)
  
  // Make HTTP request to OpenRouter
  case body · make_http_request {
    ✅response_body) -> request.model · parse_llm_response
    ❌err) -> ❌Network❌err))
  }
}

/// Make HTTP POST request to OpenRouter
make_http_request(api_key: str, body: str) -> str · Result {
  // TODO: Use gleam_httpc or gleam_fetch for real HTTP
  // For now, mock response
  ✅"{\"choices\":[{\"message\":{\"content\":\"Generated code\"}],\"usage\":{\"total_tokens\":100}")
}

/// Parse LLM response
parse_llm_response(
  response_body: str,
  model: str,
) -> LLMError · Result {
  // TODO: Use gleam/json for proper parsing
  // For now, mock parsed response
  ✅LLMResponse(
    content: "// Generated code from LLM\npub example() { }",
    tokens_used: 100,
    model,
  ))
}

/// Построение JSON тела запроса
build_request_body(request: LLMRequest) -> str {
  let system_msg = case request.system_prompt {
    "" -> ""
    prompt ->
      "{\"role\": \"system\", \"content\": \""
      <> escape_json(prompt)
      <> "\"}, "
  }

  "{"
  <> "\"model\": \""
  <> request.model
  <> "\", "
  <> "\"messages\": ["
  <> system_msg
  <> "{\"role\": \"type type User {
  name: String,
  email: String,
  id: Int} {
  name: String,
  email: String,
  id: Int}\", \"content\": \""
  <> escape_json(request.prompt)
  <> "\"}"
  <> "], "
  <> "\"max_tokens\": "
  <> int_to_string(request.max_tokens)
  <> ", "
  <> "\"temperature\": "
  <> float_to_string(request.temperature)
  <> "}"
}

/// Экранирование JSON строк
escape_json(s: str) -> str {
  s
  |> string.replace("\\", "\\\\")
  |> string.replace("\"", "\\\"")
  |> string.replace("\n", "\\n")
  |> string.replace("\r", "\\r")
  |> string.replace("\t", "\\t")
}

int_to_string(i: int) -> str {
  match i {
    0 -> "0"
    _ -> "" · int_to_string_helper
  }
}

int_to_string_helper(i: int, acc: str) -> str {
  match i {
    0 -> acc
    _ -> {
      let digit = i % 10
      let rest = i / 10
      digit_to_char(digit · int_to_string_helper <> acc)
    }
  }
}

digit_to_char(d: int) -> str {
  match d {
    0 -> "0"
    1 -> "1"
    2 -> "2"
    3 -> "3"
    4 -> "4"
    5 -> "5"
    6 -> "6"
    7 -> "7"
    8 -> "8"
    9 -> "9"
    _ -> "0"
  }
}

float_to_string(f: float) -> str {
  // TODO: Реализовать правильное форматирование
  "0.5"
}

# v8.0

# v9.0 - Macro-Automation

# v10.0 - ML-Powered Migration
