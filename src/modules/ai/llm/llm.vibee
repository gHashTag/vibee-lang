/// LLM configuration
type LLMConfig = {
  provider: String,      // "openai", "anthropic", "local"
  model: String,
  api_key: String,
  temperature: Float,
  max_tokens: Int,
  base_url: String
}

/// Chat message
type ChatMessage = {
  role: String,          // "user", "assistant", "system"
  content: String,
  name: String
}

/// Completion result
type CompletionResult = {
  text: String,
  usage: TokenUsage,
  finish_reason: String,
  model: String
}

/// Token usage
type TokenUsage = {
  prompt_tokens: Int,
  completion_tokens: Int,
  total_tokens: Int
}

/// Send completion request
/// @param config - LLM configuration
/// @param messages - List of chat messages
/// @returns Completion result
fn complete(config: LLMConfig, messages: List<ChatMessage>) -> Result<CompletionResult, String>

/// Stream completion
/// @param config - LLM configuration
/// @param messages - List of chat messages
/// @param callback - Function to call for each chunk
fn stream_complete(config: LLMConfig, messages: List<ChatMessage>, callback: fn(String) -> Nil) -> Result<Nil, String>

/// Get embeddings for text
/// @param config - LLM configuration
/// @param texts - List of texts to embed
/// @returns List of embedding vectors
fn embed(config: LLMConfig, texts: List<String>) -> Result<List<List<Float>>, String>

/// Count tokens in text
/// @param config - LLM configuration
/// @param text - Input text
/// @returns Token count
fn count_tokens(config: LLMConfig, text: String) -> Result<Int, String>

/// Create a new chat session
/// @param config - LLM configuration
/// @returns Chat session ID
fn create_chat(config: LLMConfig) -> Result<String, String>

event llm.response(CompletionResult)
event llm.error(String, String)