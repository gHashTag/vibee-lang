// AI Tools - MCP Tools for AI service integrations
  // Performance Warning:   // AI Suggestion: Replace magic numbers with named constants  // AI Suggestion: Consider extracting hardcoded strings to constants// Converted from mcp/ai_tools.gleam → ai_tools.vibee
// Covers: ElevenLabs, Hedra, BFL, Kling, HeyGen, KIE.ai, FAL.ai, Replicate, OpenAI, Video editing

// =============================================================================
// ElevenLabs TTS Tools
// =============================================================================

tool ai_elevenlabs_tts {
  /// Text-to-speech with ElevenLabs
  description: "Convert text to speech using ElevenLabs. Returns audio URL."

  @param text: str {
    description: "Text to convert to speech"
    required: true
  }

  @param voice_id: str? {
    description: "Voice ID to use (optional, default: Sarah)"
  }

  @param model: str? {
    enum: ["eleven_multilingual_v2", "eleven_turbo_v2", "eleven_monolingual_v1"]
    description: "Model to use (default: eleven_multilingual_v2)"
  }

  @param stability: float? {
    description: "Voice stability (0.0-1.0)"
  }

  @param similarity_boost: float? {
    description: "Similarity boost (0.0-1.0)"
  }
}

tool ai_elevenlabs_list_voices {
  /// list available ElevenLabs voices
  description: "list all available voices from ElevenLabs"
}

tool ai_elevenlabs_clone_voice {
  /// Clone voice from audio
  description: "Clone a voice from audio using ElevenLabs Instant Voice Cloning."

  @param name: str {
    description: "Name for the cloned voice"
    required: true
  }

  @param audio_url: str {
    description: "URL of the audio file to clone voice from (mp3, wav, ogg)"
    required: true
  }

  @param description: str? {
    description: "Description for the cloned voice"
  }
}

tool ai_elevenlabs_delete_voice {
  /// Delete a cloned voice
  description: "Delete a cloned voice from ElevenLabs by voice_id"

  @param voice_id: str {
    description: "Voice ID to delete"
    required: true
  }
}

// =============================================================================
// Hedra Avatar Tools
// =============================================================================

tool ai_hedra_create_avatar {
  /// Create talking avatar video
  description: "Create a talking avatar video using Hedra. Combines audio and image."

  @param audio_url: str {
    description: "URL of the audio file (mp3, wav)"
    required: true
  }

  @param image_url: str {
    description: "URL of the image file (jpg, png)"
    required: true
  }

  @param aspect_ratio: str? {
    enum: ["1", "16:9", "9:16", "4:3", "3:4"]
    description: "Video aspect ratio"
  }
}

tool ai_hedra_get_status {
  /// Check Hedra job status
  description: "Check the status of a Hedra avatar generation job"

  @param job_id: str {
    description: "The job ID returned from create_avatar"
    required: true
  }
}

tool ai_hedra_list_jobs {
  /// list all Hedra jobs
  description: "list all Hedra avatar generation jobs"
}

// =============================================================================
// BFL FLUX Image Generation Tools
// =============================================================================

tool ai_bfl_generate_image {
  /// Generate image with FLUX
  description: "Generate an image using BFL FLUX models (flux-pro-1.1, flux-dev, etc.)"

  @param prompt: str {
    description: "Text prompt describing the image to generate"
    required: true
  }

  @param model: str? {
    enum: ["flux-pro-1.1", "flux-pro", "flux-dev", "flux-pro-1.1-ultra"]
    description: "FLUX model to use (default: flux-pro-1.1)"
  }

  @param width: int? {
    description: "Image width in pixels"
  }

  @param height: int? {
    description: "Image height in pixels"
  }

  @param steps: int? {
    description: "Number of inference steps"
  }
}

tool ai_bfl_get_result {
  /// Get BFL task result
  description: "Get the result of a BFL image generation task"

  @param task_id: str {
    description: "The task ID returned from generate_image"
    required: true
  }
}

// =============================================================================
// Kling AI Video Tools
// =============================================================================

tool ai_kling_create_video {
  /// Generate video from text
  description: "Generate a video from text prompt using Kling AI"

  @param prompt: str {
    description: "Text prompt describing the video to generate"
    required: true
  }

  @param mode: str? {
    enum: ["std", "pro"]
    description: "Generation mode: std (faster) or pro (higher quality)"
  }

  @param duration: str? {
    enum: ["5", "10"]
    description: "Video duration in seconds (default: 5)"
  }

  @param aspect_ratio: str? {
    enum: ["16:9", "9:16", "1"]
    description: "Video aspect ratio"
  }
}

tool ai_kling_image_to_video {
  /// Animate image to video
  description: "Animate an image into a video using Kling AI"

  @param image_url: str {
    description: "URL of the image to animate"
    required: true
  }

  @param prompt: str? {
    description: "Optional prompt to guide the animation"
  }

  @param duration: str? {
    enum: ["5", "10"]
    description: "Video duration in seconds (default: 5)"
  }
}

tool ai_kling_get_task {
  /// Get Kling task status
  description: "Get the status and result of a Kling AI video task"

  @param task_id: str {
    description: "The task ID returned from create_video or image_to_video"
    required: true
  }
}

tool ai_kling_list_tasks {
  /// list all Kling tasks
  description: "list all Kling AI video generation tasks"
}

// =============================================================================
// HeyGen Avatar Tools
// =============================================================================

tool ai_heygen_create_video {
  /// Create video with HeyGen avatar
  description: "Create a video with an AI avatar speaking the provided script using HeyGen"

  @param avatar_id: str {
    description: "HeyGen avatar ID to use"
    required: true
  }

  @param script: str {
    description: "Text script for the avatar to speak"
    required: true
  }

  @param voice_id: str? {
    description: "Voice ID to use (default: avatar's default voice)"
  }

  @param background_url: str? {
    description: "URL of background image"
  }
}

tool ai_heygen_list_avatars {
  /// list available HeyGen avatars
  description: "list all available HeyGen avatars"
}

tool ai_heygen_get_video_status {
  /// Check HeyGen video status
  description: "Check the status of a HeyGen video generation"

  @param video_id: str {
    description: "The video ID returned from create_video"
    required: true
  }
}

tool ai_heygen_list_voices {
  /// list HeyGen voices
  description: "list all available HeyGen voices for avatars"
}

// =============================================================================
// KIE.ai (Veo3) Video Tools
// =============================================================================

tool ai_kieai_create_video {
  /// Generate video with KIE.ai (Veo3)
  description: "Generate video using KIE.ai. Supports veo3_fast, veo3, sora-2, wan-2.5-t2v, runway-aleph"

  @param prompt: str {
    description: "Text prompt describing the video"
    required: true
  }

  @param model: str? {
    enum: ["veo3_fast", "veo3", "sora-2", "wan-2.5-t2v", "runway-aleph"]
    description: "Video model to use (default: veo3_fast)"
  }

  @param aspect_ratio: str? {
    enum: ["9:16", "16:9", "1"]
    description: "Video aspect ratio (default: 9:16)"
  }

  @param seeds: int? {
    description: "Random seed for reproducibility (10000-99999)"
  }
}

tool ai_kieai_get_status {
  /// Check KIE.ai task status
  description: "Check the status of a KIE.ai video generation task."

  @param task_id: str {
    description: "The task ID returned from create_video"
    required: true
  }
}

tool ai_kieai_list_videos {
  /// list KIE.ai videos
  description: "list all KIE.ai video generation records with pagination"

  @param limit: int? {
    description: "Number of records to (default: 20)"
  }

  @param offset: int? {
    description: "Offset for pagination (default: 0)"
  }
}

// =============================================================================
// FAL.ai Tools (NeuroPhoto, Nano Banana, Flux Kontext)
// =============================================================================

tool ai_fal_neuro_photo {
  /// Generate NeuroPhoto with LoRA
  description: "Generate NeuroPhoto image using FAL.ai FLUX with LoRA. Uses NEURO_SAGE trigger word."

  @param prompt: str {
    description: "Text prompt (NEURO_SAGE trigger added automatically)"
    required: true
  }

  @param lora_url: str {
    description: "URL to the LoRA model weights"
    required: true
  }

  @param num_images: int? {
    description: "Number of images to generate (default: 1)"
  }

  @param aspect_ratio: str? {
    enum: ["9:16", "16:9", "1", "4:3"]
    description: "Aspect ratio (default: 9:16)"
  }

  @param seed: int? {
    description: "Random seed for reproducibility"
  }
}

tool ai_fal_nano_banana {
  /// Generate images with Nano Banana Pro
  description: "Generate images using Google Nano Banana Pro via FAL.ai. Fast and cost-effective."

  @param prompt: str {
    description: "Text prompt describing the image"
    required: true
  }

  @param num_images: int? {
    description: "Number of images (default: 1)"
  }

  @param aspect_ratio: str? {
    enum: ["1", "16:9", "9:16", "4:3", "3:4"]
    description: "Aspect ratio (default: 9:16)"
  }

  @param resolution: str? {
    enum: ["1K", "2K", "4K"]
    description: "Output resolution (default: 1K)"
  }
}

tool ai_fal_flux_kontext {
  /// Transform images with Flux Kontext
  description: "Generate or transform images using FLUX Kontext (Pro or Max). Supports image-to-image."

  @param prompt: str {
    description: "Text prompt describing the transformation"
    required: true
  }

  @param input_image_url: str {
    description: "URL of the input image to transform"
    required: true
  }

  @param model_type: str? {
    enum: ["pro", "max"]
    description: "Model version (default: pro)"
  }

  @param aspect_ratio: str? {
    enum: ["1", "16:9", "9:16", "4:3", "3:4", "21:9"]
    description: "Output aspect ratio"
  }
}

// =============================================================================
// Replicate Tools (LipSync, Morphing, FaceSwap, Upscale, LoRA)
// =============================================================================

tool ai_replicate_lipsync {
  /// Generate lip-sync video
  description: "Generate lip-sync video by synchronizing audio to video. Uses Kling LipSync via Replicate."

  @param video_url: str {
    description: "URL of the video to process"
    required: true
  }

  @param audio_url: str {
    description: "URL of the audio to synchronize"
    required: true
  }
}

tool ai_replicate_morphing {
  /// Create morphing video
  description: "Create smooth morphing video transition between two images."

  @param start_image_url: str {
    description: "URL of the starting image"
    required: true
  }

  @param end_image_url: str {
    description: "URL of the ending image"
    required: true
  }

  @param prompt: str? {
    description: "Optional prompt describing the transition style"
  }

  @param duration: str? {
    enum: ["5", "10"]
    description: "Video duration in seconds (default: 5)"
  }
}

tool ai_replicate_faceswap {
  /// Swap faces between images
  description: "Swap face from one image to another. Uses codeplugtech/face-swap."

  @param target_image_url: str {
    description: "URL of target image (face will be replaced)"
    required: true
  }

  @param swap_image_url: str {
    description: "URL of source image (face to swap in)"
    required: true
  }
}

tool ai_replicate_upscale {
  /// Upscale and enhance images
  description: "Upscale images using Clarity Upscaler. Increases resolution while preserving details."

  @param image_url: str {
    description: "URL of the image to upscale"
    required: true
  }

  @param scale: int? {
    enum: [2, 4]
    description: "Upscale factor (default: 2)"
  }

  @param creativity: float? {
    description: "Creativity level 0.0-1.0 (higher = more creative details, default: 0.35)"
  }
}

tool ai_replicate_train_lora {
  /// Train LoRA model
  description: "Train a LoRA model using Replicate Training API."

  @param images_zip_url: str {
    description: "URL to ZIP archive containing training images"
    required: true
  }

  @param trigger_word: str {
    description: "Trigger word to activate the LoRA style"
    required: true
  }

  @param model_name: str {
    description: "Name for the trained model"
    required: true
  }

  @param steps: int? {
    description: "Number of training steps (default: 1000)"
  }
}

// =============================================================================
// OpenAI Tools (Whisper, Vision, Improve Prompt)
// =============================================================================

tool ai_openai_transcribe {
  /// Transcribe audio with Whisper
  description: "Transcribe audio to text using OpenAI Whisper. Supports multiple languages."

  @param audio_url: str {
    description: "URL of the audio file to transcribe"
    required: true
  }

  @param language: str? {
    enum: ["en", "ru", "es", "fr", "de", "ja", "zh", "ko"]
    description: "Language code (auto-detect if not specified)"
  }

  @param response_format: str? {
    enum: ["json", "text", "srt", "vtt", "verbose_json"]
    description: "Output format (default: json)"
  }
}

tool ai_openai_vision {
  /// Analyze image with GPT-4 Vision
  description: "Analyze and describe an image using GPT-4 Vision. Can generate prompts for recreation."

  @param image_url: str {
    description: "URL of the image to analyze"
    required: true
  }

  @param prompt: str? {
    description: "Custom prompt for image analysis"
  }

  @param mode: str? {
    enum: ["describe", "image_to_prompt"]
    description: "Analysis mode (default: describe)"
  }
}

tool ai_openai_improve_prompt {
  /// Improve AI prompts
  description: "Improve and enhance a prompt for AI image/video generation."

  @param prompt: str {
    description: "Original prompt to improve"
    required: true
  }

  @param style: str? {
    enum: ["image", "video", "text"]
    description: "Target style for optimization (default: image)"
  }
}

// =============================================================================
// Video Editing Tools (FFmpeg)
// =============================================================================

tool video_concat {
  /// Concatenate videos
  description: "Concatenate multiple video files into one."

  @param video_urls: [str] {
    description: "Array of video file paths or URLs to concatenate"
    required: true
  }
}

tool video_add_audio {
  /// Add audio to video
  description: "Add or replace audio track in a video."

  @param video_url: str {
    description: "Path or URL to the video file"
    required: true
  }

  @param audio_url: str {
    description: "Path or URL to the audio file"
    required: true
  }

  @param volume: float? {
    description: "Audio volume (0.0-2.0, default: 1.0)"
  }

  @param replace_audio: bool? {
    description: "Replace original audio completely (default: false)"
  }
}

tool video_watermark {
  /// Add watermark to video
  description: "Add a watermark image to a video."

  @param video_url: str {
    description: "Path or URL to the video file"
    required: true
  }

  @param watermark_url: str {
    description: "Path or URL to the watermark image (PNG recommended)"
    required: true
  }

  @param position: str? {
    enum: ["topleft", "topright", "bottomleft", "bottomright", "center"]
    description: "Watermark position"
  }

  @param opacity: float? {
    description: "Watermark opacity (0.0-1.0, default: 0.7)"
  }

  @param scale: float? {
    description: "Watermark scale relative to video (0.05-1.0, default: 0.15)"
  }
}

tool video_trim {
  /// Trim video
  description: "Trim a video to a specific duration."

  @param video_url: str {
    description: "Path or URL to the video file"
    required: true
  }

  @param start_time: str {
    description: "Start time (seconds or HH:MM:SS)"
    required: true
  }

  @param duration: str {
    description: "Duration to keep (seconds or HH:MM:SS)"
    required: true
  }
}

tool video_info {
  /// Get video information
  description: "Get detailed information about a video file (duration, resolution, codec, etc.)"

  @param video_url: str {
    description: "Path or URL to the video file"
    required: true
  }
}

tool video_extract_audio {
  /// Extract audio from video
  description: "Extract audio track from a video file. Returns MP3 file path."

  @param video_url: str {
    description: "Path or URL to the video file"
    required: true
  }
}

// =============================================================================
// B-Roll Generation Tools
// =============================================================================

tool broll_generate {
  /// Generate B-Roll video
  description: "Generate B-Roll background video using AI. Choose from preset templates or custom prompt."

  @param category: str? {
    enum: ["nature", "city", "abstract", "business", "technology", "lifestyle", "space", "water", "fire", "smoke", "particles", "light"]
    description: "B-Roll category"
  }

  @param template_name: str? {
    description: "Specific template name (use broll_list_templates to see available)"
  }

  @param custom_prompt: str? {
    description: "Custom prompt (overrides category/template)"
  }

  @param model: str? {
    enum: ["kling", "veo3"]
    description: "AI model to use (default: kling)"
  }
}

tool broll_list_templates {
  /// list B-Roll templates
  description: "list available B-Roll video templates. Returns names, prompts, styles."

  @param category: str? {
    enum: ["nature", "city", "abstract", "business", "technology", "lifestyle", "space", "water", "fire", "smoke", "particles", "light"]
    description: "Filter by category (optional)"
  }
}

// =============================================================================
// Types
// =============================================================================

derive(Json)
@enum TTSModel {
  "eleven_multilingual_v2" => ElevenMultilingualV2
  "eleven_turbo_v2" => ElevenTurboV2
  "eleven_monolingual_v1" => ElevenMonolingualV1
}

derive(Json)
@enum AspectRatio {
  "1" => Square
  "16:9" => Landscape
  "9:16" => Portrait
  "4:3" => Standard43
  "3:4" => Standard34
}

derive(Json)
@enum VideoModel {
  "kling_standard" => KlingStandard
  "kling_pro" => KlingPro
  "veo3" => Veo3
}

derive(Json)
@enum ImageModel {
  "flux_dev" => FluxDev
  "flux_schnell" => FluxSchnell
  "flux_pro" => FluxPro
}

derive(Json)
@enum WatermarkPosition {
  "topleft" => TopLeft
  "topright" => TopRight
  "bottomleft" => BottomLeft
  "bottomright" => BottomRight
  "center" => Center
}

derive(Json)
@enum BRollCategory {
  "nature" => Nature
  "city" => City
  "abstract" => Abstract
  "business" => Business
  "technology" => Technology
  "lifestyle" => Lifestyle
  "space" => Space
  "water" => Water
  "fire" => Fire
  "smoke" => Smoke
  "particles" => Particles
  "light" => Light
}

type VoiceCloneResult(
  VoiceCloneResult(
    voice_id: str,
    name: str,
    status: str
  )
}

type VideoGenerationResult(
  VideoGenerationResult(
    task_id: str,
    status: str,
    video_url: str?,
    progress: int
  )
}

type ImageGenerationResult(
  ImageGenerationResult(
    request_id: str,
    status: str,
    image_url: str?
  )
}

type VideoInfo(
  VideoInfo(
    duration: float,
    width: int,
    height: int,
    codec: str,
    bitrate: int,
    fps: float
  )
}

// =============================================================================
// Helper Functions
// =============================================================================

@spec tts_model_from_string(s: str) → TTSModel {
  /// Parse TTS model from string
  given: Model string
  when: Parsing ElevenLabs request
  then: Returns TTSModel enum
}

@impl {
  match s {
    "eleven_turbo_v2" → ElevenTurboV2
    "eleven_monolingual_v1" → ElevenMonolingualV1
    _ → ElevenMultilingualV2
  }
}

@spec tts_model_to_string(m: TTSModel) → str {
  /// Convert TTS model to string
  given: TTSModel enum
  when: Building API request
  then: Returns model ID string
}

@impl {
  match m {
    ElevenMultilingualV2 → "eleven_multilingual_v2"
    ElevenTurboV2 → "eleven_turbo_v2"
    ElevenMonolingualV1 → "eleven_monolingual_v1"
  }
}

@spec aspect_ratio_from_string(s: str) → AspectRatio {
  /// Parse aspect ratio from string
  given: Ratio string
  when: Parsing video request
  then: Returns AspectRatio enum
}

@impl {
  match s {
    "16:9" → Landscape
    "9:16" → Portrait
    "4:3" → Standard43
    "3:4" → Standard34
    _ → Square
  }
}

@spec aspect_ratio_to_string(r: AspectRatio) → str {
  /// Convert aspect ratio to string
  given: AspectRatio enum
  when: Building API request
  then: Returns ratio string
}

@impl {
  match r {
    Square → "1"
    Landscape → "16:9"
    Portrait → "9:16"
    Standard43 → "4:3"
    Standard34 → "3:4"
  }
}

@spec video_model_from_string(s: str) → VideoModel {
  /// Parse video model from string
  given: Model string
  when: Parsing video generation request
  then: Returns VideoModel enum
}

@impl {
  match s {
    "kling_pro" → KlingPro
    "veo3" → Veo3
    _ → KlingStandard
  }
}

@spec image_model_from_string(s: str) → ImageModel {
  /// Parse image model from string
  given: Model string
  when: Parsing image generation request
  then: Returns ImageModel enum
}

@impl {
  match s {
    "flux_schnell" → FluxSchnell
    "flux_pro" → FluxPro
    _ → FluxDev
  }
}

@spec watermark_position_from_string(s: str) → WatermarkPosition {
  /// Parse watermark position from string
  given: Position string
  when: Parsing video editing request
  then: Returns WatermarkPosition enum
}

@impl {
  match s {
    "topleft" → TopLeft
    "topright" → TopRight
    "bottomleft" → BottomLeft
    "bottomright" → BottomRight
    "center" → Center
    _ → BottomRight
  }
}

@spec broll_category_from_string(s: str) → BRollCategory {
  /// Parse B-Roll category from string
  given: Category string
  when: Parsing B-Roll request
  then: Returns BRollCategory enum
}

@impl {
  match s {
    "nature" → Nature
    "city" → City
    "abstract" → Abstract
    "business" → Business
    "technology" → Technology
    "lifestyle" → Lifestyle
    "space" → Space
    "water" → Water
    "fire" → Fire
    "smoke" → Smoke
    "particles" → Particles
    "light" → Light
    _ → Abstract
  }
}

@spec validate_stability(value: float) → float {
  /// Validate stability parameter (0.0-1.0)
  given: float value
  when: Building TTS request
  then: Returns clamped value
}

@impl {
  case value < 0.0 {
    true → 0.0
    false → case value > 1.0 {
      true → 1.0
      false → value
    }
  }
}

@spec validate_volume(value: float) → float {
  /// Validate audio volume (0.0-2.0)
  given: float value
  when: Building video edit request
  then: Returns clamped value
}

@impl {
  case value < 0.0 {
    true → 0.0
    false → case value > 2.0 {
      true → 2.0
      false → value
    }
  }
}

// =============================================================================
// FFI Imports - handlers in infra/mcp/ai_tools_handlers.gleam
// =============================================================================

// @ffi handle_elevenlabs_tts(args: Json) → ToolResult
// @ffi handle_hedra_create_avatar(args: Json) → ToolResult
// @ffi handle_bfl_generate_image(args: Json) → ToolResult
// @ffi handle_kling_create_video(args: Json) → ToolResult
// ... (40+ handlers)

# v8.0

# v8.0
# v10.0 - ML-Powered Migration
