# ============================================================================
# COMPRESSION - Сжатие данных на Vibee
# ============================================================================
# Gzip, Zstd, Brotli, streaming compression
# ============================================================================

Specification Compression:
  """Сжатие как спецификация трансформации данных."""

  # ==========================================================================
  # TYPES
  # ==========================================================================

  Type CompressionAlgorithm:
    variants:
      - Gzip: GzipOptions
      - Deflate: DeflateOptions
      - Zstd: ZstdOptions
      - Brotli: BrotliOptions
      - LZ4: LZ4Options
      - Snappy
      - LZMA: LZMAOptions
      - Bzip2: Bzip2Options

  Type GzipOptions:
    level: Int = 6          # 1-9
    window_bits: Int = 15   # 8-15
    mem_level: Int = 8      # 1-9
    strategy: Strategy = Default

  Type DeflateOptions:
    level: Int = 6
    window_bits: Int = 15
    mem_level: Int = 8
    strategy: Strategy = Default

  Type Strategy:
    variants:
      - Default
      - Filtered
      - HuffmanOnly
      - RLE
      - Fixed

  Type ZstdOptions:
    level: Int = 3          # 1-22
    window_log: Int?        # 10-31
    hash_log: Int?
    chain_log: Int?
    search_log: Int?
    min_match: Int?
    target_length: Int?
    strategy: ZstdStrategy?
    enable_long_distance: Boolean = false
    dictionary: Bytes?

  Type ZstdStrategy:
    variants:
      - Fast
      - DFast
      - Greedy
      - Lazy
      - Lazy2
      - BtLazy2
      - BtOpt
      - BtUltra
      - BtUltra2

  Type BrotliOptions:
    quality: Int = 11       # 0-11
    window: Int = 22        # 10-24
    mode: BrotliMode = Generic

  Type BrotliMode:
    variants:
      - Generic
      - Text
      - Font

  Type LZ4Options:
    level: Int = 0          # 0 = fast, 1-12 = HC
    block_size: BlockSize = Default
    content_checksum: Boolean = false
    block_checksum: Boolean = false

  Type BlockSize:
    variants:
      - Default   # 64KB
      - Max64KB
      - Max256KB
      - Max1MB
      - Max4MB

  Type LZMAOptions:
    preset: Int = 6         # 0-9
    extreme: Boolean = false
    dict_size: Int?
    lc: Int = 3             # 0-4
    lp: Int = 0             # 0-4
    pb: Int = 2             # 0-4

  Type Bzip2Options:
    block_size: Int = 9     # 1-9 (100k-900k)
    work_factor: Int = 30   # 0-250

  # ==========================================================================
  # COMPRESSION RESULT
  # ==========================================================================

  Type CompressionResult:
    data: Bytes
    original_size: ByteSize
    compressed_size: ByteSize
    ratio: Float
    algorithm: CompressionAlgorithm

  Type DecompressionResult:
    data: Bytes
    compressed_size: ByteSize
    decompressed_size: ByteSize

  # ==========================================================================
  # BASIC OPERATIONS
  # ==========================================================================

  Behavior BasicCompression:
    When compress(data, algorithm):
      Then:
        - compressed = match algorithm:
            Gzip(opts) -> gzip_compress(data, opts)
            Deflate(opts) -> deflate_compress(data, opts)
            Zstd(opts) -> zstd_compress(data, opts)
            Brotli(opts) -> brotli_compress(data, opts)
            LZ4(opts) -> lz4_compress(data, opts)
            Snappy -> snappy_compress(data)
            LZMA(opts) -> lzma_compress(data, opts)
            Bzip2(opts) -> bzip2_compress(data, opts)
        
        - return CompressionResult(
            data: compressed,
            original_size: data.size,
            compressed_size: compressed.size,
            ratio: compressed.size / data.size,
            algorithm: algorithm
          )

    When decompress(data, algorithm):
      Then:
        - decompressed = match algorithm:
            Gzip(_) -> gzip_decompress(data)
            Deflate(_) -> deflate_decompress(data)
            Zstd(_) -> zstd_decompress(data)
            Brotli(_) -> brotli_decompress(data)
            LZ4(_) -> lz4_decompress(data)
            Snappy -> snappy_decompress(data)
            LZMA(_) -> lzma_decompress(data)
            Bzip2(_) -> bzip2_decompress(data)
        
        - return DecompressionResult(
            data: decompressed,
            compressed_size: data.size,
            decompressed_size: decompressed.size
          )

    When decompress_auto(data):
      """Auto-detect compression format"""
      Then:
        - algorithm = detect_compression(data)
        - if not algorithm:
            - raise UnknownCompressionFormat
        - return decompress(data, algorithm)

    When detect_compression(data):
      Then:
        - # Check magic bytes
        - if data.starts_with([0x1f, 0x8b]):
            - return Gzip(GzipOptions())
        - if data.starts_with([0x78]):
            - return Deflate(DeflateOptions())
        - if data.starts_with([0x28, 0xb5, 0x2f, 0xfd]):
            - return Zstd(ZstdOptions())
        - if data.starts_with([0x04, 0x22, 0x4d, 0x18]):
            - return LZ4(LZ4Options())
        - if data.starts_with([0x42, 0x5a, 0x68]):
            - return Bzip2(Bzip2Options())
        - if data.starts_with([0xfd, 0x37, 0x7a, 0x58, 0x5a, 0x00]):
            - return LZMA(LZMAOptions())
        - return null

  # ==========================================================================
  # STREAMING COMPRESSION
  # ==========================================================================

  Type CompressStream:
    algorithm: CompressionAlgorithm
    state: CompressorState
    input_buffer: Buffer
    output_buffer: Buffer

  Type DecompressStream:
    algorithm: CompressionAlgorithm
    state: DecompressorState
    input_buffer: Buffer
    output_buffer: Buffer

  Behavior StreamingCompression:
    When create_compress_stream(algorithm):
      Then:
        - state = match algorithm:
            Gzip(opts) -> gzip_init_compress(opts)
            Deflate(opts) -> deflate_init_compress(opts)
            Zstd(opts) -> zstd_init_compress(opts)
            Brotli(opts) -> brotli_init_compress(opts)
            LZ4(opts) -> lz4_init_compress(opts)
            _ -> raise StreamingNotSupported(algorithm)
        
        - return CompressStream(
            algorithm: algorithm,
            state: state,
            input_buffer: Buffer(),
            output_buffer: Buffer()
          )

    When create_decompress_stream(algorithm):
      Then:
        - state = match algorithm:
            Gzip(_) -> gzip_init_decompress()
            Deflate(_) -> deflate_init_decompress()
            Zstd(_) -> zstd_init_decompress()
            Brotli(_) -> brotli_init_decompress()
            LZ4(_) -> lz4_init_decompress()
            _ -> raise StreamingNotSupported(algorithm)
        
        - return DecompressStream(
            algorithm: algorithm,
            state: state,
            input_buffer: Buffer(),
            output_buffer: Buffer()
          )

    When compress_stream.write(data):
      Then:
        - input_buffer.write(data)
        - while input_buffer.readable() >= min_input_size:
            - chunk = input_buffer.read(chunk_size)
            - compressed = compress_chunk(state, chunk, false)
            - output_buffer.write(compressed)

    When compress_stream.read(size):
      Then:
        - return output_buffer.read(min(size, output_buffer.readable()))

    When compress_stream.flush():
      Then:
        - # Process remaining input
        - remaining = input_buffer.read_all()
        - if remaining.is_not_empty():
            - compressed = compress_chunk(state, remaining, false)
            - output_buffer.write(compressed)
        
        - # Flush compressor
        - flushed = flush_compressor(state)
        - output_buffer.write(flushed)

    When compress_stream.finish():
      Then:
        - compress_stream.flush()
        - final = finalize_compressor(state)
        - output_buffer.write(final)
        - return output_buffer.read_all()

    When decompress_stream.write(data):
      Then:
        - input_buffer.write(data)
        - while input_buffer.readable() > 0:
            - chunk = input_buffer.read(chunk_size)
            - decompressed = decompress_chunk(state, chunk)
            - output_buffer.write(decompressed)
            - if state.finished:
                - break

    When decompress_stream.read(size):
      Then:
        - return output_buffer.read(min(size, output_buffer.readable()))

    When decompress_stream.finish():
      Then:
        - if not state.finished:
            - raise IncompleteStream
        - return output_buffer.read_all()

  # ==========================================================================
  # TRANSFORM STREAMS
  # ==========================================================================

  Behavior TransformStreams:
    When compress_transform(input_stream, algorithm):
      """Create a transform stream that compresses data"""
      Then:
        - compressor = create_compress_stream(algorithm)
        - return TransformStream(
            transform: (chunk) ->
              compressor.write(chunk)
              compressor.read(compressor.output_buffer.readable()),
            flush: () ->
              compressor.finish()
          )

    When decompress_transform(input_stream, algorithm):
      """Create a transform stream that decompresses data"""
      Then:
        - decompressor = create_decompress_stream(algorithm)
        - return TransformStream(
            transform: (chunk) ->
              decompressor.write(chunk)
              decompressor.read(decompressor.output_buffer.readable()),
            flush: () ->
              decompressor.finish()
          )

    When pipe_compress(input, output, algorithm):
      Then:
        - compressor = create_compress_stream(algorithm)
        - async for chunk in input:
            - compressor.write(chunk)
            - compressed = compressor.read(compressor.output_buffer.readable())
            - if compressed.is_not_empty():
                - await output.write(compressed)
        - final = compressor.finish()
        - await output.write(final)

    When pipe_decompress(input, output, algorithm):
      Then:
        - decompressor = create_decompress_stream(algorithm)
        - async for chunk in input:
            - decompressor.write(chunk)
            - decompressed = decompressor.read(decompressor.output_buffer.readable())
            - if decompressed.is_not_empty():
                - await output.write(decompressed)
        - final = decompressor.finish()
        - await output.write(final)

  # ==========================================================================
  # FILE COMPRESSION
  # ==========================================================================

  Behavior FileCompression:
    When compress_file(input_path, output_path, algorithm):
      Then:
        - input_stream = await fs.create_read_stream(input_path)
        - output_stream = await fs.create_write_stream(output_path)
        - await pipe_compress(input_stream, output_stream, algorithm)
        - await output_stream.close()

    When decompress_file(input_path, output_path, algorithm):
      Then:
        - input_stream = await fs.create_read_stream(input_path)
        - output_stream = await fs.create_write_stream(output_path)
        - algorithm = algorithm ?? detect_compression_from_file(input_path)
        - await pipe_decompress(input_stream, output_stream, algorithm)
        - await output_stream.close()

    When detect_compression_from_file(path):
      Then:
        - # Check extension
        - ext = path.extension()
        - match ext:
            ".gz" | ".gzip" -> return Gzip(GzipOptions())
            ".zst" | ".zstd" -> return Zstd(ZstdOptions())
            ".br" -> return Brotli(BrotliOptions())
            ".lz4" -> return LZ4(LZ4Options())
            ".bz2" -> return Bzip2(Bzip2Options())
            ".xz" | ".lzma" -> return LZMA(LZMAOptions())
            _ ->
              # Check magic bytes
              header = await fs.read_file(path, size: 16)
              return detect_compression(header)

  # ==========================================================================
  # ARCHIVE FORMATS
  # ==========================================================================

  Type TarOptions:
    format: TarFormat = POSIX
    preserve_permissions: Boolean = true
    preserve_timestamps: Boolean = true
    follow_symlinks: Boolean = false

  Type TarFormat:
    variants: [POSIX, GNU, USTAR, PAX]

  Type TarEntry:
    name: String
    type: TarEntryType
    size: ByteSize
    mode: FileMode
    uid: Int
    gid: Int
    mtime: Timestamp
    linkname: String?
    content: Bytes?

  Type TarEntryType:
    variants:
      - File
      - Directory
      - Symlink
      - Hardlink
      - CharDevice
      - BlockDevice
      - FIFO

  Type ZipOptions:
    compression: ZipCompression = Deflate
    level: Int = 6
    password: String?
    comment: String?

  Type ZipCompression:
    variants: [Store, Deflate, Bzip2, LZMA, Zstd]

  Type ZipEntry:
    name: String
    size: ByteSize
    compressed_size: ByteSize
    compression: ZipCompression
    crc32: Int
    mtime: Timestamp
    comment: String?
    is_directory: Boolean

  Behavior ArchiveOperations:
    When tar_create(output_path, files, options):
      Then:
        - output = await fs.create_write_stream(output_path)
        - for file in files:
            - entry = await create_tar_entry(file, options)
            - await write_tar_entry(output, entry)
        - await write_tar_end(output)
        - await output.close()

    When tar_extract(input_path, output_dir, options):
      Then:
        - input = await fs.create_read_stream(input_path)
        - async for entry in read_tar_entries(input):
            - output_path = output_dir / entry.name
            - match entry.type:
                File ->
                  await fs.write_file(output_path, entry.content)
                  if options.preserve_permissions:
                    await fs.chmod(output_path, entry.mode)
                Directory ->
                  await fs.mkdir(output_path, { recursive: true })
                Symlink ->
                  await fs.symlink(entry.linkname, output_path)

    When tar_list(input_path):
      Then:
        - input = await fs.create_read_stream(input_path)
        - entries = []
        - async for entry in read_tar_entries(input):
            - add entry to entries
        - return entries

    When zip_create(output_path, files, options):
      Then:
        - zip = ZipWriter(output_path, options)
        - for file in files:
            - if await fs.is_directory(file):
                - await zip.add_directory(file)
            - else:
                - content = await fs.read_file(file)
                - await zip.add_file(file.basename(), content)
        - await zip.close()

    When zip_extract(input_path, output_dir, options):
      Then:
        - zip = await ZipReader.open(input_path, options)
        - for entry in zip.entries():
            - output_path = output_dir / entry.name
            - if entry.is_directory:
                - await fs.mkdir(output_path, { recursive: true })
            - else:
                - content = await zip.read_entry(entry)
                - await fs.write_file(output_path, content)
        - await zip.close()

    When zip_list(input_path):
      Then:
        - zip = await ZipReader.open(input_path)
        - entries = zip.entries()
        - await zip.close()
        - return entries

  # ==========================================================================
  # DICTIONARY COMPRESSION
  # ==========================================================================

  Behavior DictionaryCompression:
    When train_dictionary(samples, size):
      """Train a Zstd dictionary from samples"""
      Then:
        - return zstd_train_dictionary(samples, size)

    When compress_with_dictionary(data, dictionary, options):
      Then:
        - opts = options with { dictionary: dictionary }
        - return compress(data, Zstd(opts))

    When decompress_with_dictionary(data, dictionary):
      Then:
        - return zstd_decompress_with_dict(data, dictionary)

  # ==========================================================================
  # EXAMPLE
  # ==========================================================================

  Example "Compression Usage":
    ```vibee
    # Basic compression
    data = "Hello, World!".repeat(1000).encode()
    
    compressed = compress(data, Gzip(GzipOptions(level: 9)))
    log.info("Ratio: ${compressed.ratio}")
    
    decompressed = decompress(compressed.data, Gzip(GzipOptions()))
    assert(decompressed.data == data)

    # Streaming compression
    input = await fs.create_read_stream("/data/large_file.txt")
    output = await fs.create_write_stream("/data/large_file.txt.zst")
    await pipe_compress(input, output, Zstd(ZstdOptions(level: 19)))

    # File compression
    await compress_file("/data/file.txt", "/data/file.txt.gz", Gzip(GzipOptions()))
    await decompress_file("/data/file.txt.gz", "/data/file_restored.txt")

    # Auto-detect decompression
    data = await fs.read_file("/data/compressed_file")
    decompressed = decompress_auto(data)

    # Create tar.gz archive
    await tar_create("/backup/archive.tar", ["/data/dir1", "/data/dir2"], TarOptions())
    await compress_file("/backup/archive.tar", "/backup/archive.tar.gz", Gzip(GzipOptions()))

    # Create zip archive
    await zip_create("/backup/archive.zip", ["/data/file1.txt", "/data/file2.txt"], ZipOptions(
      compression: Deflate,
      level: 9,
      password: "secret"
    ))

    # Dictionary compression for similar data
    samples = await load_samples("/data/logs/*.log")
    dictionary = train_dictionary(samples, 100.KB)
    
    for log_file in log_files:
      data = await fs.read_file(log_file)
      compressed = compress_with_dictionary(data, dictionary, ZstdOptions(level: 3))
      # Much better compression ratio for similar data
    ```
