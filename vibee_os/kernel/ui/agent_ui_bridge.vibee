name: vibee_os_agent_ui_bridge
version: "0.1.0"
language: gleam
module: kernel/ui/agent_bridge
description: Agent-UI Bridge - connects SWE Agent to Pixel Grid for self-evolving visual interfaces

behaviors:
  - name: agent_sees_screen
    given: Pixel grid is rendered
    when: agent_capture_screen is called
    then: Agent receives semantic representation of current UI
    test_cases:
      - name: capture_full_screen
        input: {width: 1920, height: 1080}
        expected: {captured: true, elements_detected: 15, layout_understood: true}

  - name: agent_creates_ui
    given: Agent has UI specification in mind
    when: agent_render_spec is called with description
    then: UI is generated and rendered to pixel grid
    test_cases:
      - name: create_from_description
        input: {description: "A calculator with number buttons 0-9 and operations +,-,*,/"}
        expected: {created: true, buttons: 14, layout: "grid"}

  - name: agent_modifies_ui
    given: UI exists and agent wants to improve it
    when: agent_modify is called with instruction
    then: UI is modified according to instruction
    test_cases:
      - name: make_button_bigger
        input: {instruction: "Make the submit button 50% larger", target: "submit_btn"}
        expected: {modified: true, size_change: 1.5}

  - name: agent_learns_from_interaction
    given: User interacts with UI
    when: Interaction events are collected
    then: Agent learns patterns and suggests improvements
    test_cases:
      - name: learn_click_patterns
        input: {events: [{type: "click", x: 100, y: 200, missed: true}], count: 100}
        expected: {patterns_found: 3, suggestions: 2}

  - name: agent_evolves_ui_autonomously
    given: Agent has permission and fitness metrics
    when: agent_auto_evolve is called
    then: Agent runs evolution loop with human checkpoints
    test_cases:
      - name: auto_evolve_with_approval
        input: {generations: 10, checkpoint_interval: 5}
        expected: {evolved: true, human_approved: true, fitness_improved: true}

types:
  ScreenCapture:
    width: int
    height: int
    pixels: [[int]]
    elements: [UIElement]
    layout_tree: LayoutNode
    semantic_description: str

  UIElement:
    id: str
    type: str
    bounds: Rect
    text: str?
    color: int
    interactive: bool
    children: [UIElement]

  LayoutNode:
    type: LayoutType
    bounds: Rect
    children: [LayoutNode]
    properties: {str: str}

  LayoutType:
    variants:
      - Root
      - Container
      - Row
      - Column
      - Grid
      - Stack
      - Absolute

  Rect:
    x: int
    y: int
    width: int
    height: int

  UISpec:
    description: str
    elements: [ElementSpec]
    layout: LayoutSpec
    style: StyleSpec

  ElementSpec:
    type: str
    id: str?
    text: str?
    action: str?
    children: [ElementSpec]?

  LayoutSpec:
    type: LayoutType
    gap: int?
    padding: int?
    align: str?
    justify: str?

  StyleSpec:
    background: int?
    foreground: int?
    font_size: int?
    border_radius: int?
    shadow: bool?

  ModifyInstruction:
    target: str
    action: ModifyAction
    params: {str: str}

  ModifyAction:
    variants:
      - Resize: {scale: float}
      - Move: {dx: int, dy: int}
      - Recolor: {color: int}
      - Restyle: {style: StyleSpec}
      - Delete
      - Duplicate
      - Reparent: {new_parent: str}

  InteractionPattern:
    type: PatternType
    frequency: int
    elements: [str]
    description: str
    suggestion: str?

  PatternType:
    variants:
      - MissedClick
      - LongHover
      - RapidClick
      - ScrollFatigue
      - BackAndForth
      - Abandonment
      - Success

  AgentUIState:
    current_screen: ScreenCapture?
    interaction_history: [InteractionEvent]
    patterns: [InteractionPattern]
    evolution_state: EvolutionState?
    pending_changes: [ModifyInstruction]

  InteractionEvent:
    type: str
    x: int
    y: int
    target: str?
    timestamp: int
    success: bool?

  EvolutionState:
    generation: int
    population: [UISpec]
    best_fitness: float
    running: bool
    checkpoint_pending: bool

  AgentUIConfig:
    auto_evolve: bool
    evolution_interval_ms: int
    checkpoint_generations: int
    require_human_approval: bool
    max_mutations_per_cycle: int

functions:
  # Screen understanding
  - name: agent_capture_screen
    params: {grid: str}
    returns: ScreenCapture
    description: Capture and understand current screen

  - name: agent_describe_screen
    params: {capture: ScreenCapture}
    returns: str
    description: Generate natural language description

  - name: agent_find_element
    params: {capture: ScreenCapture, description: str}
    returns: UIElement?
    description: Find element by description

  # UI creation
  - name: agent_render_spec
    params: {spec: UISpec, grid: str}
    returns: bool
    description: Render UI specification to grid

  - name: agent_create_from_description
    params: {description: str, grid: str}
    returns: UISpec
    description: Create UI from natural language

  - name: agent_generate_spec
    params: {description: str}
    returns: UISpec
    description: Generate UI spec without rendering

  # UI modification
  - name: agent_modify
    params: {instruction: ModifyInstruction, grid: str}
    returns: bool
    description: Modify existing UI

  - name: agent_modify_natural
    params: {instruction: str, grid: str}
    returns: bool
    description: Modify UI from natural language

  - name: agent_undo
    params: {grid: str}
    returns: bool
    description: Undo last modification

  # Learning
  - name: agent_record_interaction
    params: {event: InteractionEvent}
    returns: void
    description: Record user interaction

  - name: agent_analyze_patterns
    params: {events: [InteractionEvent]}
    returns: [InteractionPattern]
    description: Analyze interaction patterns

  - name: agent_suggest_improvements
    params: {patterns: [InteractionPattern]}
    returns: [ModifyInstruction]
    description: Suggest UI improvements

  # Evolution
  - name: agent_auto_evolve
    params: {grid: str, config: AgentUIConfig}
    returns: EvolutionState
    description: Start autonomous evolution

  - name: agent_evolve_step
    params: {state: EvolutionState}
    returns: EvolutionState
    description: Run one evolution step

  - name: agent_approve_evolution
    params: {state: EvolutionState}
    returns: bool
    description: Human approves evolution checkpoint

  - name: agent_rollback_evolution
    params: {state: EvolutionState, to_generation: int}
    returns: EvolutionState
    description: Rollback to previous generation

imports:
  - kernel.ui.pixel_grid
  - kernel.ui.evolution_engine
  - kernel.agent.core
  - kernel.safety.human_loop

agent_ui_workflow: |
  # Agent UI Workflow
  
  ```
  ┌─────────────────────────────────────────────────────────────────┐
  │                    AGENT UI LOOP                                │
  │                                                                 │
  │  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐ │
  │  │  OBSERVE │───▶│ ANALYZE  │───▶│  DECIDE  │───▶│   ACT    │ │
  │  │ (Screen) │    │(Patterns)│    │(Improve) │    │ (Modify) │ │
  │  └──────────┘    └──────────┘    └──────────┘    └──────────┘ │
  │       ▲                                               │        │
  │       │                                               │        │
  │       │         ┌──────────┐    ┌──────────┐         │        │
  │       └─────────│  LEARN   │◀───│ FEEDBACK │◀────────┘        │
  │                 │(Patterns)│    │  (User)  │                  │
  │                 └──────────┘    └──────────┘                  │
  │                                                                │
  └─────────────────────────────────────────────────────────────────┘
  ```
  
  1. **OBSERVE**: Agent captures screen state
  2. **ANALYZE**: Agent finds interaction patterns
  3. **DECIDE**: Agent chooses improvements
  4. **ACT**: Agent modifies UI (with approval if needed)
  5. **FEEDBACK**: User interacts with new UI
  6. **LEARN**: Agent learns from feedback
  7. **REPEAT**

natural_language_ui: |
  # Natural Language UI Creation
  
  User: "Create a login form with email and password fields"
  
  Agent thinks:
  1. Need a container for the form
  2. Email input field with label
  3. Password input field with label
  4. Submit button
  5. Standard login layout (vertical stack)
  
  Agent generates:
  ```yaml
  description: "Login form"
  layout:
    type: Column
    gap: 16
    padding: 24
    align: center
  elements:
    - type: Text
      text: "Login"
      style: {font_size: 24}
    - type: Input
      id: email
      placeholder: "Email"
    - type: Input
      id: password
      placeholder: "Password"
      secure: true
    - type: Button
      id: submit
      text: "Sign In"
      action: submit_login
  style:
    background: 0xFFFFFF
    border_radius: 8
    shadow: true
  ```
  
  Agent renders to pixel grid → User sees login form

self_improving_ui: |
  # Self-Improving UI
  
  The Agent continuously improves UI based on:
  
  1. **User Behavior**
     - Click accuracy
     - Task completion time
     - Error rates
     - Abandonment points
  
  2. **Fitness Metrics**
     - Usability score
     - Aesthetic score
     - Performance score
     - Accessibility score
  
  3. **Evolution**
     - Genetic mutations
     - Crossover of successful layouts
     - Selection of fittest variants
  
  4. **Human Feedback**
     - Explicit ratings
     - Implicit signals (time spent, returns)
     - Direct corrections
  
  ## Safety Bounds
  
  - Human approval required for major changes
  - Checkpoints every N generations
  - Rollback always available
  - Changes logged for audit
