name: vibee_os_llm_cache
version: "0.1.0"
language: gleam
module: kernel/agent/llm_cache
description: LLM Cache - кэширование ответов LLM для экономии токенов и ускорения

behaviors:
  - name: cache_hit_returns_cached
    given: Request was made before
    when: Same request is made again
    then: Cached response is returned without API call
    test_cases:
      - name: exact_match
        input:
          messages: [{role: "user", content: "Hello"}]
          cached: true
        expected:
          from_cache: true
          api_called: false
          tokens_saved: 50
      - name: cache_miss
        input:
          messages: [{role: "user", content: "New question"}]
          cached: false
        expected:
          from_cache: false
          api_called: true
          result_cached: true

  - name: semantic_cache_similar_queries
    given: Similar (not exact) request was made before
    when: Semantically similar request is made
    then: Cached response is returned if similarity above threshold
    test_cases:
      - name: similar_question
        input:
          query: "What is the capital of France?"
          cached_query: "What's France's capital city?"
          similarity: 0.95
          threshold: 0.9
        expected:
          from_cache: true
          semantic_match: true
      - name: different_question
        input:
          query: "What is the capital of France?"
          cached_query: "What is the population of France?"
          similarity: 0.6
          threshold: 0.9
        expected:
          from_cache: false

  - name: cache_invalidation
    given: Cached entry exists
    when: TTL expires or invalidation triggered
    then: Entry is removed from cache
    test_cases:
      - name: ttl_expiry
        input:
          entry_age_seconds: 3700
          ttl_seconds: 3600
        expected:
          expired: true
          removed: true
      - name: manual_invalidation
        input:
          key: "cache_key_1"
          invalidate: true
        expected:
          removed: true
      - name: pattern_invalidation
        input:
          pattern: "user_*"
          matching_entries: 5
        expected:
          removed: 5

  - name: cache_warming
    given: Common queries are known
    when: System starts
    then: Cache is pre-populated with common responses
    test_cases:
      - name: warm_common_queries
        input:
          queries: ["help", "version", "status"]
        expected:
          warmed: 3
          ready_for_instant_response: true

  - name: cache_statistics
    given: Cache is in use
    when: Stats are requested
    then: Hit rate, size, savings are reported
    test_cases:
      - name: get_stats
        input:
          total_requests: 1000
          cache_hits: 700
        expected:
          hit_rate: 0.7
          tokens_saved: 35000
          cost_saved_usd: 0.35

types:
  # Cache Entry
  CacheEntry:
    key: str
    request_hash: str
    request: CachedRequest
    response: CachedResponse
    created_at: int
    expires_at: int
    hit_count: int
    last_hit_at: int
    embedding: [float]?               # For semantic cache

  CachedRequest:
    messages: [Message]
    model: str
    temperature: float
    max_tokens: int

  Message:
    role: str
    content: str

  CachedResponse:
    content: str
    tokens_used: int
    model: str
    finish_reason: str

  # Cache Configuration
  CacheConfig:
    backend: CacheBackend
    ttl_seconds: int
    max_entries: int
    max_size_bytes: int
    semantic_cache: bool
    semantic_threshold: float
    eviction_policy: EvictionPolicy
    warm_on_start: bool
    warm_queries: [str]?

  CacheBackend:
    variants:
      - Memory
      - SQLite: {path: str}
      - Redis: {url: str}
      - File: {path: str}

  EvictionPolicy:
    variants:
      - LRU                           # Least Recently Used
      - LFU                           # Least Frequently Used
      - FIFO                          # First In First Out
      - TTL                           # Time To Live only

  # Cache Key
  CacheKey:
    messages_hash: str
    model: str
    temperature_bucket: int           # Bucket to allow similar temps
    namespace: str?

  # Cache Result
  CacheResult:
    hit: bool
    entry: CacheEntry?
    match_type: MatchType?

  MatchType:
    variants:
      - Exact
      - Semantic: {similarity: float}

  # Cache Statistics
  CacheStats:
    total_requests: int
    cache_hits: int
    cache_misses: int
    hit_rate: float
    entries_count: int
    size_bytes: int
    tokens_saved: int
    cost_saved_usd: float
    avg_hit_latency_ms: float
    avg_miss_latency_ms: float

  # Invalidation
  InvalidationRequest:
    type: InvalidationType
    target: str

  InvalidationType:
    variants:
      - ByKey: {key: str}
      - ByPattern: {pattern: str}
      - ByAge: {older_than_seconds: int}
      - ByNamespace: {namespace: str}
      - All

  InvalidationResult:
    removed: int
    errors: [str]

  # Warming
  WarmingRequest:
    queries: [str]
    model: str
    priority: WarmingPriority

  WarmingPriority:
    variants:
      - Background
      - Immediate

  WarmingResult:
    warmed: int
    failed: int
    duration_ms: int

functions:
  # Core cache operations
  - name: cache_get
    params: {key: CacheKey}
    returns: CacheResult
    description: Get from cache

  - name: cache_set
    params: {key: CacheKey, request: CachedRequest, response: CachedResponse}
    returns: bool
    description: Store in cache

  - name: cache_delete
    params: {key: str}
    returns: bool
    description: Delete from cache

  # Semantic cache
  - name: cache_get_semantic
    params: {request: CachedRequest, threshold: float}
    returns: CacheResult
    description: Get semantically similar cached response

  - name: cache_compute_similarity
    params: {a: CachedRequest, b: CachedRequest}
    returns: float
    description: Compute similarity between requests

  # Key generation
  - name: cache_key_generate
    params: {request: CachedRequest}
    returns: CacheKey
    description: Generate cache key from request

  - name: cache_key_hash
    params: {messages: [Message]}
    returns: str
    description: Hash messages for cache key

  # Invalidation
  - name: cache_invalidate
    params: {request: InvalidationRequest}
    returns: InvalidationResult
    description: Invalidate cache entries

  - name: cache_clear
    params: {}
    returns: int
    description: Clear entire cache

  # Warming
  - name: cache_warm
    params: {request: WarmingRequest}
    returns: WarmingResult
    description: Pre-populate cache

  # Statistics
  - name: cache_stats
    params: {}
    returns: CacheStats
    description: Get cache statistics

  - name: cache_cost_saved
    params: {}
    returns: float
    description: Calculate cost saved by caching

  # Configuration
  - name: cache_configure
    params: {config: CacheConfig}
    returns: bool
    description: Configure cache

  # Lifecycle
  - name: cache_init
    params: {config: CacheConfig}
    returns: bool
    description: Initialize cache

  - name: cache_shutdown
    params: {}
    returns: bool
    description: Shutdown cache (persist if needed)

imports:
  - kernel.agent.llm_client
  - kernel.agent.embedding_model

default_config: |
  # Default Cache Configuration
  
  ```yaml
  backend: Memory
  ttl_seconds: 3600              # 1 hour
  max_entries: 10000
  max_size_bytes: 104857600      # 100 MB
  semantic_cache: true
  semantic_threshold: 0.92
  eviction_policy: LRU
  warm_on_start: false
  ```
  
  ## Production Configuration
  
  ```yaml
  backend:
    SQLite:
      path: "/var/lib/vibee/llm_cache.db"
  ttl_seconds: 86400             # 24 hours
  max_entries: 100000
  semantic_cache: true
  semantic_threshold: 0.95
  eviction_policy: LRU
  warm_on_start: true
  warm_queries:
    - "Generate Vibee spec for"
    - "Fix this error"
    - "Explain this code"
  ```

cache_key_strategy: |
  # Cache Key Strategy
  
  ## Exact Match Key
  ```
  key = hash(
    sorted(messages),
    model,
    temperature_bucket(temperature),
    namespace
  )
  ```
  
  Temperature bucketing:
  - 0.0-0.3 → "deterministic"
  - 0.3-0.7 → "balanced"
  - 0.7-1.0 → "creative"
  
  ## Semantic Key
  ```
  key = embedding(last_user_message)
  similarity = cosine(key, cached_key)
  hit if similarity > threshold
  ```

cost_savings: |
  # Cost Savings Calculation
  
  ```
  tokens_saved = sum(cached_response.tokens_used for hit in cache_hits)
  
  cost_saved = tokens_saved * price_per_token
  
  For DeepSeek:
    input: $0.0001 / 1K tokens
    output: $0.0002 / 1K tokens
  
  Example:
    1000 cache hits
    avg 500 tokens per response
    = 500,000 tokens saved
    = $0.10 saved (output tokens)
  ```
  
  ## ROI
  ```
  cache_cost = memory_cost + compute_cost
  savings = tokens_saved * token_price
  roi = (savings - cache_cost) / cache_cost
  
  Typical ROI: 10x - 100x
  ```

semantic_cache_flow: |
  # Semantic Cache Flow
  
  ```
  Request
     │
     ▼
  ┌─────────────┐
  │ Exact Match │──── Hit ────▶ Return Cached
  │   Lookup    │
  └──────┬──────┘
         │ Miss
         ▼
  ┌─────────────┐
  │  Embed      │
  │  Request    │
  └──────┬──────┘
         │
         ▼
  ┌─────────────┐
  │  Semantic   │──── Hit ────▶ Return Cached
  │   Search    │   (sim > 0.92)
  └──────┬──────┘
         │ Miss
         ▼
  ┌─────────────┐
  │  Call LLM   │
  └──────┬──────┘
         │
         ▼
  ┌─────────────┐
  │ Store in    │
  │   Cache     │
  └──────┬──────┘
         │
         ▼
    Return Response
  ```
