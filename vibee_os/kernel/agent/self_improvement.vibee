name: vibee_os_self_improvement
version: "0.1.0"
language: zig
module: kernel/agent/self_improvement
description: Self-Improvement Loop - agent learns and improves from experience (Devin-inspired feedback loops)

behaviors:
  - name: learn_from_success
    given: Agent successfully completes a task
    when: record_success is called
    then: Successful patterns are extracted and stored
    test_cases:
      - name: learn_code_pattern
        input: {task: "implement auth", outcome: "success", code_quality: 0.95}
        expected: {pattern_extracted: true, stored_in_memory: true}

  - name: learn_from_failure
    given: Agent fails a task
    when: record_failure is called
    then: Failure is analyzed and anti-patterns are stored
    test_cases:
      - name: learn_from_bug
        input: {task: "parse JSON", outcome: "failure", error: "null pointer"}
        expected: {anti_pattern_extracted: true, fix_strategy_stored: true}

  - name: improve_prompts
    given: Agent has execution history
    when: optimize_prompts is called
    then: System prompts are refined based on performance
    test_cases:
      - name: improve_code_gen_prompt
        input: {prompt_id: "code_gen", success_rate: 0.7, target_rate: 0.9}
        expected: {prompt_improved: true, new_success_rate: 0.85}

  - name: improve_tools
    given: Tool usage patterns are analyzed
    when: optimize_tools is called
    then: Tool definitions and usage are improved
    test_cases:
      - name: improve_file_tool
        input: {tool: "file_read", avg_tokens: 500, errors: 10}
        expected: {tool_improved: true, new_avg_tokens: 300, new_errors: 2}

  - name: reflect_on_performance
    given: Sufficient execution history exists
    when: reflect is called
    then: Agent generates insights about its own performance
    test_cases:
      - name: weekly_reflection
        input: {period: "week", tasks_completed: 100}
        expected: {insights_generated: 5, improvements_suggested: 3}

  - name: self_test
    given: Agent has made improvements
    when: self_test is called
    then: Agent tests its own capabilities
    test_cases:
      - name: test_after_improvement
        input: {improvement_id: "imp_123"}
        expected: {tests_run: 50, tests_passed: 48, regression: false}

types:
  LearningRecord:
    id: str
    task_type: str
    outcome: Outcome
    context: str
    actions_taken: [str]
    duration_ms: int
    tokens_used: int
    feedback: Feedback?
    patterns_extracted: [Pattern]
    timestamp: int

  Outcome:
    variants:
      - Success
      - PartialSuccess
      - Failure
      - Timeout
      - Cancelled

  Feedback:
    source: FeedbackSource
    rating: int  # 1-5
    comment: str?
    corrections: [Correction]?

  FeedbackSource:
    variants:
      - User
      - AutoTest
      - SelfEvaluation
      - PeerAgent

  Correction:
    what_was_wrong: str
    what_should_be: str
    severity: str

  Pattern:
    id: str
    type: PatternType
    description: str
    context: str
    frequency: int
    success_rate: float
    examples: [str]

  PatternType:
    variants:
      - SuccessPattern    # What works
      - AntiPattern       # What to avoid
      - Optimization      # How to be faster
      - ErrorRecovery     # How to handle errors

  PromptOptimization:
    prompt_id: str
    original_prompt: str
    optimized_prompt: str
    changes: [PromptChange]
    before_metrics: PromptMetrics
    after_metrics: PromptMetrics

  PromptChange:
    type: ChangeType
    description: str
    rationale: str

  ChangeType:
    variants:
      - AddInstruction
      - RemoveInstruction
      - ClarifyInstruction
      - AddExample
      - ReorderSections

  PromptMetrics:
    success_rate: float
    avg_tokens: int
    avg_latency_ms: int
    user_satisfaction: float

  ToolOptimization:
    tool_id: str
    changes: [ToolChange]
    before_metrics: ToolMetrics
    after_metrics: ToolMetrics

  ToolChange:
    type: str
    description: str

  ToolMetrics:
    usage_count: int
    success_rate: float
    avg_tokens: int
    error_rate: float

  Reflection:
    period: str
    summary: str
    insights: [Insight]
    improvements: [ImprovementSuggestion]
    metrics: PerformanceMetrics

  Insight:
    category: str
    description: str
    evidence: [str]
    confidence: float

  ImprovementSuggestion:
    id: str
    description: str
    expected_impact: str
    effort: EffortLevel
    priority: int

  EffortLevel:
    variants:
      - Low
      - Medium
      - High

  PerformanceMetrics:
    tasks_completed: int
    success_rate: float
    avg_duration_ms: int
    avg_tokens_per_task: int
    user_satisfaction: float
    self_improvement_count: int

  SelfTest:
    id: str
    test_suite: str
    results: [TestResult]
    passed: int
    failed: int
    regressions: [str]

  TestResult:
    test_name: str
    passed: bool
    duration_ms: int
    error: str?

  ImprovementCycle:
    id: str
    started_at: int
    completed_at: int?
    phases: [CyclePhase]
    outcome: CycleOutcome?

  CyclePhase:
    name: str
    status: PhaseStatus
    duration_ms: int
    artifacts: [str]

  PhaseStatus:
    variants:
      - Pending
      - InProgress
      - Completed
      - Failed
      - Skipped

  CycleOutcome:
    success: bool
    improvements_applied: int
    performance_delta: float
    rollbacks: int

functions:
  # Learning
  - name: record_success
    params: {task: str, context: str, actions: [str], feedback: Feedback?}
    returns: LearningRecord
    description: Record successful task completion

  - name: record_failure
    params: {task: str, context: str, error: str, feedback: Feedback?}
    returns: LearningRecord
    description: Record task failure

  - name: extract_patterns
    params: {records: [LearningRecord]}
    returns: [Pattern]
    description: Extract patterns from learning records

  - name: get_relevant_patterns
    params: {task: str, limit: int}
    returns: [Pattern]
    description: Get patterns relevant to task

  # Prompt optimization
  - name: analyze_prompt_performance
    params: {prompt_id: str}
    returns: PromptMetrics
    description: Analyze prompt performance

  - name: optimize_prompt
    params: {prompt_id: str}
    returns: PromptOptimization
    description: Optimize prompt based on performance

  - name: apply_prompt_optimization
    params: {optimization: PromptOptimization}
    returns: bool
    description: Apply prompt optimization

  - name: rollback_prompt
    params: {prompt_id: str}
    returns: bool
    description: Rollback to previous prompt version

  # Tool optimization
  - name: analyze_tool_usage
    params: {tool_id: str}
    returns: ToolMetrics
    description: Analyze tool usage patterns

  - name: optimize_tool
    params: {tool_id: str}
    returns: ToolOptimization
    description: Optimize tool definition

  - name: apply_tool_optimization
    params: {optimization: ToolOptimization}
    returns: bool
    description: Apply tool optimization

  # Reflection
  - name: reflect
    params: {period: str}
    returns: Reflection
    description: Generate reflection on performance

  - name: generate_insights
    params: {records: [LearningRecord]}
    returns: [Insight]
    description: Generate insights from records

  - name: suggest_improvements
    params: {reflection: Reflection}
    returns: [ImprovementSuggestion]
    description: Suggest improvements based on reflection

  # Self-testing
  - name: self_test
    params: {test_suite: str}
    returns: SelfTest
    description: Run self-tests

  - name: detect_regressions
    params: {before: SelfTest, after: SelfTest}
    returns: [str]
    description: Detect regressions between test runs

  # Improvement cycle
  - name: start_improvement_cycle
    params: {}
    returns: ImprovementCycle
    description: Start a self-improvement cycle

  - name: run_improvement_cycle
    params: {cycle_id: str}
    returns: CycleOutcome
    description: Run full improvement cycle

  - name: get_improvement_history
    params: {limit: int}
    returns: [ImprovementCycle]
    description: Get history of improvement cycles

imports:
  - kernel.agent.core
  - kernel.agent.semantic_memory
  - kernel.safety.human_loop

improvement_cycle: |
  VIBEE OS Self-Improvement Cycle:
  
  ┌─────────────────────────────────────────────────────────────────┐
  │                  SELF-IMPROVEMENT LOOP                          │
  │                                                                 │
  │    ┌──────────┐                              ┌──────────┐      │
  │    │  OBSERVE │                              │  DEPLOY  │      │
  │    │ (Record) │                              │ (Apply)  │      │
  │    └────┬─────┘                              └────▲─────┘      │
  │         │                                         │            │
  │         ▼                                         │            │
  │    ┌──────────┐    ┌──────────┐    ┌──────────┐  │            │
  │    │  ANALYZE │───▶│  REFLECT │───▶│  IMPROVE │──┘            │
  │    │(Patterns)│    │(Insights)│    │(Optimize)│               │
  │    └──────────┘    └──────────┘    └──────────┘               │
  │                                         │                      │
  │                                         ▼                      │
  │                                    ┌──────────┐               │
  │                                    │   TEST   │               │
  │                                    │(Validate)│               │
  │                                    └──────────┘               │
  │                                                                │
  └─────────────────────────────────────────────────────────────────┘
  
  Phases:
  
  1. OBSERVE
     - Record all task executions
     - Capture success/failure outcomes
     - Collect user feedback
     - Log token usage and latency
  
  2. ANALYZE
     - Extract success patterns
     - Identify anti-patterns
     - Find optimization opportunities
     - Detect recurring errors
  
  3. REFLECT
     - Generate insights from data
     - Compare with past performance
     - Identify trends
     - Prioritize improvements
  
  4. IMPROVE
     - Optimize prompts
     - Refine tool definitions
     - Update strategies
     - Add new patterns to memory
  
  5. TEST
     - Run self-tests
     - Check for regressions
     - Validate improvements
     - Measure performance delta
  
  6. DEPLOY
     - Apply validated improvements
     - Update system configuration
     - Notify human of changes
     - Monitor for issues

safety_bounds: |
  Self-Improvement Safety Bounds:
  
  1. HUMAN APPROVAL
     - All improvements require human approval
     - Critical changes need explicit confirmation
     - Rollback always available
  
  2. BOUNDED CHANGES
     - Max 3 improvements per cycle
     - Max 1 cycle per day (auto)
     - Changes must be reversible
  
  3. REGRESSION PREVENTION
     - Full test suite before/after
     - No deployment if regressions detected
     - Automatic rollback on failure
  
  4. TRANSPARENCY
     - All changes logged
     - Reasoning explained
     - Metrics tracked
  
  5. GRADUAL ROLLOUT
     - Canary testing first
     - Staged deployment
     - Monitoring at each stage
