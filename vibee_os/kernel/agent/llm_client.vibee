name: vibee_os_llm_client
version: "0.1.0"
language: gleam
module: kernel/agent/llm_client
description: Real LLM Client - HTTP клиент для DeepSeek и других провайдеров. Генерируется в рабочий код.

behaviors:
  - name: send_chat_completion
    given: API key is configured via ${DEEPSEEK_API_KEY}
    when: chat_complete is called with messages
    then: HTTP POST to api.deepseek.com/v1/chat/completions returns response
    test_cases:
      - name: simple_chat
        input:
          messages:
            - role: "user"
              content: "Hello"
          model: "deepseek-chat"
        expected:
          status: 200
          has_choices: true
          has_content: true
      - name: with_system_prompt
        input:
          messages:
            - role: "system"
              content: "You are a helpful assistant"
            - role: "user"
              content: "Hi"
        expected:
          status: 200
          has_content: true
      - name: invalid_api_key
        input:
          api_key: "invalid"
          messages:
            - role: "user"
              content: "Hello"
        expected:
          status: 401
          error: "authentication_error"

  - name: stream_chat_completion
    given: API key is configured
    when: chat_stream is called with messages
    then: Server-sent events are received token by token
    test_cases:
      - name: stream_response
        input:
          messages:
            - role: "user"
              content: "Count to 5"
          stream: true
        expected:
          chunks_received: true
          final_content_complete: true

  - name: call_with_tools
    given: Tools are defined
    when: chat_complete is called with tools
    then: LLM may return tool_calls to execute
    test_cases:
      - name: tool_call_requested
        input:
          messages:
            - role: "user"
              content: "What's the weather in Tokyo?"
          tools:
            - name: "get_weather"
              description: "Get weather for a city"
              parameters:
                city: "string"
        expected:
          has_tool_calls: true
          tool_name: "get_weather"
          tool_args_has_city: true

  - name: handle_rate_limit
    given: Rate limit is exceeded
    when: API returns 429
    then: Client waits and retries
    test_cases:
      - name: retry_after_rate_limit
        input:
          rate_limited: true
          retry_after_ms: 1000
        expected:
          retried: true
          eventual_success: true

  - name: fallback_to_local
    given: Remote API is unavailable
    when: Connection fails
    then: Client falls back to local model if configured
    test_cases:
      - name: fallback_on_timeout
        input:
          timeout: true
          fallback_configured: true
        expected:
          used_fallback: true
          fallback_model: "local"

types:
  # HTTP Request/Response
  HTTPRequest:
    method: HTTPMethod
    url: str
    headers: {str: str}
    body: str?
    timeout_ms: int

  HTTPMethod:
    variants:
      - GET
      - POST
      - PUT
      - DELETE

  HTTPResponse:
    status: int
    headers: {str: str}
    body: str

  # Chat Completion
  ChatRequest:
    model: str
    messages: [ChatMessage]
    tools: [ToolDef]?
    tool_choice: ToolChoice?
    max_tokens: int?
    temperature: float?
    stream: bool?
    response_format: ResponseFormat?

  ChatMessage:
    role: MessageRole
    content: str
    name: str?
    tool_calls: [ToolCall]?
    tool_call_id: str?

  MessageRole:
    variants:
      - System
      - User
      - Assistant
      - Tool

  ToolDef:
    type: str                         # "function"
    function: FunctionDef

  FunctionDef:
    name: str
    description: str
    parameters: JSONSchema

  JSONSchema:
    type: str
    properties: {str: PropertyDef}
    required: [str]?

  PropertyDef:
    type: str
    description: str?
    enum: [str]?

  ToolChoice:
    variants:
      - Auto
      - None
      - Required
      - Function: {name: str}

  ResponseFormat:
    variants:
      - Text
      - JSON

  ToolCall:
    id: str
    type: str                         # "function"
    function: FunctionCall

  FunctionCall:
    name: str
    arguments: str                    # JSON string

  ChatResponse:
    id: str
    object: str
    created: int
    model: str
    choices: [Choice]
    usage: Usage

  Choice:
    index: int
    message: ChatMessage
    finish_reason: FinishReason

  FinishReason:
    variants:
      - Stop
      - Length
      - ToolCalls
      - ContentFilter

  Usage:
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

  # Streaming
  StreamChunk:
    id: str
    object: str
    created: int
    model: str
    choices: [StreamChoice]

  StreamChoice:
    index: int
    delta: Delta
    finish_reason: FinishReason?

  Delta:
    role: MessageRole?
    content: str?
    tool_calls: [ToolCallDelta]?

  ToolCallDelta:
    index: int
    id: str?
    function: FunctionDelta?

  FunctionDelta:
    name: str?
    arguments: str?

  # Client Configuration
  LLMClientConfig:
    provider: Provider
    api_key_env: str                  # Environment variable name, e.g., "DEEPSEEK_API_KEY"
    api_endpoint: str
    model: str
    max_tokens: int
    temperature: float
    timeout_ms: int
    retry_count: int
    retry_delay_ms: int
    fallback: LLMClientConfig?

  Provider:
    variants:
      - DeepSeek
      - OpenAI
      - Anthropic
      - Local

  # Client State
  LLMClient:
    config: LLMClientConfig
    stats: ClientStats

  ClientStats:
    total_requests: int
    total_tokens: int
    total_cost_usd: float
    errors: int
    avg_latency_ms: float

  # Errors
  LLMError:
    variants:
      - AuthenticationError: {message: str}
      - RateLimitError: {retry_after_ms: int}
      - InvalidRequestError: {message: str}
      - ConnectionError: {message: str}
      - TimeoutError: {timeout_ms: int}
      - ServerError: {status: int, message: str}

  # Result type
  LLMResult:
    variants:
      - Ok: {response: ChatResponse}
      - Err: {error: LLMError}

functions:
  # Client lifecycle
  - name: client_new
    params: {config: LLMClientConfig}
    returns: LLMClient
    description: Create new LLM client

  - name: client_default
    params: {}
    returns: LLMClient
    description: Create client with default DeepSeek config

  # Chat completion
  - name: chat_complete
    params: {client: LLMClient, request: ChatRequest}
    returns: LLMResult
    description: Send chat completion request

  - name: chat_simple
    params: {client: LLMClient, user_message: str}
    returns: LLMResult
    description: Simple chat with just user message

  - name: chat_with_system
    params: {client: LLMClient, system_prompt: str, user_message: str}
    returns: LLMResult
    description: Chat with system prompt

  # Streaming
  - name: chat_stream
    params: {client: LLMClient, request: ChatRequest, on_chunk: str}
    returns: LLMResult
    description: Stream chat completion

  # Tool calling
  - name: chat_with_tools
    params: {client: LLMClient, request: ChatRequest, tool_executor: str}
    returns: LLMResult
    description: Chat with automatic tool execution loop

  # Utilities
  - name: health_check
    params: {client: LLMClient}
    returns: bool
    description: Check if API is available

  - name: get_stats
    params: {client: LLMClient}
    returns: ClientStats
    description: Get usage statistics

  - name: estimate_cost
    params: {prompt_tokens: int, completion_tokens: int, model: str}
    returns: float
    description: Estimate cost in USD

  # HTTP (low-level)
  - name: http_request
    params: {request: HTTPRequest}
    returns: HTTPResponse
    description: Send HTTP request

  - name: http_post_json
    params: {url: str, headers: {str: str}, body: str}
    returns: HTTPResponse
    description: POST JSON to URL

imports:
  - kernel.security.secrets_manager
  - config.llm_providers

default_config: |
  # Default DeepSeek Configuration
  
  ```yaml
  config:
    provider: DeepSeek
    api_key_env: "DEEPSEEK_API_KEY"
    api_endpoint: "https://api.deepseek.com/v1/chat/completions"
    model: "deepseek-chat"
    max_tokens: 4096
    temperature: 0.7
    timeout_ms: 30000
    retry_count: 3
    retry_delay_ms: 1000
  ```

http_implementation: |
  # HTTP Implementation Notes
  
  The http_request function generates to:
  
  ## Gleam (using gleam_httpc)
  ```gleam
  pub fn http_request(request: HTTPRequest) -> HTTPResponse {
    let req = httpc.Request(
      method: request.method,
      url: request.url,
      headers: request.headers,
      body: request.body,
    )
    httpc.send(req)
  }
  ```
  
  ## Zig (using std.http)
  ```zig
  pub fn http_request(request: HTTPRequest) HTTPResponse {
      var client = std.http.Client{};
      defer client.deinit();
      
      var req = client.request(.POST, request.url, ...);
      // ...
  }
  ```

api_request_format: |
  # DeepSeek API Request Format
  
  ```json
  POST https://api.deepseek.com/v1/chat/completions
  Authorization: Bearer ${DEEPSEEK_API_KEY}
  Content-Type: application/json
  
  {
    "model": "deepseek-chat",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant"},
      {"role": "user", "content": "Hello"}
    ],
    "max_tokens": 4096,
    "temperature": 0.7,
    "stream": false
  }
  ```
  
  Response:
  ```json
  {
    "id": "chatcmpl-xxx",
    "object": "chat.completion",
    "created": 1234567890,
    "model": "deepseek-chat",
    "choices": [{
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! How can I help you?"
      },
      "finish_reason": "stop"
    }],
    "usage": {
      "prompt_tokens": 10,
      "completion_tokens": 8,
      "total_tokens": 18
    }
  }
  ```

pricing: |
  # DeepSeek Pricing (per 1K tokens)
  
  | Model | Input | Output |
  |-------|-------|--------|
  | deepseek-chat | $0.0001 | $0.0002 |
  | deepseek-reasoner | $0.0005 | $0.001 |
  
  Example: 1M tokens = $0.10 - $0.20
  
  Compare to:
  | Model | Input | Output |
  |-------|-------|--------|
  | GPT-4 Turbo | $0.01 | $0.03 |
  | Claude 3.5 Sonnet | $0.003 | $0.015 |
  
  DeepSeek is 100x cheaper than GPT-4!
