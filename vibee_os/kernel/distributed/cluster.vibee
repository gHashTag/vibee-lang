# ============================================================================
# CLUSTER - Распределённые вычисления на Vibee
# ============================================================================
# Кластеризация, consensus, sharding, service discovery, load balancing
# ============================================================================

Specification Cluster:
  """
  Кластер как спецификация распределённой системы.
  Каждый узел - актор, каждое сообщение - событие.
  """

  # ==========================================================================
  # ТИПЫ ДАННЫХ
  # ==========================================================================

  Type Node:
    id: NodeId
    name: String
    address: Address
    port: Port
    state: NodeState
    role: NodeRole
    metadata: Map<String, Any>
    joined_at: Timestamp
    last_heartbeat: Timestamp
    load: NodeLoad

  Type NodeId:
    value: UUID

  Type NodeState:
    variants:
      - Joining
      - Active
      - Leaving
      - Down
      - Suspect

  Type NodeRole:
    variants:
      - Leader
      - Follower
      - Candidate
      - Observer

  Type NodeLoad:
    cpu_percent: Float
    memory_percent: Float
    connections: Int
    requests_per_second: Float
    latency_ms: Float

  Type Address:
    variants:
      - IPv4: String
      - IPv6: String
      - DNS: String

  Type ClusterConfig:
    name: String
    seeds: List<(Address, Port)>
    bind_address: Address = "0.0.0.0"
    bind_port: Port = 7946
    
    # Membership
    gossip_interval: Duration = 1.second
    gossip_nodes: Int = 3
    probe_interval: Duration = 1.second
    probe_timeout: Duration = 500.ms
    suspicion_mult: Int = 4
    
    # Consensus
    consensus: ConsensusConfig
    
    # Security
    encryption_key: String?
    auth_token: String?

  Type ConsensusConfig:
    algorithm: ConsensusAlgorithm = Raft
    election_timeout: Duration = 150.ms..300.ms
    heartbeat_interval: Duration = 50.ms
    snapshot_interval: Int = 10000
    max_log_entries: Int = 100000

  Type ConsensusAlgorithm:
    variants:
      - Raft
      - Paxos
      - PBFT
      - Gossip

  # ==========================================================================
  # СОСТОЯНИЕ КЛАСТЕРА
  # ==========================================================================

  Type ClusterState:
    config: ClusterConfig
    self: Node
    nodes: Map<NodeId, Node>
    leader: NodeId?
    term: Int
    commit_index: Int
    last_applied: Int
    log: List<LogEntry>
    state_machine: StateMachine

  Type LogEntry:
    index: Int
    term: Int
    command: Command
    timestamp: Timestamp

  Type Command:
    variants:
      - Set: (key: String, value: Bytes)
      - Delete: String
      - CAS: (key: String, expected: Bytes?, new_value: Bytes)
      - Custom: (type: String, data: Bytes)

  Type StateMachine:
    data: Map<String, Bytes>
    version: Int
    last_snapshot: Snapshot?

  Type Snapshot:
    index: Int
    term: Int
    data: Bytes
    created_at: Timestamp

  # ==========================================================================
  # MEMBERSHIP (SWIM Protocol)
  # ==========================================================================

  Behavior Membership:
    """SWIM протокол для membership"""

    State:
      members: Map<NodeId, MemberState>
      incarnation: Int = 0

    Type MemberState:
      node: Node
      state: NodeState
      incarnation: Int
      suspicion_timer: Timer?

    When join_cluster(seeds):
      Then:
        - for seed in seeds:
            - result = await send_join_request(seed)
            - if result.success:
                - merge_members(result.members)
                - set self.state to Active
                - emit NodeJoined(self)
                - return Ok()
        - return Error("Failed to join cluster")

    When leave_cluster():
      Then:
        - set self.state to Leaving
        - broadcast_leave()
        - await wait_for_acks(timeout: 5.seconds)
        - set self.state to Down
        - emit NodeLeft(self)

    When gossip_loop():
      """Периодический gossip"""
      Then:
        - loop every config.gossip_interval:
            - targets = select_random_nodes(config.gossip_nodes)
            - for target in targets:
                - await gossip_with(target)

    When gossip_with(target):
      Then:
        - message = GossipMessage(
            from: self.id,
            members: get_member_states(),
            incarnation: incarnation
          )
        - response = await send(target, message, timeout: config.probe_timeout)
        - if response:
            - merge_gossip(response)
        - else:
            - suspect_node(target)

    When probe_loop():
      """Периодическая проверка узлов"""
      Then:
        - loop every config.probe_interval:
            - target = select_random_node()
            - if target:
                - alive = await probe(target)
                - if not alive:
                    - await indirect_probe(target)

    When probe(target):
      Then:
        - response = await send_ping(target, timeout: config.probe_timeout)
        - return response != null

    When indirect_probe(target):
      """Непрямая проверка через другие узлы"""
      Then:
        - witnesses = select_random_nodes(3, exclude: [target])
        - results = await parallel(
            witnesses.map(w -> send_ping_req(w, target))
          )
        - if results.any(r -> r.alive):
            - mark_alive(target)
        - else:
            - suspect_node(target)

    When suspect_node(node_id):
      Then:
        - member = members[node_id]
        - if member.state != Suspect:
            - set member.state to Suspect
            - start suspicion_timer for member
            - emit NodeSuspected(node_id)

    When suspicion_timer_expired(node_id):
      Then:
        - member = members[node_id]
        - set member.state to Down
        - remove member from active members
        - emit NodeDown(node_id)

    When refute_suspicion():
      """Опровергнуть подозрение о себе"""
      Then:
        - increment incarnation
        - broadcast_alive(self.id, incarnation)

  # ==========================================================================
  # CONSENSUS (Raft)
  # ==========================================================================

  Behavior RaftConsensus:
    """Raft алгоритм консенсуса"""

    State:
      role: NodeRole = Follower
      current_term: Int = 0
      voted_for: NodeId?
      log: List<LogEntry> = []
      commit_index: Int = 0
      last_applied: Int = 0
      
      # Leader state
      next_index: Map<NodeId, Int> = {}
      match_index: Map<NodeId, Int> = {}
      
      # Timers
      election_timer: Timer
      heartbeat_timer: Timer?

    When start_raft():
      Then:
        - reset_election_timer()
        - start apply_loop()

    When election_timer_expired():
      Given role is Follower or Candidate
      Then:
        - start_election()

    When start_election():
      Then:
        - increment current_term
        - set role to Candidate
        - set voted_for to self.id
        - reset_election_timer()
        
        - votes = 1  # Vote for self
        - last_log = log.last()
        
        - responses = await parallel(
            nodes.map(n -> request_vote(n, current_term, last_log))
          )
        
        - for response in responses:
            - if response.term > current_term:
                - become_follower(response.term)
                - return
            - if response.vote_granted:
                - increment votes
        
        - if votes > nodes.size / 2:
            - become_leader()

    When request_vote(candidate_id, term, last_log):
      Then:
        - if term < current_term:
            - return VoteResponse(term: current_term, vote_granted: false)
        
        - if term > current_term:
            - become_follower(term)
        
        - log_ok = last_log.term > log.last().term or
            (last_log.term == log.last().term and last_log.index >= log.last().index)
        
        - if (voted_for == null or voted_for == candidate_id) and log_ok:
            - set voted_for to candidate_id
            - reset_election_timer()
            - return VoteResponse(term: current_term, vote_granted: true)
        
        - return VoteResponse(term: current_term, vote_granted: false)

    When become_leader():
      Then:
        - set role to Leader
        - set leader to self.id
        
        # Initialize leader state
        - for node in nodes:
            - next_index[node.id] = log.size + 1
            - match_index[node.id] = 0
        
        - start heartbeat_timer
        - emit LeaderElected(self.id, current_term)
        
        # Send initial heartbeat
        - broadcast_append_entries()

    When become_follower(term):
      Then:
        - set current_term to term
        - set role to Follower
        - set voted_for to null
        - stop heartbeat_timer if running
        - reset_election_timer()

    When heartbeat_timer_expired():
      Given role is Leader
      Then:
        - broadcast_append_entries()
        - reset heartbeat_timer

    When broadcast_append_entries():
      Then:
        - for node in nodes:
            - spawn send_append_entries(node)

    When send_append_entries(node):
      Then:
        - prev_index = next_index[node.id] - 1
        - prev_term = if prev_index > 0 then log[prev_index].term else 0
        - entries = log.slice(next_index[node.id])
        
        - response = await append_entries_rpc(
            node,
            term: current_term,
            leader_id: self.id,
            prev_log_index: prev_index,
            prev_log_term: prev_term,
            entries: entries,
            leader_commit: commit_index
          )
        
        - if response.term > current_term:
            - become_follower(response.term)
            - return
        
        - if response.success:
            - next_index[node.id] = prev_index + entries.size + 1
            - match_index[node.id] = prev_index + entries.size
            - try_advance_commit_index()
        - else:
            - decrement next_index[node.id]
            - retry send_append_entries(node)

    When append_entries(leader_id, term, prev_log_index, prev_log_term, entries, leader_commit):
      Then:
        - if term < current_term:
            - return AppendResponse(term: current_term, success: false)
        
        - if term > current_term:
            - become_follower(term)
        
        - reset_election_timer()
        - set leader to leader_id
        
        # Check log consistency
        - if prev_log_index > 0:
            - if log.size < prev_log_index:
                - return AppendResponse(term: current_term, success: false)
            - if log[prev_log_index].term != prev_log_term:
                - truncate log to prev_log_index - 1
                - return AppendResponse(term: current_term, success: false)
        
        # Append new entries
        - for entry in entries:
            - if log.size >= entry.index and log[entry.index].term != entry.term:
                - truncate log to entry.index - 1
            - if log.size < entry.index:
                - append entry to log
        
        # Update commit index
        - if leader_commit > commit_index:
            - set commit_index to min(leader_commit, log.last().index)
        
        - return AppendResponse(term: current_term, success: true)

    When try_advance_commit_index():
      Given role is Leader
      Then:
        - for n in (commit_index + 1)..log.size:
            - if log[n].term == current_term:
                - replicated = 1 + match_index.values().count(m -> m >= n)
                - if replicated > nodes.size / 2:
                    - set commit_index to n

    When apply_loop():
      Then:
        - loop:
            - while last_applied < commit_index:
                - increment last_applied
                - entry = log[last_applied]
                - result = apply_to_state_machine(entry.command)
                - emit CommandApplied(entry, result)

    When client_request(command):
      Given role is Leader
      Then:
        - entry = LogEntry(
            index: log.size + 1,
            term: current_term,
            command: command,
            timestamp: now()
          )
        - append entry to log
        - broadcast_append_entries()
        - await wait_for_commit(entry.index)
        - return apply_to_state_machine(command)

    When client_request(command):
      Given role is not Leader
      Then:
        - if leader:
            - return redirect_to_leader(leader, command)
        - else:
            - return Error("No leader available")

  # ==========================================================================
  # SHARDING
  # ==========================================================================

  Behavior Sharding:
    """Распределение данных по шардам"""

    Type ShardConfig:
      total_shards: Int = 256
      replication_factor: Int = 3
      strategy: ShardStrategy = ConsistentHash

    Type ShardStrategy:
      variants:
        - ConsistentHash
        - Range
        - Directory

    Type Shard:
      id: ShardId
      range: (start: Int, end: Int)
      primary: NodeId
      replicas: List<NodeId>
      state: ShardState

    Type ShardId:
      value: Int

    Type ShardState:
      variants:
        - Active
        - Migrating
        - Recovering
        - Offline

    State:
      shards: Map<ShardId, Shard>
      ring: ConsistentHashRing
      shard_map: Map<NodeId, List<ShardId>>

    When initialize_shards():
      Then:
        - ring = ConsistentHashRing(virtual_nodes: 150)
        - for node in nodes:
            - ring.add(node.id)
        
        - for i in 0..config.total_shards:
            - primary = ring.get_node(i)
            - replicas = ring.get_next_nodes(i, config.replication_factor - 1)
            - shards[i] = Shard(
                id: ShardId(i),
                range: calculate_range(i),
                primary: primary,
                replicas: replicas,
                state: Active
              )

    When get_shard_for_key(key):
      Then:
        - hash = murmur3_hash(key)
        - shard_id = hash % config.total_shards
        - return shards[shard_id]

    When route_request(key, request):
      Then:
        - shard = get_shard_for_key(key)
        - node = get_healthy_node(shard)
        - return await forward_to_node(node, request)

    When get_healthy_node(shard):
      Then:
        - if nodes[shard.primary].state == Active:
            - return shard.primary
        - for replica in shard.replicas:
            - if nodes[replica].state == Active:
                - return replica
        - raise NoHealthyNodeError(shard.id)

    When rebalance_shards():
      """Перебалансировка при изменении кластера"""
      Then:
        - new_assignment = calculate_optimal_assignment()
        - migrations = diff_assignments(shard_map, new_assignment)
        
        - for migration in migrations:
            - await migrate_shard(migration.shard, migration.from, migration.to)

    When migrate_shard(shard_id, from_node, to_node):
      Then:
        - shard = shards[shard_id]
        - set shard.state to Migrating
        - emit ShardMigrationStarted(shard_id, from_node, to_node)
        
        # Stream data to new node
        - await stream_shard_data(shard_id, from_node, to_node)
        
        # Update routing
        - update_shard_assignment(shard_id, to_node)
        
        # Cleanup old node
        - await cleanup_shard(shard_id, from_node)
        
        - set shard.state to Active
        - emit ShardMigrationCompleted(shard_id)

  # ==========================================================================
  # SERVICE DISCOVERY
  # ==========================================================================

  Behavior ServiceDiscovery:
    """Обнаружение сервисов в кластере"""

    Type Service:
      name: String
      version: String
      instances: List<ServiceInstance>
      health_check: HealthCheck?
      metadata: Map<String, Any>

    Type ServiceInstance:
      id: InstanceId
      node: NodeId
      address: Address
      port: Port
      state: InstanceState
      weight: Int = 100
      metadata: Map<String, Any>
      registered_at: Timestamp
      last_health_check: Timestamp?

    Type InstanceState:
      variants:
        - Healthy
        - Unhealthy
        - Draining
        - Deregistered

    Type HealthCheck:
      type: HealthCheckType
      interval: Duration = 10.seconds
      timeout: Duration = 5.seconds
      healthy_threshold: Int = 2
      unhealthy_threshold: Int = 3

    Type HealthCheckType:
      variants:
        - HTTP: (path: String, expected_status: Int)
        - TCP
        - gRPC
        - Script: String

    State:
      services: Map<String, Service>
      watchers: Map<String, List<Watcher>>

    When register_service(name, instance):
      Then:
        - service = services.get_or_create(name)
        - add instance to service.instances
        - if service.health_check:
            - start_health_check(instance, service.health_check)
        - notify_watchers(name, ServiceRegistered(instance))
        - emit ServiceRegistered(name, instance)

    When deregister_service(name, instance_id):
      Then:
        - service = services[name]
        - instance = service.instances.find(i -> i.id == instance_id)
        - set instance.state to Deregistered
        - stop_health_check(instance)
        - remove instance from service.instances
        - notify_watchers(name, ServiceDeregistered(instance))
        - emit ServiceDeregistered(name, instance_id)

    When discover(name):
      """Получить здоровые инстансы сервиса"""
      Then:
        - service = services[name]
        - if not service:
            - return []
        - return service.instances.filter(i -> i.state == Healthy)

    When discover_one(name, strategy):
      """Получить один инстанс по стратегии"""
      Then:
        - instances = await discover(name)
        - if instances.is_empty():
            - return null
        - return select_instance(instances, strategy)

    When select_instance(instances, strategy):
      Then:
        - match strategy:
            RoundRobin -> round_robin_select(instances)
            Random -> random_select(instances)
            LeastConnections -> least_connections_select(instances)
            WeightedRandom -> weighted_random_select(instances)
            IPHash(ip) -> ip_hash_select(instances, ip)

    When watch(name, callback):
      """Подписаться на изменения сервиса"""
      Then:
        - watcher = Watcher(callback)
        - add watcher to watchers[name]
        - return WatchHandle(() -> remove watcher from watchers[name])

    When health_check_loop(instance, config):
      Then:
        - consecutive_failures = 0
        - consecutive_successes = 0
        
        - loop every config.interval:
            - healthy = await perform_health_check(instance, config)
            
            - if healthy:
                - consecutive_failures = 0
                - increment consecutive_successes
                - if instance.state == Unhealthy and consecutive_successes >= config.healthy_threshold:
                    - set instance.state to Healthy
                    - notify_watchers(instance.service, InstanceHealthy(instance))
            - else:
                - consecutive_successes = 0
                - increment consecutive_failures
                - if instance.state == Healthy and consecutive_failures >= config.unhealthy_threshold:
                    - set instance.state to Unhealthy
                    - notify_watchers(instance.service, InstanceUnhealthy(instance))

  # ==========================================================================
  # LOAD BALANCING
  # ==========================================================================

  Behavior LoadBalancer:
    """Балансировка нагрузки"""

    Type LoadBalancerConfig:
      strategy: LoadBalanceStrategy = RoundRobin
      health_check: HealthCheck?
      sticky_sessions: StickyConfig?
      circuit_breaker: CircuitBreakerConfig?

    Type LoadBalanceStrategy:
      variants:
        - RoundRobin
        - Random
        - LeastConnections
        - WeightedRoundRobin
        - IPHash
        - LeastResponseTime
        - ResourceBased

    Type StickyConfig:
      cookie_name: String = "SERVERID"
      ttl: Duration = 1.hour

    Type CircuitBreakerConfig:
      failure_threshold: Int = 5
      success_threshold: Int = 2
      timeout: Duration = 30.seconds
      half_open_requests: Int = 3

    State:
      backends: List<Backend>
      current_index: Int = 0
      circuit_states: Map<BackendId, CircuitState>

    Type Backend:
      id: BackendId
      address: Address
      port: Port
      weight: Int = 100
      connections: Int = 0
      total_requests: Int = 0
      failed_requests: Int = 0
      avg_response_time: Duration = 0.ms

    Type CircuitState:
      state: CircuitStatus
      failures: Int = 0
      successes: Int = 0
      last_failure: Timestamp?
      next_attempt: Timestamp?

    Type CircuitStatus:
      variants: [Closed, Open, HalfOpen]

    When balance(request):
      Then:
        - backend = select_backend(request)
        - if not backend:
            - return Error("No healthy backends")
        
        - if circuit_states[backend.id].state == Open:
            - if now() < circuit_states[backend.id].next_attempt:
                - return balance(request)  # Try another backend
            - set circuit_states[backend.id].state to HalfOpen
        
        - start = now()
        - result = await forward(backend, request)
        - duration = now() - start
        
        - update_backend_stats(backend, result, duration)
        - update_circuit_breaker(backend, result)
        
        - return result

    When select_backend(request):
      Then:
        - healthy = backends.filter(b -> 
            b.state == Healthy and circuit_states[b.id].state != Open
          )
        
        - if healthy.is_empty():
            - return null
        
        - match config.strategy:
            RoundRobin ->
              backend = healthy[current_index % healthy.size]
              increment current_index
              backend
            
            Random ->
              random_choice(healthy)
            
            LeastConnections ->
              healthy.min_by(b -> b.connections)
            
            WeightedRoundRobin ->
              weighted_round_robin(healthy)
            
            IPHash ->
              ip = request.remote_addr
              healthy[hash(ip) % healthy.size]
            
            LeastResponseTime ->
              healthy.min_by(b -> b.avg_response_time)
            
            ResourceBased ->
              healthy.max_by(b -> 
                (1 - nodes[b.node].load.cpu_percent) * 
                (1 - nodes[b.node].load.memory_percent)
              )

    When update_circuit_breaker(backend, result):
      Then:
        - circuit = circuit_states[backend.id]
        
        - if result.is_error():
            - increment circuit.failures
            - set circuit.last_failure to now()
            
            - if circuit.state == HalfOpen:
                - set circuit.state to Open
                - set circuit.next_attempt to now() + config.circuit_breaker.timeout
            
            - elif circuit.failures >= config.circuit_breaker.failure_threshold:
                - set circuit.state to Open
                - set circuit.next_attempt to now() + config.circuit_breaker.timeout
                - emit CircuitOpened(backend.id)
        
        - else:
            - increment circuit.successes
            
            - if circuit.state == HalfOpen:
                - if circuit.successes >= config.circuit_breaker.success_threshold:
                    - set circuit.state to Closed
                    - reset circuit.failures
                    - emit CircuitClosed(backend.id)

  # ==========================================================================
  # СОБЫТИЯ
  # ==========================================================================

  Events:
    NodeJoined:
      node_id: NodeId
      address: Address

    NodeLeft:
      node_id: NodeId
      reason: String

    NodeDown:
      node_id: NodeId

    NodeSuspected:
      node_id: NodeId

    LeaderElected:
      leader_id: NodeId
      term: Int

    CommandApplied:
      index: Int
      command: Command
      result: Any

    ShardMigrationStarted:
      shard_id: ShardId
      from_node: NodeId
      to_node: NodeId

    ShardMigrationCompleted:
      shard_id: ShardId
      duration: Duration

    ServiceRegistered:
      service: String
      instance: InstanceId

    ServiceDeregistered:
      service: String
      instance: InstanceId

    CircuitOpened:
      backend_id: BackendId

    CircuitClosed:
      backend_id: BackendId

  # ==========================================================================
  # ПРИМЕР ИСПОЛЬЗОВАНИЯ
  # ==========================================================================

  Example "Distributed Key-Value Store":
    ```vibee
    # Конфигурация кластера
    cluster = Cluster(
      config: ClusterConfig(
        name: "kv-cluster",
        seeds: [("10.0.0.1", 7946), ("10.0.0.2", 7946)],
        consensus: ConsensusConfig(
          algorithm: Raft,
          election_timeout: 150.ms..300.ms
        )
      ),
      sharding: ShardConfig(
        total_shards: 256,
        replication_factor: 3
      )
    )

    # Запуск
    await cluster.start()
    await cluster.join()

    # Операции с данными
    await cluster.set("user:123", serialize(user))
    user = deserialize(await cluster.get("user:123"))

    # CAS операция
    result = await cluster.cas("counter", old_value, new_value)
    ```

  Example "Microservices with Service Discovery":
    ```vibee
    # Регистрация сервиса
    await cluster.register_service("user-service", ServiceInstance(
      address: "10.0.1.5",
      port: 8080,
      metadata: { version: "1.2.0" }
    ))

    # Обнаружение и вызов
    instance = await cluster.discover_one("user-service", LeastConnections)
    response = await http.get("http://${instance.address}:${instance.port}/users/123")

    # Подписка на изменения
    cluster.watch("user-service", (event) ->
      match event:
        ServiceRegistered(i) -> log.info("New instance: ${i.id}")
        ServiceDeregistered(i) -> log.info("Instance removed: ${i.id}")
        InstanceHealthy(i) -> log.info("Instance healthy: ${i.id}")
        InstanceUnhealthy(i) -> log.warn("Instance unhealthy: ${i.id}")
    )
    ```
