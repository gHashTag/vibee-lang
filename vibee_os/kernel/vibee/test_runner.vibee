name: vibee_os_test_runner
version: "0.1.0"
language: gleam
module: kernel/vibee/test_runner
description: Test Runner - Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ test_cases Ğ¸Ğ· .vibee ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹

behaviors:
  - name: run_single_test
    given: Test case with input and expected output
    when: test_run is called
    then: Test is executed and result compared
    test_cases:
      - name: test_passes
        input:
          behavior: "add"
          test_case:
            name: "add_positive"
            input: {a: 2, b: 3}
            expected: {result: 5}
          actual_result: {result: 5}
        expected:
          passed: true
          duration_ms: 1
      - name: test_fails
        input:
          behavior: "add"
          test_case:
            name: "add_wrong"
            input: {a: 2, b: 3}
            expected: {result: 6}
          actual_result: {result: 5}
        expected:
          passed: false
          error: "expected result=6, got result=5"
      - name: test_throws
        input:
          behavior: "divide"
          test_case:
            name: "divide_by_zero"
            input: {a: 10, b: 0}
            expected: {error: "division_by_zero"}
          actual_error: "division_by_zero"
        expected:
          passed: true

  - name: run_behavior_tests
    given: Behavior with multiple test_cases
    when: behavior_test is called
    then: All test_cases are executed
    test_cases:
      - name: all_pass
        input:
          behavior:
            name: "add"
            test_cases:
              - {name: "t1", input: {a: 1, b: 1}, expected: {result: 2}}
              - {name: "t2", input: {a: 0, b: 0}, expected: {result: 0}}
        expected:
          total: 2
          passed: 2
          failed: 0
      - name: some_fail
        input:
          behavior:
            name: "add"
            test_cases:
              - {name: "t1", input: {a: 1, b: 1}, expected: {result: 2}}
              - {name: "t2", input: {a: 1, b: 1}, expected: {result: 3}}
        expected:
          total: 2
          passed: 1
          failed: 1

  - name: run_spec_tests
    given: Specification with behaviors and test_cases
    when: spec_test is called
    then: All behaviors' test_cases are executed
    test_cases:
      - name: full_spec
        input:
          spec:
            name: "calculator"
            behaviors:
              - name: "add"
                test_cases: [{name: "t1"}, {name: "t2"}]
              - name: "subtract"
                test_cases: [{name: "t3"}]
        expected:
          total: 3
          behaviors_tested: 2

  - name: filter_tests
    given: Specification with many tests
    when: filter pattern is provided
    then: Only matching tests are run
    test_cases:
      - name: filter_by_behavior
        input:
          spec: {behaviors: ["add", "subtract", "multiply"]}
          filter: "add"
        expected:
          behaviors_run: ["add"]
      - name: filter_by_test_name
        input:
          spec: {test_cases: ["add_positive", "add_negative", "sub_positive"]}
          filter: "*positive*"
        expected:
          tests_run: ["add_positive", "sub_positive"]

  - name: parallel_execution
    given: Multiple independent tests
    when: parallel mode is enabled
    then: Tests run concurrently
    test_cases:
      - name: parallel_faster
        input:
          tests: 10
          each_duration_ms: 100
          parallel: true
        expected:
          total_duration_ms_less_than: 200

  - name: generate_report
    given: Test results exist
    when: report is requested
    then: Formatted report is generated
    test_cases:
      - name: text_report
        input:
          results: {total: 10, passed: 8, failed: 2}
          format: "text"
        expected:
          has_summary: true
          has_failures: true
      - name: json_report
        input:
          results: {total: 10, passed: 8, failed: 2}
          format: "json"
        expected:
          valid_json: true
      - name: junit_report
        input:
          results: {total: 10, passed: 8, failed: 2}
          format: "junit"
        expected:
          valid_xml: true

types:
  # Test Case
  TestCase:
    name: str
    input: {str: Value}
    expected: {str: Value}
    tags: [str]?
    timeout_ms: int?
    skip: bool?
    skip_reason: str?

  Value:
    variants:
      - Null
      - Bool: {value: bool}
      - Int: {value: int}
      - Float: {value: float}
      - String: {value: str}
      - List: {values: [Value]}
      - Map: {entries: {str: Value}}

  # Test Result
  TestResult:
    test_case: str
    behavior: str
    passed: bool
    expected: {str: Value}
    actual: {str: Value}?
    error: str?
    duration_ms: int
    output: str?

  # Behavior Result
  BehaviorResult:
    behavior: str
    total: int
    passed: int
    failed: int
    skipped: int
    duration_ms: int
    results: [TestResult]

  # Spec Result
  SpecResult:
    spec: str
    version: str
    total: int
    passed: int
    failed: int
    skipped: int
    duration_ms: int
    behaviors: [BehaviorResult]

  # Test Suite (multiple specs)
  SuiteResult:
    specs: [SpecResult]
    total: int
    passed: int
    failed: int
    skipped: int
    duration_ms: int

  # Test Runner Config
  RunnerConfig:
    parallel: bool
    max_parallel: int
    timeout_ms: int
    fail_fast: bool
    verbose: bool
    filter: str?
    tags: [str]?
    report_format: ReportFormat

  ReportFormat:
    variants:
      - Text
      - JSON
      - JUnit
      - TAP
      - Markdown

  # Test Executor
  TestExecutor:
    type: ExecutorType
    config: {str: str}

  ExecutorType:
    variants:
      - Simulated                     # Compare expected vs expected (for spec validation)
      - Interpreted                   # Interpret spec directly
      - Compiled                      # Run compiled code
      - External                      # Call external process

  # Comparison
  CompareResult:
    match: bool
    differences: [Difference]

  Difference:
    path: str
    expected: Value
    actual: Value

  # Coverage
  Coverage:
    behaviors_total: int
    behaviors_tested: int
    test_cases_total: int
    test_cases_run: int
    lines_total: int
    lines_covered: int
    percent: float

functions:
  # Core test execution
  - name: test_run
    params: {test_case: TestCase, executor: TestExecutor}
    returns: TestResult
    description: Run single test case

  - name: behavior_test
    params: {behavior: Behavior, executor: TestExecutor}
    returns: BehaviorResult
    description: Run all tests for behavior

  - name: spec_test
    params: {spec: Specification, config: RunnerConfig}
    returns: SpecResult
    description: Run all tests in spec

  - name: suite_test
    params: {specs: [Specification], config: RunnerConfig}
    returns: SuiteResult
    description: Run tests across multiple specs

  # Filtering
  - name: filter_tests
    params: {spec: Specification, filter: str}
    returns: Specification
    description: Filter tests by pattern

  - name: filter_by_tags
    params: {spec: Specification, tags: [str]}
    returns: Specification
    description: Filter tests by tags

  # Comparison
  - name: compare_values
    params: {expected: Value, actual: Value}
    returns: CompareResult
    description: Deep compare two values

  - name: compare_maps
    params: {expected: {str: Value}, actual: {str: Value}}
    returns: CompareResult
    description: Compare two maps

  # Reporting
  - name: format_result
    params: {result: SpecResult, format: ReportFormat}
    returns: str
    description: Format result for output

  - name: format_text
    params: {result: SpecResult}
    returns: str
    description: Format as human-readable text

  - name: format_json
    params: {result: SpecResult}
    returns: str
    description: Format as JSON

  - name: format_junit
    params: {result: SpecResult}
    returns: str
    description: Format as JUnit XML

  - name: format_tap
    params: {result: SpecResult}
    returns: str
    description: Format as TAP (Test Anything Protocol)

  # Coverage
  - name: calculate_coverage
    params: {spec: Specification, result: SpecResult}
    returns: Coverage
    description: Calculate test coverage

  # Utilities
  - name: count_tests
    params: {spec: Specification}
    returns: int
    description: Count total test cases

  - name: list_behaviors
    params: {spec: Specification}
    returns: [str]
    description: List behavior names

  - name: get_failures
    params: {result: SpecResult}
    returns: [TestResult]
    description: Get failed tests only

imports:
  - kernel.vibee.compiler
  - kernel.vibee.verifier

test_output_format: |
  # Test Output Format
  
  ## Text (Default)
  ```
  ğŸ“‹ calculator v1.0.0
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  âœ… add (3/3)
    âœ“ add_positive (1ms)
    âœ“ add_negative (1ms)
    âœ“ add_zero (1ms)
  
  âœ… subtract (2/2)
    âœ“ subtract_positive (1ms)
    âœ“ subtract_negative (1ms)
  
  âŒ divide (2/3)
    âœ“ divide_normal (1ms)
    âœ“ divide_float (1ms)
    âœ— divide_by_zero (1ms)
      â””â”€ expected: {error: "division_by_zero"}
         actual:   {crash: true}
  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total: 8 | âœ… Passed: 7 | âŒ Failed: 1 | â­ Skipped: 0
  Duration: 8ms
  Coverage: 87.5%
  ```
  
  ## JSON
  ```json
  {
    "spec": "calculator",
    "version": "1.0.0",
    "total": 8,
    "passed": 7,
    "failed": 1,
    "skipped": 0,
    "duration_ms": 8,
    "behaviors": [
      {
        "name": "add",
        "passed": 3,
        "failed": 0,
        "results": [...]
      }
    ]
  }
  ```
  
  ## JUnit XML
  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <testsuites tests="8" failures="1" time="0.008">
    <testsuite name="calculator" tests="8" failures="1">
      <testcase name="add.add_positive" time="0.001"/>
      <testcase name="divide.divide_by_zero" time="0.001">
        <failure message="expected error, got crash"/>
      </testcase>
    </testsuite>
  </testsuites>
  ```

comparison_rules: |
  # Value Comparison Rules
  
  ## Exact Match
  ```yaml
  expected: {result: 5}
  actual: {result: 5}
  # PASS
  ```
  
  ## Type Coercion
  ```yaml
  expected: {result: 5}
  actual: {result: 5.0}
  # PASS (int == float if equal)
  ```
  
  ## Partial Match (with *)
  ```yaml
  expected: {id: "*", name: "John"}
  actual: {id: "abc123", name: "John"}
  # PASS (* matches any value)
  ```
  
  ## Array Order
  ```yaml
  expected: {items: [1, 2, 3]}
  actual: {items: [1, 2, 3]}
  # PASS (order matters)
  
  expected: {items: [1, 2, 3]}
  actual: {items: [3, 2, 1]}
  # FAIL (order matters)
  ```
  
  ## Nested Objects
  ```yaml
  expected: {user: {name: "John", age: 30}}
  actual: {user: {name: "John", age: 30, extra: "ignored"}}
  # PASS (extra fields ignored)
  ```
  
  ## Error Matching
  ```yaml
  expected: {error: "division_by_zero"}
  actual_error: "division_by_zero"
  # PASS (error string matches)
  ```
