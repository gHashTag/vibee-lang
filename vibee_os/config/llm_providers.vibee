name: vibee_os_llm_providers
version: "0.1.0"
language: gleam
module: config/llm_providers
description: LLM Provider Configuration - API keys and endpoints for all supported providers

# ============================================
# DEEPSEEK CONFIGURATION (VERIFIED WORKING)
# ============================================
deepseek:
  api_key: "${DEEPSEEK_API_KEY}"      # From secrets store
  api_endpoint: "https://api.deepseek.com/v1/chat/completions"
  models:
    - deepseek-chat           # General purpose
    - deepseek-reasoner       # Advanced reasoning (R1)
  default_model: "deepseek-chat"
  max_tokens: 8192
  temperature: 0.7
  rate_limits:
    requests_per_minute: 60
    tokens_per_minute: 100000
  pricing:
    input_per_1k: 0.0001      # $0.0001 per 1K input tokens
    output_per_1k: 0.0002     # $0.0002 per 1K output tokens
  features:
    - function_calling
    - streaming
    - json_mode

# ============================================
# OTHER PROVIDERS (CONFIGURE AS NEEDED)
# ============================================
anthropic:
  api_key: "${ANTHROPIC_API_KEY}"
  api_endpoint: "https://api.anthropic.com/v1/messages"
  models:
    - claude-3-5-sonnet-20241022
    - claude-3-opus-20240229
    - claude-3-haiku-20240307
  default_model: "claude-3-5-sonnet-20241022"
  max_tokens: 8192
  temperature: 0.7

openai:
  api_key: "${OPENAI_API_KEY}"
  api_endpoint: "https://api.openai.com/v1/chat/completions"
  models:
    - gpt-4-turbo
    - gpt-4o
    - gpt-4o-mini
  default_model: "gpt-4o"
  max_tokens: 4096
  temperature: 0.7

google:
  api_key: "${GOOGLE_API_KEY}"
  api_endpoint: "https://generativelanguage.googleapis.com/v1beta/models"
  models:
    - gemini-pro
    - gemini-pro-vision
  default_model: "gemini-pro"
  max_tokens: 8192
  temperature: 0.7

local:
  api_endpoint: "http://localhost:11434/api/chat"
  models:
    - llama3:8b
    - llama3:70b
    - mistral:7b
    - codellama:34b
  default_model: "llama3:8b"
  max_tokens: 4096
  temperature: 0.7

# ============================================
# DEFAULT CONFIGURATION
# ============================================
default_provider: "deepseek"

fallback_chain:
  - deepseek
  - anthropic
  - openai
  - local

behaviors:
  - name: provider_selection
    given: LLM request is made
    when: provider is not specified
    then: Use default_provider (deepseek)
    test_cases:
      - name: use_deepseek_default
        input: {request: "Hello"}
        expected: {provider: "deepseek", model: "deepseek-chat"}

  - name: fallback_on_error
    given: Primary provider fails
    when: Error occurs (rate limit, connection, etc.)
    then: Try next provider in fallback_chain
    test_cases:
      - name: deepseek_fails_use_anthropic
        input: {primary: "deepseek", error: "rate_limited"}
        expected: {fallback: "anthropic"}

  - name: cost_optimization
    given: Multiple providers available
    when: Cost optimization is enabled
    then: Choose cheapest provider for task
    test_cases:
      - name: simple_task_use_deepseek
        input: {task: "simple", providers: ["deepseek", "anthropic"]}
        expected: {chosen: "deepseek", reason: "lowest_cost"}

types:
  ProviderConfig:
    name: str
    api_key: str
    api_endpoint: str
    models: [str]
    default_model: str
    max_tokens: int
    temperature: float
    rate_limits: RateLimits?
    pricing: Pricing?
    features: [str]

  RateLimits:
    requests_per_minute: int
    tokens_per_minute: int

  Pricing:
    input_per_1k: float
    output_per_1k: float

functions:
  - name: get_provider
    params: {name: str}
    returns: ProviderConfig
    description: Get provider configuration by name

  - name: get_default_provider
    params: {}
    returns: ProviderConfig
    description: Get default provider (deepseek)

  - name: get_fallback_chain
    params: {}
    returns: [ProviderConfig]
    description: Get fallback chain

  - name: estimate_cost
    params: {provider: str, input_tokens: int, output_tokens: int}
    returns: float
    description: Estimate cost in USD

imports:
  - kernel.agent.llm_backend
